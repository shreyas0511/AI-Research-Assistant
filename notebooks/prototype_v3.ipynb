{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e65618",
   "metadata": {},
   "source": [
    "Final prototype implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d5d5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary libraries\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Dict\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "import json\n",
    "from langchain_core.tools import tool\n",
    "import feedparser\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from urllib.parse import quote\n",
    "import re\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8f9a45",
   "metadata": {},
   "source": [
    "Initialize the LLM and embeddings model for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "14970514",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "# initialize an embedding model\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"text-embedding-004\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aa9443",
   "metadata": {},
   "source": [
    "Initialize the AgentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "de3cf704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need states for:\n",
    "# user query\n",
    "# plan generated by the LLM \n",
    "# results obtained by each search tool\n",
    "# reflection\n",
    "# summary\n",
    "class AgentState(TypedDict):\n",
    "    query: str # The user query\n",
    "    original_plan: Dict[str, str] # Entire plan returned by the planner, with rationale and reflection\n",
    "    plan: List[Dict] # List of plan dicts with tool name, search parameters and reasoning\n",
    "    results: Dict[str, List] # Result of each search tool\n",
    "    reflection: bool | None # To determine whether the findings are enough to summarize\n",
    "    reflection_notes: str # LLM's reasoning notes for the reflection\n",
    "    summary: str # Final summary\n",
    "    relevant_docs: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e210e6",
   "metadata": {},
   "source": [
    "Now we create the Planner node.\n",
    "\n",
    "The Planner node:\n",
    "* First checks if there is any reflection already present in the state, if so, generates a new plan\n",
    "* Will have a system prompt and a user query from the state. \n",
    "* Makes the LLM return a structured JSON object with the detailed execution plan.\n",
    "* Parse that JSON and update the state variables accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "09690deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for first time plan\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert research agent planner.\n",
    "\n",
    "You have access to the following tools:\n",
    "- arxiv_search: for academic papers\n",
    "\n",
    "Your job is to take a user query and return a structured execution plan in STRICT JSON format.\n",
    "Do not include any text outside of the JSON. Do not explain your reasoning in prose.\n",
    "\n",
    "The JSON must follow this schema exactly:\n",
    "\n",
    "{\n",
    "\"plan\": [\n",
    "    {\n",
    "    \"tool\": \"<tool_name>\",\n",
    "    \"purpose\": \"<why this step is included>\",\n",
    "    \"query\": {\n",
    "        \"search_terms\": [\"<list of exact search terms>\"],\n",
    "        \"additional_focus\": [\"<list of optional focus keywords>\"]\n",
    "    },\n",
    "    \"rationale\": \"<why these parameters were chosen>\"\n",
    "    }\n",
    "],\n",
    "\"reflection\": {\n",
    "    \"purpose\": \"<why reflection is needed>\",\n",
    "    \"analysis_focus\": [\"<list of aspects to check>\"],\n",
    "    \"rationale\": \"<why this reflection matters>\"\n",
    "}\n",
    "}\n",
    "\n",
    "Return ONLY VALID JSON. Do not include markdown formatting (NO ```json ... ```), explanations, or extra text.\n",
    "\"\"\"\n",
    "\n",
    "# response = llm.invoke(system_prompt + \"\\nGenerate a report on evolution of LLMs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "128e7593",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_reflection = \"\"\"\n",
    "You are an expert research agent planner.\n",
    "\n",
    "Given the original plan you generated and the reflections from the result of following that plan,\n",
    "We determined that the original plan did not yield the desired results.\n",
    "Given the notes from the reflection and original plan, generate a new plan in STRICT JSON format.\n",
    "Do not include any text outside of the JSON. Do not explain your reasoning in prose.\n",
    "\n",
    "You have access to the following tools:\n",
    "- arxiv_search: for academic papers\n",
    "\n",
    "The JSON must follow this schema exactly:\n",
    "\n",
    "{\n",
    "\"plan\": [\n",
    "    {\n",
    "    \"tool\": \"<tool_name>\",\n",
    "    \"purpose\": \"<why this step is included>\",\n",
    "    \"query\": {\n",
    "        \"search_terms\": [\"<list of exact search terms>\"],\n",
    "        \"additional_focus\": [\"<list of optional focus keywords>\"]\n",
    "    },\n",
    "    \"rationale\": \"<why these parameters were chosen>\"\n",
    "    }\n",
    "],\n",
    "\"reflection\": {\n",
    "    \"purpose\": \"<why reflection is needed>\",\n",
    "    \"analysis_focus\": [\"<list of aspects to check>\"],\n",
    "    \"rationale\": \"<why this reflection matters>\"\n",
    "}\n",
    "}\n",
    "\n",
    "Return ONLY VALID JSON. Do not include markdown formatting (NO ```json ... ```), explanations, or extra text.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "7738e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planner(state: AgentState) -> AgentState:\n",
    "    \"\"\" Generate a detailed execution plan for the user query \"\"\"\n",
    "\n",
    "    print(f\"State:\\n{state}\\n\")\n",
    "\n",
    "    # complete message to pass to the LLM\n",
    "    message = \"\"\n",
    "\n",
    "    # First check if there are any previous reflections in reflection_notes\n",
    "    # If yes, generate a new plan/ or additional search parameters\n",
    "\n",
    "    if state[\"reflection_notes\"] != \"\":\n",
    "        print(\">> GENERATING NEW PLAN...\")\n",
    "        message = system_prompt_reflection + f\"\\nUser query:\\n{state[\"query\"]}\\nOriginal plan:\\n{json.dumps(state[\"original_plan\"])}\\nReflection notes:\\n{state[\"reflection_notes\"]}\"\n",
    "    else:\n",
    "        # If no, this is the first run, run normally\n",
    "        print(\">> GENERATING INITIAL PLAN...\")\n",
    "        message = system_prompt + f\"\\nUser query:\\n{state[\"query\"]}\"\n",
    "\n",
    "    response_json = llm.invoke(message).content\n",
    "    if response_json.startswith(\"```json\"):\n",
    "        response_json = response_json[7:-3]\n",
    "    \n",
    "    try:\n",
    "        response_dict = json.loads(response_json)\n",
    "        state[\"original_plan\"] = response_dict # Load plan JSON as python dictionary\n",
    "        state[\"plan\"] = response_dict[\"plan\"]\n",
    "\n",
    "        print(f\"PLAN GENERATED, total {len(state[\"plan\"])} queries to be searched:\")\n",
    "        print(state[\"plan\"])\n",
    "\n",
    "        return {\n",
    "            **state,\n",
    "            \"original_plan\": response_dict,\n",
    "            \"plan\": response_dict[\"plan\"]\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(response_json)\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0d890",
   "metadata": {},
   "source": [
    "The planner node is now ready, let's start defining our tool nodes.\n",
    "\n",
    "For now, I am only including an arxiv search, will add web search, google scholar search etc. later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "a8e479de",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_expansion_prompt = f\"\"\"\n",
    "    You are an expert at constructing arxiv API queries. \n",
    "    Given the following search terms and additional focus terms, generate efficient arXiv API queries.\n",
    "    Do not generate overly complex queries, as arxiv sometimes does not return results if queries are too complex.\n",
    "\n",
    "    Requirements:\n",
    "    - Always include the exact search_terms verbatim.\n",
    "    - Incorporate additional_focus terms.\n",
    "    - Use arXiv field prefixes where appropriate:\n",
    "      - ti: for title\n",
    "      - abs: for abstract\n",
    "      - cat:cs.CL for computational linguistics\n",
    "    - Combine terms with AND/OR for precision.\n",
    "    - Return 2-3 queries max.\n",
    "\n",
    "    Return only a JSON list of objects. \n",
    "    Each object must have:\n",
    "    - \"search_query\": a valid arXiv API query string\n",
    "    - \"max_results\": an integer (default 5)\n",
    "\n",
    "    Return only valid JSON. Do not include markdown formatting, explanations, or extra text.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "ad7f5e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_arxiv(state: AgentState) -> AgentState:\n",
    "    \"\"\" given the current Agent State, search arxiv and return appropriate papers. \"\"\"\n",
    "\n",
    "    print(\"\\n>> SEARCHING ARXIV ...\")\n",
    "    # pass the search_terms and additional_terms to the llm\n",
    "    search_terms = state[\"plan\"][0][\"query\"][\"search_terms\"]\n",
    "    additional_focus = state[\"plan\"][0][\"query\"][\"additional_focus\"]\n",
    "\n",
    "    # make the llm generate appropriate arxiv search queries\n",
    "    message = query_expansion_prompt + f\"\\nSearch terms:{search_terms}\\nAdditional focus:{additional_focus}\"\n",
    "\n",
    "    queries_json = llm.invoke(message).content\n",
    "    if queries_json.startswith(\"```json\"):\n",
    "        queries_json = queries_json[7:-3]\n",
    "\n",
    "    try:\n",
    "      queries_dict = json.loads(queries_json)\n",
    "\n",
    "      # use those queries to search the web for top results\n",
    "\n",
    "      base_url = 'http://export.arxiv.org/api/query?'\n",
    "      results = []\n",
    "      max_results = 5\n",
    "\n",
    "      # construct valid arxiv url for each search query returned by the LLM and get appropriate papers\n",
    "      for query in queries_dict:\n",
    "          search_query = quote(query[\"search_query\"])  # URL-encode\n",
    "          url = base_url + f\"search_query={search_query}&max_results={max_results}&sortBy=submittedDate&sortOrder=descending\"\n",
    "          print(f\"url: {url}\")\n",
    "          feed = feedparser.parse(url)\n",
    "          results.append(feed.entries)\n",
    "\n",
    "      # for now, store all results just as a list\n",
    "      # will use a vector store later\n",
    "\n",
    "      for i in range(len(results)):\n",
    "        for result in results[i]:\n",
    "            result_dict = {\n",
    "              \"title\": result['title'],\n",
    "              \"published\": result['published'],\n",
    "              \"summary\": result['summary'],\n",
    "              \"arxiv_link\": result['link']\n",
    "            }\n",
    "            # not storing for now, will store later\n",
    "            # \"pdf_link\": result['links'][2]['href'] # or just /pdf instead of /obs in the arxiv link\n",
    "\n",
    "            state[\"results\"][\"arxiv\"].append(result_dict)\n",
    "\n",
    "      # finally, pop the current tool from plan (pop(0))\n",
    "      state[\"plan\"].pop(0)\n",
    "      print(f\">> {len(state[\"plan\"])} SEARCHES LEFT...\")\n",
    "      \n",
    "      return {**state}\n",
    "    \n",
    "    except Exception as e:\n",
    "      print(queries_json)\n",
    "      print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1167b1",
   "metadata": {},
   "source": [
    "Now, we have defined the planner and tool. Let's add the router node that will check if any tools are yet to be called and routes actions accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "9d797623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state: AgentState) -> str:\n",
    "    \"\"\" To check if any more tool calls are left and routing to the appropriate tools \"\"\"\n",
    "\n",
    "    # if plan is empty, go to the reflection step\n",
    "    if len(state[\"plan\"]) == 0:\n",
    "        return \"relevance\"\n",
    "    else:\n",
    "        return \"tool call\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ee6d1",
   "metadata": {},
   "source": [
    "Between the router/ tool call and reflection, add a retrieval step.\n",
    "\n",
    "In this step, instead of passing all retrieved papers to the LLM, blowing it's context limits, we retrieve top-k papers similar to the user query and analysis focus generated in the plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "40c07bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: AgentState) -> AgentState:\n",
    "    \"\"\" retrieve the top relevant papers from all papers retrieved from arxiv search \"\"\"\n",
    "\n",
    "    print(\"\\n>> RETRIEVING RELEVANT PAPERS...\")\n",
    "    docs = []\n",
    "\n",
    "    for paper in state[\"results\"][\"arxiv\"]:\n",
    "        # print(f\"PAPER:\\n{paper}\")\n",
    "        content = f\"Title: {paper[\"title\"]}\\nSummary:\\n{paper[\"summary\"]}\\nLink: {paper[\"arxiv_link\"]}\"\n",
    "        docs.append(Document(page_content=content))\n",
    "\n",
    "    print(f\">> LOADED {len(docs)} PAPERS\")\n",
    "\n",
    "    if len(docs) == 0:\n",
    "        return state\n",
    "\n",
    "    # Create embeddings and FAISS index\n",
    "    # vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "    # Define a query (combine user query + analysis focus)\n",
    "    user_query = state[\"query\"]\n",
    "\n",
    "    combined_query = f\"{user_query}. {' '.join(state[\"original_plan\"][\"reflection\"][\"analysis_focus\"])}\"\n",
    "\n",
    "    # # Get query embedding\n",
    "    # query_embedding = embeddings.embed_query(combined_query)\n",
    "\n",
    "    # # Compute similarity scores\n",
    "    # scores = vectorstore.index.search(np.array([query_embedding]), len(docs))[1][0]\n",
    "\n",
    "    # Get embeddings\n",
    "    query_emb = np.array(embeddings.embed_query(combined_query))\n",
    "    doc_embs = np.array([embeddings.embed_query(d.page_content) for d in docs])\n",
    "\n",
    "    # Normalize\n",
    "    query_emb = query_emb / np.linalg.norm(query_emb)\n",
    "    doc_embs = doc_embs / np.linalg.norm(doc_embs, axis=1, keepdims=True)\n",
    "\n",
    "    # Cosine similarity (0â€“1 scale)\n",
    "    similarities = np.dot(doc_embs, query_emb)\n",
    "\n",
    "    print(\"MIN, MAX, MEAN\\n\")\n",
    "    print(np.min(similarities), np.max(similarities), np.mean(similarities), \"\\n\")\n",
    "    threshold = np.mean(similarities) + 0.005\n",
    "\n",
    "    # Retrieve top-k relevant documents\n",
    "    # retriever = vectorstore.as_retriever(search_kwargs={\"k\": len(docs)})\n",
    "    # results = retriever.invoke(combined_query)\n",
    "\n",
    "    # Display results\n",
    "    # print(\"\\nTop Relevant Papers:\")\n",
    "    # for i, doc in enumerate(results, 1):\n",
    "    #     print(f\"\\n{i}. Source: {doc.metadata['source']}\")\n",
    "    #     print(doc.page_content[:500] + (\"...\" if len(doc.page_content) > 500 else \"\"))\n",
    "\n",
    "    # store relevant docs content in relevant_docs state\n",
    "    # for doc in results:\n",
    "    #     state[\"relevant_docs\"].append(doc.page_content)\n",
    "\n",
    "    # Select docs above threshold\n",
    "    count = 0\n",
    "    for doc, score in zip(docs, similarities):\n",
    "        if score >= threshold:\n",
    "            count += 1\n",
    "            state[\"relevant_docs\"].append(doc.page_content)\n",
    "\n",
    "    print(f\">> USING {count} / {len(docs)} PAPERS FOR REFLECTION...\")\n",
    "\n",
    "    return {\n",
    "        **state\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db538f9",
   "metadata": {},
   "source": [
    "Now, we add the reflection node, that takes the current top relevant docs and determines if we have enough to answer the users query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "aa478a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_prompt = \"\"\"\n",
    "You are a research agent tasked with evaluating whether the collected papers are sufficient to fulfill the current research plan.\n",
    "\n",
    "    Instructions:\n",
    "    - Read the research plan's reflection goal carefully.\n",
    "    - Review the list of retrieved papers (title, summary, link).\n",
    "    - Decide whether these papers are sufficient to proceed to summarization.\n",
    "    - If sufficient, explain why.\n",
    "    - If not, explain what is missing and suggest new directions to search.\n",
    "\n",
    "    Return only valid JSON in the following format:\n",
    "    {\n",
    "    \"sufficient\": true or false,\n",
    "    \"notes\": \"Your reasoning and suggestions\"\n",
    "    }\n",
    "    Return only valid JSON. Do not include markdown formatting, explanations, or extra text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "f6a19776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection(state: AgentState) -> AgentState:\n",
    "    \"\"\" reflect on the current findings \"\"\"\n",
    "\n",
    "    # This will again be a conditional edge, which will either route to summarize or back to plan\n",
    "    # Implement reflection/ EVAL logic\n",
    "    # If reflection sufficient, route to the summarize state\n",
    "    # Else, store reflection_notes, go back to the planner with current reflection, generate and execute new plan\n",
    "\n",
    "    original_reflection = json.dumps(state[\"original_plan\"][\"reflection\"])\n",
    "    # papers_json = json.dumps(state[\"results\"][\"arxiv\"])\n",
    "    papers_json = '\\n'.join(state[\"relevant_docs\"])\n",
    "\n",
    "    message = reflection_prompt + \"\\nplanned reflection:\\n\" + original_reflection + \"\\nTop relevant papers retrieved from arxiv search:\\n\"+papers_json\n",
    "\n",
    "    if len(state[\"relevant_docs\"]) == 0:\n",
    "        message = reflection_prompt + \"\\nplanned reflection:\\n\" + original_reflection + \"\\nTop relevant papers retrieved from arxiv search: No relevant papers retrieved, search with different search terms and additional terms compared to the previous search parameters.\"\n",
    "    \n",
    "    response_json = llm.invoke(message).content\n",
    "    if response_json.startswith(\"```json\"):\n",
    "        response_json = response_json[7:-3]\n",
    "\n",
    "    try:\n",
    "        response_dict = json.loads(response_json)\n",
    "        if not response_dict[\"sufficient\"]:\n",
    "            print(\">> CURRENT PAPERS ARE NOT SUFFICIENT...\")\n",
    "        else:\n",
    "            print(\">> CURRENT PAPERS ARE SUFFICIENT...\")\n",
    "        print(f\">> >> {response_dict[\"notes\"]}\\n\")\n",
    "\n",
    "        return {\n",
    "            **state,\n",
    "            \"results\": {\"arxiv\":[]},\n",
    "            \"reflection\": response_dict[\"sufficient\"],\n",
    "            \"reflection_notes\": response_dict[\"notes\"]\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\">> ERROR IN REFLECTION\", response_json)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "5b3748c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_router(state: AgentState):\n",
    "    if state[\"reflection\"]:\n",
    "        return \"summarize\"\n",
    "    else:\n",
    "        return \"plan\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4758d892",
   "metadata": {},
   "source": [
    "Now we add the summarize node to summarize findings and answer the user's query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "7d46c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_prompt=\"\"\"\n",
    "You are a research agent tasked with summarizing the findings from a set of retrieved papers.\n",
    "\n",
    "    Instructions:\n",
    "    - Read the titles, summaries, and links of the papers.\n",
    "    - Synthesize the key insights relevant to the original research goal.\n",
    "    - Reference paper titles and include links where appropriate.\n",
    "    - Write a coherent, readable summary suitable for a research report.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "5be0800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(state: AgentState) -> AgentState:\n",
    "    \"\"\" summarize the findings \"\"\"\n",
    "\n",
    "    # Summarize the findings and store them in the state[\"summary\"]\n",
    "    # Storing as string for now, will write to a file later\n",
    "\n",
    "    # papers = json.dumps(state[\"results\"][\"arxiv\"])\n",
    "    papers = '\\n'.join(state[\"relevant_docs\"])\n",
    "\n",
    "    # User Query:\n",
    "    # {state[\"query\"]}\n",
    "\n",
    "    # Papers:\n",
    "    # {papers}\n",
    "\n",
    "    message = summarize_prompt + f\"\\nUser query:\\n{state[\"query\"]}\\nPapers:\\n{papers}\"\n",
    "\n",
    "    print(\">> GENERATING SUMMARY...\")\n",
    "\n",
    "    summary = llm.invoke(message).content\n",
    "    print(\">> SUMMARIZED RESULTS !!\\n\")\n",
    "    # print(f\"SUMMARY:\\n {summary}\\n\")\n",
    "    # state[\"summary\"] = summary\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"summary\": summary\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f04f8aa",
   "metadata": {},
   "source": [
    "Now, create the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "1da61f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# add the planner node and set it as the start node\n",
    "graph.add_node(\"planner\", planner)\n",
    "graph.set_entry_point(\"planner\")\n",
    "\n",
    "# add the router node\n",
    "# add planner -> router edge\n",
    "graph.add_node(\"router\", lambda state:state)\n",
    "graph.add_edge(\"planner\", \"router\")\n",
    "\n",
    "# add the retrieval node\n",
    "graph.add_node(\"retrieval\", retrieve)\n",
    "\n",
    "# add the arxiv search node\n",
    "# conditional edge from router -> arxiv\n",
    "# edge from arxiv -> router\n",
    "graph.add_node(\"search_arxiv\", search_arxiv)\n",
    "graph.add_conditional_edges(\n",
    "    \"router\",\n",
    "    router,\n",
    "    {\n",
    "        \"tool call\": \"search_arxiv\",\n",
    "        \"relevance\": \"retrieval\"\n",
    "    }\n",
    ")\n",
    "graph.add_edge(\"search_arxiv\", \"router\")\n",
    "\n",
    "# add reflection and summarize node\n",
    "# conditional edge from reflection -> summarize\n",
    "# conditional edge from reflection -> planner\n",
    "graph.add_node(\"reflection\", reflection)\n",
    "\n",
    "# add an edge from retrieval to reflection\n",
    "graph.add_edge(\"retrieval\", \"reflection\")\n",
    "\n",
    "graph.add_node(\"summarize\", summarize)\n",
    "graph.add_node(\"reflection_router\", lambda state:state)\n",
    "graph.add_edge(\"reflection\", \"reflection_router\")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"reflection_router\",\n",
    "    reflection_router,\n",
    "    {\n",
    "        \"summarize\":\"summarize\",\n",
    "        \"plan\": \"planner\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# mark summarize node as the end node\n",
    "graph.add_edge(\"summarize\", END)\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93fd885",
   "metadata": {},
   "source": [
    "Visualize the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "224a37cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAALxCAIAAADt0etgAAAQAElEQVR4nOydB2DT1haGr2Rnk0ESCCMkzEDYu4VS9ixQoFD23hR4jFIoZRVaKKPQllKgjLJn2XuWTSl7770Tdvay9Y6txDhOHBLHdmTr/8rLk66urmVL99e559yhFASBAQBAaigZAAAYAQIBADAKBAIAYBQIBADAKBAIAIBRIBAAAKNAIIBtc3b/2yd3o2MiVHGx6oRYbcyeZ0yt/X9HQR3HCUzNaZI06TzP1AnvMzBe4HkuWQpF/TmO45lAu3SqwGnSmKZYStb8vy4npVCBCqaKF7cFQc3propXqtUJvLitLUXvkCNTKhWOTrxPbocSH3vmyOfIJAyHfhDAFtm28Pnze9GxMSoHB97BiXdw5hU8i4/V1F1OwQSVJg/vyKvj1JraqX3GqT5zCl4dr2YKxsQMCk2tVyUIiYpAlZnT/MfxnKAWNJU68UTa05ytqSxJ5yYVyKnjBXFbUL+/PN6BqeOTdjTK8r6WKZ14QcXFx6piolUJcQIpTvacTp98niN/sDOTHhAIYGNs+P3J8wcxTi58odLZarbMwXhm01w6HnblWNir5zGOzoom3fPmLiQtgwICAWyGm2cj/1n93NVD2bhbHp+8Dsy+2Db/2YMbkTn8XVoPzsskAwQC2AY7/wp9cDOieoscxT9yZ/bLkh8exEare08qwKQBBALYAFeOhx/f/qrXxPxMBuxe+uLh9Yhe0tAICASQOpvnPH3xOK6nPNRB5NDfL6+dDus7pSDLamzcwwPsneNb34Q8ipWVOhA1vvQtWMLtr3H3WVYDgQCS5tzB111HSaVBbk3qd/bjeW7T3KcsS4FAAOny17gHeQo6O7oxedJ1XOCTm1EJ0SwLgUAAiXLzTGRMZEKL/hKK+VmfnPmcl029z7IOCASQKEc3v/ALdGHy5ssh/lFhCSyOZRUQCCBRoiMTmvXKw6zInTt3mjRpwjLOt99+u3nzZmYZXN2VmxZkmScCAgGkyIE1LxydlErrjk64evUqMwmTT0wP+YtnC30Uw7IICASQIk/uRHv4WGqocXh4+LRp05o1a/bpp5/26dNn06ZNlDh37tzx48c/f/68YsWKK1asoJQ1a9YMGDCgZs2aDRo0GDly5OPHj8XTV69eTSkHDx6sXLnyzz//TPmfPn36ww8/UE5mASo18I6PV7MsAgIBpEhUREKuQEvZDyQEFy9epDq/bt26kiVL/vTTT7Tbt2/fzp0758qV6/Tp0x06dDh//jyJSJkyZUgCKP/r169Hjx4tnu7o6BgZGUnnTpgwoXXr1seOHaPEMWPGkGQwC5DNk1fw3M0zUSwrwHwQQIqoEgS//JbyUJ49e5a04OOPP6btgQMH1q1b18vLyyBPqVKl1q5dGxAQoFRq6kh8fPyQIUPevXvn6enJcVxMTEyXLl0qVapEh2JjY5mFUToqnj+ICargyqwOBAJIEUEQsmW31HjNsmXLLl++/O3bt+XLl69SpUpwcHDKPAqFgtoU06dPv3z5MtkLYiLZESQQ4naJEiWYteCYKjI8nmUFaGIAacLxzFIN7++//759+/b//vvv0KFD69WrN2fOnISEBIM8hw4doqPFixefP3/+qVOnZs2aZZCBGhrMWgiM51jWAAsCSBGe4yLDVcwyeHh4dO/evVu3bhcuXDhw4MDChQvd3d07duyon2fjxo1kaPTv31/cJb8myzoEgTm5ZE1VhQUBpAivZKEPLBLbIz8ChSfIiUCuBJIA8ixQGOL69esps+XMmVO3+88//7CsIz5WnTOLpq6EQAAp4ujEP71nkUEI5HScN2/eiBEjyHx49erV9u3bSR1IKegQuSRfvnxJwYgHDx4EBQWdOHGCIhrU+hCjnsSzZ89SFujk5ERSosvMzI06ganVQokqWTNNDgQCSJFc+V3evrSIW87NzY3il6GhoT169GjQoMHSpUsHDx78xRdf0KFq1aqRUgwbNmz37t1fffVV1apVyQ1BXsznz59TpJP8Ef/73/927dqVskxqsJCf4uuvv46ONr+o/bfjtdIhq1wQmDAGSJLYSGH+6DsDfinMZM+SHx84u/Btvs7HsgJYEECKOLlxCkdu6/xnTPaEv46v+WUulkUgigEkSvka2U/tf51GhokTJ+7duzfVQ+QLEDs4pYRinBbqE02kUXIal7R69epcuVKXgC3znjo6834BWTaFN5oYQLrMGX6nYCn3Bp1ypnr07du3UVGpd0COjY0l32Gqh7y9vZ2dLdWJ++lTo8Mu07gk8nEa045ZQ25/3jtPQHAW9KEUgUAA6RJyP37drAf9f5apJ2L11MdUOdsN92dZB3wQQLr45XfIH5xtyQ8PmPw4d+Dd21dxWasODAIBJE7jHrl4BbdyyiMmJxIi2b/bX0ph2ns0MYANsHNxSOjDmC5jA5kMuHM+atfypxJpWEEggG2watrjyLD4nj/Y+RT4G/949vRuZP/pUnG7QCCAzbB3RejNs+H+hd2a9cuyfgGW49KRsOM7Xip5rsdECYkgBALYFCq2eOKDyHcJ2f0cqjbOmb+EdWettAx7V7y4cylcnSCUqur16Rc+TEpAIIDt8eRm7IH1z8NfJ9AD7OzGu3ko3TyVCkcWH/N+CgmeZ2q9CSUMdhlH/zHx2ed4Jqj1sglc0gHGcZpD2nM12fWvgVynapWgXzKn8fhzglrQFp2YzvFUhma0leaYtmClA+Xho8IToiMSoiJU8bFqFzdlwVLutVpLSxpEIBDAhrl2Ivz2pYh3r+ITYgSVWh0X8/5h5pKqeaq72qTEKq9/iNPKhraKiwhUtxUKXpuabMQUp2CCKtnpBueK6Ryn/Z/6fYqDg0aceCXn7qXMW8i1alNvJmEgEAAY5ciRIxs2bPjll1+YXMFYDACMksYACpkAgQDAKBAICAQARoFAQCAAMEp8fLyDQ5YNtZYCEAgAjAILAgIBgFEgEBAIAIwCgYBAAGAUEgj4IAAAqQMLAgIBgFFIIBQKBZMxmFEKAKPAgoAFAYBRIBAQCACMgo5SEAgAjAILAgIBgFEgEBAIAIwCgYBAAGAU+CAgEAAYBRYEBAIAo0AgIBAAGAUCAYEAwCgQCAgEAEaBkxICAYBRVCoVLAgAQOp4eXlBIAAAqRMWFhYbG8tkDAQCAKOQ+UB+SiZjIBAAGAUCAYEAwCgKhYL8lEzGQCAAMAosCAgEAEaBQEAgADAKBAICAYBRIBAQCACMAoGAQABgFEQxIBAAGAUWBAQCAKNAICAQABgFAgGBAMAoEAgIBABGgUBg8V4AjIIoBiwIAIwCC4ITBIEBAPRo3Ljx8+fPqWpwHCemqNXqvHnzbtu2jckMNDEAMKRjx47Ozs48z3NJUGLdunWZ/IBAAGBI27Zt8+TJo58SEBDQunVrJj8gEAAYQiYDGRFOTk66lEqVKhlIhkyAQACQCs2aNQsMDBS3c+bM2b59eyZLIBAApE6XLl1cXV1po1y5cgULFmSyBFEMYDOEPIq7ejwsOjJO7JrA8UxQJx7iaZseZrVgkE7bdIDaDHSI8qjf5ye/o0C7nGaDiScmnkK7mv9pDp05fTomJrZkqZIe7h6JpWkyJ30EOS/1qg+VqaZyqEBtafofJ54rlqr7LIMMhIOzMmcep/J1PJlkgEAA22DxhAfRESoHJz4hTp1Y/7UVLhGqsYKaEzQ1WF8gmCgQ9D+q9MnzaxVCUwM0BelXAjGyKYiHNHWYTwp2clxiYYlF6ReYJBcCL3BqzvDytLvaYjSXmZigf51aHJ35hASBqVnlz7zL1ZCETKCjFLABFo66753HueWgXEwGPLwRd3TDExc3ZbGKbiyrgQUBpM7CsQ/yFsr2SXMfJidW/nSvQYdc+Uu5sCwFTkogaS4cikiIF+SmDkSuQNfDm0JZVgOBAJLm7uUwZzc5PqUFS3tER6pZVgMfBJA00ZEqQSXHVrCTM4uPy/qBpBAIIGlUCWq1LAUinnRRAt8bAgEAMAp8EEDSaDoycUyGcEwSXxwWBJA2mr5HTIYITBJfHAIBpA3HYEFkIRAIIGk0nZFhQWQd8EEASaNQagY1yRBYEAB8GFWC4ZBHmQAfBADAKEKyoeRZBgQCACnCCYxDEwOAtNHM0SLLKAbP8ZwEFAICASQNp7Gz5RjGUAtqKUzFgCgGkDRqTVvcbC/S5l/UXbpsAQPpBhYEkDQcJ4mmuGyBQABJo+lpjTnPsg4IBJA0Gkcdl7GOEE0+r9G+XbcbN64ePvKPm5tbqVLlvhv5g3s2d4NsGzauOXHiyLVrlx2dnMqULt+jR/+8efwpfeOmtcuWL/h1xrxx44ffv3+3YMHCX7bq0LBBUzo0fsK3dD116zSaPPX76Oio4sVL9e09KDi4pFjgrt1bt2xdf+/e7QIFCteuVb/lF+1EL2OzFnU6d+x5+Og/Fy+eO7D/dDq/BS9IwkkJHwSQNDwnZLSeKBTKv9etaNLki3/2nZo6edbDh/d/nzXNIM+lS+cpsUSJMhMm/PztiPFv3ryeOGm0eMjBwSEiInzm71O/+XoMlVCjet2p0yaEhDxn2sW+r1y9uHffjrlzlu3cftTJ0emnKePEs/bt3zVl6vigIsVWLt/Ss0f/detXzpo9XVfgth0bCxcuOm3qHyzdqDk4KQH4ECq14dzw6aFwoaBKFT8mZaGXfLPPWx08uDc+Pl4/A6UvWri2Q/tu5cpWpJytv+xIpsS7sHfiUcrcpXNvykMlNKjfhCrq7ds3xEPRUVHfDBubJ3deEos6tRs+evQgKiqK0nfs2FS6dLnBg77Nnt27fLlK3br03bRpLekO0xpBHh6eA/sPq1jhI2ZroIkBJI1pVja9rnXbefPkowr/9OnjwMACukSFQkEpf8yefu365cjISDHx7ZvXnh6Jq1EUK1ZC3HDXLplDNoW4my8gv7jcFpFN22wJDw9zdna+fOVC5069dOWXK1dJrVZfvHSuRvU6tFs0qDjLIJzASSG+C4EAksY0K9vJyVm37eyimTk+MjJCP8OxY4dGj/2aLIg+vQcVKlTk9Jn/ho8YoJ/BWLuGT23oWFxcHGnQwr9m0z/9dNGCIBwdHVkGEcQVfbIaCASwQ/TlICY6mv46OydbYIKcAqVKlSVngbirMxBMgywIMivq12tcXWsv6MiT25+ZitY7y7IcCASQNLyS4xQZtiIuXDij2751+wb5C/LmzaefISzsXS6/3LrdI0f+YZmjUKGg8Ihw8miIu2RQPHv2JGdOP2Yy0hjDCiclkDTqBEHI+OTvL16GUiBDpVJRCGPb9g21atV3cnLSz0BezFOnT5w7fzohIYFyionPQ54xU+nVY8CxYwd37NxMrgcKkUz4YeTQYX2p6cFMRdPEwGhOANLGtJ6UTRq3uHLl4uw5v9A2xRQGDvjGIEP37l9FRUWOHjM0Ojr6ixZtKdJJL/xvR/5v1Hc/MpOgBsu8uStWrFz057yZMTHRJYqX/vGHGQaqZItgbU4gaZb/9CAuRvhyaP70n9KsRZ2W/AldiwAAEABJREFUX7Tr3Kkns2Ue3or4Z8Xzgb8UZlkKLAgApAmclAB8CLmO9iZxgA8CgA9hggNi88b9zPaRiCpCIICkkbGLTBKj3BHmBJKGV3CcgskSSUgjLAggadQqU/pB2AGasRgSsCFgQQBJo5m6VZYPqaD51hiLAUDaCHLtqSONubRgQQBJI43J3+ULLAggaTQ+CFkuvYeOUgCkA/maD+goBcCHwLT3WQsEAkgaal9gOGEWAoEAksbZmZeEqW11FLyj0gHT3gOQJtm8HVWxTIa8fhKldMj66gmBAJKmUSe/6Kh4Jr/OlLcvhOXwd2ZZDQQCSBsFCwzKtnraPSYnjqx7GROlat4vN8tqMKMUkDRLly79/fffJwxZ8uiyS678rgFFs3EKldrAoOA4g+eY47Sei6Q0LnGVCdrnxMXCBc3/i3vaPDzH1EJSXv1aIXZG0LpBOO1uUvRRE1shByovlqM9kLiShfZqWNIl8TxTq1niEe2fpCziNXDav+LH8Y6KN49iH96IoDO6jgtgEgBOSiBRtmzZMnPmzGbNmp06dYp2Lx0JP3vg9bO7UfGxhu0NqmKaOqjv0TNYdEY8pKvmKTMkYbAWhaCbkEIwPPReXJIXmHiKkEoujSzo9W4QNFKl0Qe1kFgyeSWVjoqc+Zw+75P1toMILAggOY4cOULSUKpUqf/9739eXl7M7li4cOH8+fOzZcuWPXt2+poVK1YMDg4uUKAAkx4QCCAhLl++TNLg5uZG0iDNCmMWXr582bdv3/v374u1z9HR0dvb29fXt0SJEsOHD2dSAgIBJMGTJ09IGkJCQkgaypcvz+ydCRMmbNq0SX8hP7Vas5z3uXPnmJSADwJkMZGRkSQN//33H0lD7dq1mTxo0aLFiRMnQkNDdSnOzs7Hjx9nEgNhTpCVzJ0797PPPgsKCqLXqXzUgWkW2inl7+9PVoO4SxsSVAcGgQBZxZo1a6pWrapUKg8dOtSyZUsmP5o3b05WA9OGX0+fPr1z504mPSAQwNrs2bOncePGDx8+PHjwYM+etr3+VWZo1KiRn59mdd+AgAByRlSuXLlWrVpS8wnCSQmsx8mTJ8ndEBgYSO4GsW7InFmzZm3ZsoUUU9wNDw8npYiNjaWgBpMGEAhgDW7dukXSoFKpSBqKFSvGgHH27duXkJDQsGFDJgEgEMCyUMyfpIEEgqShSpUqDKSD0aNHjxo1ysXFhWU1EAhgKcheIGnYvXs3SQOFKhjICFFRUdevX8/yLiFwUgKLsGjRok8++YQcDbt27YI6mICrq6uvr2+WO3EhEMDMbNq0ibzx9AI8ceJE+/btGTAVim4MHDgwJCSEbDGWRaCJAczGoUOHfvvtN7KKBw0a5O7uzoA5UKvV5LbMnz9/UFAQszoQCGAGLly4QO4GLy8vkgZ67zFgbsgWmz9/vpubG7MuEAiQKR4+fEhWw9u3b8kTWaZMGQYsxosXL6jhFhgYyKwIBAKYSHh4OEnD2bNnyWqoUaMGA5bn9u3ba9asoQgosxZwUgJT+OOPPz7//POSJUtu2LAB6mA1ChcuXLx48cePHzNrAYEAGWPlypUff/wxBeEOHDjQvHlzBqxLixYtvL29T506lZCQwCwPBAKklx07djRs2JCibseOHevWrRsDWQSpc+nSpatVqxYba/ElQ+CDAB/mxIkT5G4oUqQIeSJ9fX0ZkAa3bt3KmTOnp6cnsxgQCJAWN27cIGngeZ48kSQQDEiMmzdvnj592nId0jDlHEgdakrMnDnzwYMHZDVUrlyZAUkSFBS0bds2CjZbqPsJLAhgiFqtnj59+sGDB8lqqF+/PgOS58WLFxR1LliwIDM3cFICQ8aOHUvN2u3bt0MdbIUcOXK8fPlyzpw5zNxAIIAhT58+pUAmAzYFNQWioqKYuYEPAhiiVCqtE2MHZuQjLczcwIIAhkAgbJGIiIjnz58zcwOBAIZAIGyR//7775dffmHmBk0MYAgEwhbJli1brly5mLmBQABDIBC2CHwQwEpAIGwR+CCAlYBA2CLwQQArAYGwReCDAFYCAmGLwAcBrAQEwhaBDwJYCQiELQIfBLASEAhbBD4IYCUgELYIfBDASkAgbBH4IICVgEDYIvBBACsBgbBFLOSDwJRzIJF69eq9fv1arVZzWujBoL++vr67d+9mQK6giQESadasGSmCQqHgeZ426C+JRcWKFRmwBeCDAJalffv2BgvD5smTp1OnTgzYAhbyQUAgQCLe3t5NmjQhC0KXUrZs2WLFijFgC8AHASwOmam9evW6desWbefMmXPixInlypVjQMbAggDvobdQ8+bNnZycaJtsB6iDDWEhHwTCnDbJnYvR8bHxhqkcYynNQY5jmngEM7AUxThFyhLKFGxYrsidqMio2pVaXj8Vpl8IS1k2WaBMEP+m/FCWGryS88uTzdOPY8CskA9iz549U6ZMYWYFAmFjLP/pYdjreJ5nCXEZaBsKWvX4MFqJKZa9tZBduH+Su38yND0niRrB0gfvQNoR6ujIl6nhXam+BVedlRvwQQA2/7u73rmda36Zx9GF2TQXD729evJNg065A4o6MyBhIBA2w7yRdwuV9anc0H7eussn3v2ogW/5Oh4MZJoILWY3IuCktA32LAtVOirsSR2IYpW8zhx4xYA5QD8IWfPsboxPbnuzxivW946PVUdHM5B5MB+ErImLS1A62mNzXeDePI12KWTjPhUJgPkgZE1CvKCyxxGWKpVajYGj5gBjMYB9IuAZNAeYDwLYIVyqnbtAxoEPAtghaoGhT6VZgA8C2CE8p+mGCTIPfBDyxk5rkUAWhJqBzAMfhLwRBLsUCc2oLh4mhBmAD0Lm2Kk3T9D09mcg08AHAewQ+CjNBXwQwA7hEOQ0E/BBADtEQDcIMwEfhKzhOPu0xTm7jc9YG/ggZI0g2O2bFhaEWYAPQtYILGt8/Rs3rf1pyjhmMewzeJsVwAchazjtqAXrc+PGVWZJOPSkNBMW8kHAgrBP7t69XatOxRMnjrZq3bBn73Zi4rFjh3r36dCgUdXWbT/7bvSQkJBEi7RR42qr1yzVnTt12oQ+fTvSxuChvXfv2bZnz3Yq6uat65Ry5crF4SMGfN6sVqcuX8ye80tkZKR4yrjvh0/4YeSf82ZSzrPnTrF0g56U5oIcEEOGDGHmBgJhnzg4ONDfpcsXtGnd6euho2n79Jn/xn7/Tf36jdeu3jFuzOSQkGe/zpycdiG/zpgXHFySTjmw/3RQkWKPnzwaNvyrmNiYWb8v+mH8z3fv3hoytLe4Djh93N17t+nfxB9mFC5clKUbWBDmAj4IeZPBKAanbZFUqvjxl606BBcrQdt/LZpT/dParVq29/T0KlGi9Ff9hpJ9cT0jLYh9+3Y6KB1IGgIC8ufPX3DY12Nu3b5x9NhB8eOeP386ftzUqlWre7hnZBJaxDnNBOaklDU8M2XMQlCRYN02vfCLaZVCpGhQcfp7/foVlm6uXLlAJZC+iLu5cuXOk8f/4qVz4m5gQAFnZ1MmxeNgQZgD9IOQNWpB/F/GcNQuose09mdsbKyT0/sK7OrqSn+joiLTX1pERDhZHORl0E988/qVwWdlFBgQZsFC/SAgELJAfLfHxLyfQDpSKw0+3r4pM6vUqlQL8fbxLVWqbLeuffUTPT28WCbQhDmhEObAQutiQCBkgVKpLBoUTDEIXYq4XbBQEfrr6OgUHR2lO/To0YNUCylUsMievdvLlC7P84kt0/v37/r7B7BMoGlfoI1hDiy0Nid8EHKhRfM25FBcv35VWHjYufOnZ8+ZUb5cpSLaiEPx4qUOHd5P7x/aXrZ84cuX75fkzJs337Vrlyly+ebN61atOqjV6lmzp8fExJCIUFCze882FLlgmUBQY7yWeUA/CFmjjQVm6k1L0coe3b9a8/eyZs1rT5n6felS5caO+Uk8NKD/MO/sPk2b1azX4OPY2Jg6tRvqzmra+AuKUHwzvP+du7coPLFwwRoXZ5c+/Tp27try/IUz3wwbQ+FPlhk4hukgzIKF+kFgbU7bYM7wO3kLu9Rqk4fZF0vG327e198/CEv4ZhaszQnsEM3rCa8oc4CxGLKG4zGoCaQF+kHIGkEt2OXsbLwCXa3NA+aDkDn2OWmtWoV+EOYBYzEAAEaBD0LWwAcB0gY+CFmjiUbbY49DDqt7mwn4IOSNYJ89VjRjMTBhjDmADwIAYBT4IGSN3U57j3nvzQR8ELJGrZnW2g5rkoAZpcwEfBCyRjtWCzUJGAU+CPkSGxurUqkYAMbBnJRyRJxXft26dZxS5eCgYHaHQskxBwYyD3wQ8oKCmlOnTn337t2kSZM6dOiw9N7DmBj7jAf6+7swkGkwJ6VcePXqlUKh4DiuYMGCX375pZgYEOR2+2IEsy/+3fLKyUXJHBnIPJgPQhZs3LiR7AWlUunp6alTB6Jmax+eZ3uXhjA74vbFd3W+zMmAOYAPwp4JCQnZt28fbeTLl2/Xrl3UnkyZp/uEwLcvo7fOffz0VhyzZeIi2LFNL1ZOuttmqH/+UmhfmAcL+SAw5VzW8+jRo379+k2ZMqVEiRIfzLz2lyevn8eq1YI6IZlLQq0Zz/X+VqoFpr/OjnYgh6AbM64/sEN/O9lZAqcLrKoFjtduJ5WTPFFXQtIpujW7dfl1KZyCp49QOKgrNspW/tPcDEgbCESWERoaOm/evNGjR79+/drb2ztD50a+YwlxyQKfSRUySQTe7ycdFrT2olpvVz8ne59y5uzp/fv3Dx8+giq7WKC4oRMBcUd3NBl6BYryYphfwRYu/m3z5s3u7u7UjCpcuHCxYsUCAwMLFCiQOzf0wnSwLob9QMFLNze3H3/8sWFDzfzRGVUHws2TaaqaZeAco+KEN56+liq/R48eR48effjw4bNnz65du7Zz5076BchCJslYtGgRAyZhoXUxIBBWJTw8fPLkybVq1apbt+7MmTOZJKlevfonn3zCLAYZDnXq1Fm8eDHTrvqrVqtfahEXCgemAR+EbfPkyZO8efOSxtMP3qBBAyZvyCnbs2dPsiB0KTzPnzx5kgGJgSiGNRgxYsSsWbOYZvWa+tJXB1KxiRMnMkvi5+dXo0YN3a5KpRINCmAyGIthe9y6dYta2kyrCz/99BOzEeLi4uLj45mFad26dc6ciZ0g/P39qeW1evVqBkwF/SBsjA0bNowdO5ba27RNTW5mO5Dr9LvvvmMWJiAgoEqVKuSAyJEjx9atW5cuXUrh3jFjxjBgEvBB2Abklj9//ny7du1u375NMTwG0qRevXp79+7V7e7atWvOnDkLFy709fVlQALAgjAb9DIMDQ2dNGlShQoVaNd21WHTpk2WMFZTRV8dmNZ4mT17dseOHQ8cOMBARoAPQrrcu3dv0KBB1G6nSP6yZcuCgoKYLRMbG5uFEUeK9ZAdsWPHDsmGgaUJfBBShEwG+rtmzZo2bdo4OTm5uNjDyIKWLVsOHjyYZWvm1zsAABAASURBVCnTpk3z8vLq1asXA+kDPghp8fbt25EjR37++eeNGjViwDKQN6dPnz5//fVXekapAEsAgcgwly5dKlWq1JkzZ8jpUKlSJWZ3rFixIjIysnfv3kwCqFSq7t27kwq3bduWAeNgPghJ8PXXX4vhevJE2qU6sKz2QRigUCiWLFny5MmTUaNGMWAcC/kgYEGki5MnTzo7O5cuXfrixYv0l9k1pA70VDg4SGuuyN27d8+aNYuaGzly5GAgBSQQx48fHzJkCDMrEIgPs3nz5j179lD8Uuz1BLIKCuN169Zt2LBhttXxzKaBQBiF9JhUmSQ5JCTEz8+PyYb58+eTudSpUycmSUaMGJEnTx6KKzOgB3wQ1oMa4fRbU/CSAn5MO7KIyYmYmBgpvzamTJni4+PTo0cPBvSAD8IaXLt2jZoSv/32G7UmyD3GZEl8fDzHcUqlpOcKuXDhQs+ePRctWlSyZEkG4IOwNHfu3ClUqBA5zCtXrhwcHMyA5KFHlyKg9erVa9++PQOWAU0MTZcnirHfvHmTtrt06QJ1IEt148aNTPKQmUMWBHkuR44cyWQPxmKYH3FE0KtXryZOnIgOkTpsaynQoUOHUlCjcePG5EtmMsZCPgj5zklJbnAvL69atWpRy4IBPajK2Zb/pW7duqVKlaLmBrXAaZvJEozFMA+7du2iGF7NmjXFSSIZsCOorZEzZ06zO+rkTIYFgoxPK8xHZiFOnjxJcYrWrVunZ9gl6QiTKnQLLNcKWLVqVVBQkDirhSVwcnLiOI5ZhhUrVuzbt0+G0+dbqB9EhgWCLoLi5MymoAuOi4vz8PCgL5v+R5MaIJIN9b1588ZyAhEWFuakhVkGHx8fywkE046mo+bGwoUL7b5TvD779++3xLoYdu6kVKs1K0nR+1Zc7dKiz6Xd4O7ubjl1sALkjzh16tSvv/66fPlyJhuk4oOwFQuCRIHehGQFmOxvk60FYWksbUHo+O23354+fWr2l6qssEMLQnSRkO2QPXt22faGzAwkrNQiY7YPBarq169PAWxLdBCQGugH8WFIFF69eiU2K8hI5nl0AzMFewps1alTZ+nSpT179qT2ObNr7GFOyjZt2qxcuZJZgOjoaPGxJqvBptvPWcW9e/caNmx4+fJl2iZvrqOjIzMV/aImTpyY5d0cc+TIsW3btoMHD06fPp3ZLxbyQdjDO/bdu3fUIKdmLa+FyZK2bdvqL3WZGezSlTtp0qQ8efJ07dpVNDDtj48++sgSHUBsuDpFa2HaN54YpJAtISEhb9++ZWaCBNd2u7qkQbt27b755puPP/74woULzO6wkA8is156sif79es3YcIEiiqR23/27NkJCQlLliw5efJkaGhoiRIlPv/888qVK6c88erVqytWrLhx44anpyeJX8eOHV1dXc+cOTNq1CgyBXWzGFMGcjVR+VTI5s2bqdjr16+TAUwZ6J0p9pKmlwO99GrXrk0nkmQUK1aM2pz0VyyB2mZ//PHHy5cvCxYs2LRpU93audQo3bFjx/379/Pnz1+jRo3mzZvb6JuTHvcRI0bQRrdu3apUqTJu3Djapqbc3r17ySNDBnbp0qUHDhyos63SOCRi4IMIDw9fsGDB7t276U6VK1eue/fu4pqa9MOS3U5NCcpQtGjR9u3blylThkkbemzoEaLHg+64ZGfEMQ26HVLsByHOXEjPXKtWrcRJfkgjNm7cSLpAMvHpp5/++OOPR44cMTjryZMn3333HYVLya0yduxYUhmSdlKWsmXLki1w7NgxXc7jx49TSoUKFehBnDNnTpEiRciOGjZsGHnaZ82aJVZpCkZeu3Zt//79M2fO3LRpE/kgfv75Z/F0+tVIXMiw/OGHHz755BP6OHGAFv2dMWNG4cKFFy1aREfpgufOnctsE6qW9B1pg76LqA7kltu6dWuvXr3ovnTp0uXw4cMbNmwQM6dxSAcJgW5CSropY8aMITWhJ4/eBC9evKBdSqR7RykU7KB7MX78+Hz58tFHv379mtkCpHd0qcOHD2d2hIV8EJm1IMQqWr58+S+++IJpBwLu27evdevWjRs3pl16XV+5coWeRVIK/bOoflKt1q1tO3jwYHpYSQuqV69O0n706FHdnOu0XatWLYpWBgUF/fnnn+SDdHd3pw+lZ5SeSJIJal8wbXODhINsENquWbMmmRJRUVG0S/WBdIGMC6adhzoyMpLSmXZERsmSJQcMGMC0fk16mZB2kElC28zGIVPz77//JgmoWrUq7dJPSvq7atWqZs2a0d0xdki/BH1LSjTZ5s+fTxLAtMtwr1+//s2bN2R9kF47OzuLd5AsCHIE0r02uNGShV5m//zzDzlT6eH09vZmts9HWpi5MU9HIHqxixu3bt2it4p+N34yYsny0dVkEWpf0COlmwPWz88vd+7cZCNU17J9+3Zx5Vuy/8nWoJpPrWISSHLCkUbQ8ypWcqadykEslh5fUR2YVkqZtp7Q40sVQFQHEbItmTYaShfQoUMHXTpZLpRIF2Arz3caPH78mDwIuhYW094dUsanT5+SjBo7pF8CNRlcXFzETmL0A9K2qA5Mu+Co2Jwh6C6QzXLx4kWd4UC3idkO9GDQw/ntt9+S/WWJd6+VsdBYDPMIhC4qRk8b0y4eYZCB3jn6AkHf5ObNm6TfBnmYVlDoNU6tEnoWyabw9fWldiPlp1cZmbIUKO3Rowd5E86ePau/UEKqwQsyg6nap4x6koRRPVmsRT/djH6+LESsrvrfWhyZRuqQxiH9RNqmFLLUmPaGpjpojRxM1LgglwRFMUlxyOho0qQJszWoaUkvMztQB2YxH4SZuxL7+Pgwrf1GISX9dIO1DMioo2rfuXNn/URRQehRIyPi33//JZcbOSNI5imFHtadO3fSKZQoZhaVKG3EvlIpc9ITTxWjbt261apV008nK4bZPm5ubkwrjroU0eCi35yaGMYO6f9KjlrEbbLLSCxIZw0kmJwXJLL0JhAlxha19dy5c/TWodYTswsk6oMwgHRBfBfpHNpkF5BXXGf/ixQoUIB8iqVKldI9dg8ePNDNzkBuCDFgcefOHdGTRM802b2i81yEfBMfvBjRc0ENY10KmcRkPvTp04dsELJKdBdJzzqFiOxjRRb6avTFxUacmEKRIHp6yBajymzskIGMkiKQl4dkgn5A+vGp5Sie8ujRI/IEk7eSbgedqBs1n57bITX69u1LJiqzFyzkgzBzPwgSAgpYUvyS2vNUFamlQNEKijIaZCOPJj2CFDigh4/azAsXLqS7RR4H8Wjx4sWprpJ/kXQkMDCQaVvFYrOCQnr04Ooc7x+cZYx8pRQ6XbduHZ1IXrS1a9dSUJNpI4JkpFDoTnQ9/PTTT9S0tt0BCOQ7ZNq3OjloyNois2v16tUnTpyg3418xlu2bKEfnLQ4jUMGBVIKGRekm+R+JtGnG0TWHP2SFDmigHFAQADdGmqwkLeIbsepU6fOnz9PHiWKcTAboX///qR09jRUR6L9IFLy5ZdfUmWmqkgPDZm7wcHBKdc4oSeV1IHyUBCeXkr0dqJABjkddBmolUHecgpAirvUKKAwBz2y33//PWkKed2pAUw/B4XcdD6zVKlXrx7VhOXLl9O5ZEhTDF/sB0EhDHrW16xZQ48+FUgXSSXbbh9tqsP0TZctW0Z1eOrUqaS2VMMnT55MtZfaTeS4oZsi5kzjkAHU4iOBoN+E1HPatGkUJ2ba1xS59Mh/SaEiMvroTfD7779TM57aGhQfod+Tfu2mTZsyaUM3nR4AS7xvsxAL+SBkMWGMaWC4t4Ww2nDvVCFbkmwH0ghmX0hlXYwsEQjyrln/9Q6BoI8gy0sMZ5iRrBWIKlWqUFtMaksTSxbbGItBhisW+LE+YhNdjH3YB9SknTFjhl2qg6zng5Dy/LH2ja1PP6fP4sWLixUrRhYEs0dkvS6GzAdrZi26ofTMlqFY1aFDh+x4tmtZz0lJVi7F5K3ccIUPQoQiwW/fvjXjgIUs8UFUq1aNwrowRTNKhisAGZzWrzYUS+vXrx/VWGZFpPzOpPixNSc+oVA0VWndiAybg4LoFNm1b3WQylgMBy3MumTPnt3FxQXyryMzU8KZgOUW0bECS5cuLViwoEG3evvDNsZiWAh6AzCQpWzevNnT07NmzZrMprhy5cr+/fuXLFnC7B1Zr8154MCBypUri8OQQJZAbqDatWvrz+VjE3z66af0Xk3PSosgVWzDNT137lw5LG0gZcj3RK9i2+oTMWTIkEmTJslEHWTdD6JWrVqIdGY55AOyIZlevnx5YGCgHcwAlE5k3Q+ib9++DEgAMiIoyP3VV18xaXP9+vXdu3cvW7aMyQZZ+yCOHDlSunRp3RR1IAv58ccfR44cKfGB0uRM3bZtG6zOzGMbTYyFCxdSKJ4BCTB69GiJq8OwYcO+//57uamDrH0Q1atXh/kgHaitK1lv5apVq3Lnzm1z4djMYw9rc5pM9+7dbbcbn/1B98ISz2LmuXnz5tatW1POmSwHZO2DOHHiROHChX19fRmQBg8ePPD395daW6N27dqbNm3Snz8dZBLbsCDIHX3nzh0GJAOpg9SmtBo+fDj5R2SrDrL2QVSpUgXmg6Qg26Ft27YPHz5k0mDNmjU5cuTQXyRJbsjaB9GxY0dxnV4gHcaMGbNv3z4mAci63LBhwzfffMNkjKx9EKdOnQoICPDz82MApKBevXp///23lWcDkAm2YUGsW7dOf/0bIBGoibFr1y6WpYwcOXLEiBFQB1n7ICpVqgTzQYKQWUeN/0uXLom71atXZ9aF3hwkDXXr1mWyR45jMcqXL6/bFicpowYR+aJ2797NgDT49ddfnzx50qhRI3GVs8WLF+uWO7I09+/fJ3mixgUDtrI2p3kJDg6+du2a/tRvtC39hZtkRadOnUggxFlt1Wq1NWOfffr0WblyJQNabGNtTvPSvXt3g0li8uXL17p1awYkwGeffVa5cuWnT5/qz0BrNZ/3qFGjhg4dKq4mD5g8fRB16tQxiG7WrFlTf41vkIVUqVIl5QCZqKgoZnkoqElvDnGZVSAi034QZETo+sblyZOnRYsWDEiDMWPGDBo0iPyU+vNrW8GCoNDJ8uXLv/vuOwb0sJAPQuoCQY7xoKAgcZteWRiyJSmaNGkyc+bMChUq6FbfsoJA9O3bd+7cuQwkhxwQZl+5l9lEmLNLly6kjiQNbdu2ZUBi+Pv7z58/nxxD1PQjZ4SlnZRktgwcOBDNzJRYyAeRgZ6UB/9+cftCZHysSpWQaFIKFHxkQvINJjqs1ALjtVtqxvEs6SMEjj4w5bagPV+HrjTdUSpUzw+mV0hq+VMWmJSNpVzOSUxM7RCncOAdnfmSVbw+aiT1Tjh7lr54cCMiIU6tuzXpQ/zq6SLlj2wM7R1n6Sw51ZtihPRdbYrHw8jnfuDrUFRGoeSyeShbDAhws4WpSPbv35+V62L8s+bl7YuRRcp+hjyuAAAQAElEQVR6Fi7rwevG+OpuWdKGINZkddIGS3ZbBa3FIiRt654M3baYl87ltSUYoj2c8pFKfCSSPoi21UmfTtvilQjGBMLI40Sh1bgYdvPs2wuH3ygcWMW60tWIzXOevXwaV7RS9iIlPdV8MoEwrFLJ93ntD5Xe/OLPl6KSplJrtbfPoOTUazenlXKWSnoqmfWfnjTyJ7/NaYlKmoJDD3nYK9XlYy+X/nCnx6RC1l2oyBSycizGut+ehr9OaDU0gMmSNdMe5Mnv/FlPKXblXPHTY7XANe+flwGLsWLSvZYDC+TwZzLkwz6Id8+FF49jZKsORItBgWTAM+lx9XhURFg81MHSBBR33zrvAZM2WdYP4vCWEJdstjE7voUg89LBmf9nzSsmMS6deOPuZe11UmXIp819Y6KlNTtOSrKsH0RkWAJVDyZvlA5c2KtoJjFiIxOcXOV+a6yDwKkfXYtjEibLxmLERidYcaF5iRIXI8RES27ijNgYtULybzb7QIhnCSoSCOn6KuU4FkM6UFCDgl4MAKki6/kgshxNlFQtOQsCAB1ZNh8Ex5OKyP3lKVBgX3rtLI5L7I0GLA7HBGn/1lnmg9DWDbm/PHmFFJsYgsBg1lgJgXHS/q2zzAdBFgQn+4YItS8kqJIcL/CSXiUTWI+s80EIzBYmvrYsHM8ppKeSgppTI4hhLThpN7SzzAchCOkf0WO3UD1MSGBSQ+MeUsAJYQ2sVgkSEhLIFmAZp0CBAq1atXr79i3LOC4uLroB+wbIuotk+qGqKEkLgpQLTghroJVha/zUgiAkmPQu8tBi2rlq4x54hDnTBVVFlex7iwEpQ8piick4IBDpgucFiS1knQRaGFZD2j91XFxcZGQkMzcfbmKglcs0NhgnsbWsNXAS95vZF5wg6bepuPIAMzfpCHMyeCml6oNAgMmKCExyjcw2bdroVgZxdHTMli0bMzcffurpzZlVnQibtaizdNkCZm6+Hz9i2DdfZegU+CAyw5dtGi1Y+AezOhZ6fqSJffog7t2707Z9E2NH27TuVLpUOSYBeAVTSq8npaaJgTaGccz7/HDS/q2zzAdhUW7cvJrG0fbtujJpoO0HITlrHk2MtDHj8yNoJ1plWcEXX3xBTYlbt24dPXrU1dW1ZMmSw4cPT9ma2L59+3///Xf79m1qa5QqVapr16558uSh9C1btqxatWrq1Kk//vjjgwcPChQo0KJFi/r166fz09PT1ZrLaFdrMu3Wr181aEivWnUqhoWHUcqu3Vu/GtC1UeNq9Hfd+pXiRJiLFs+dMnV8SMhzyvb3uhV3796mjRMnjrZq3bBn73YsuYl45crF4SMGfN6sVqcuX8ye84solqdOn6BTLl++oPvoa9evaAr57xhtb9i4hk5p+nnNll82mPDDyCdPHzNTUSiZg6P0XiAcS88Mzvo8fHh//IRvW7Ss1/yLuqPGDL106byYTvHzP+fN7NajdeOm1UeM/B/dBd0p//57ZOKk0W3aNabbN/TrvufOnxbTU94vMnFXr1lK2ejf18P66QonlEoHuh31G1Zp8nmNb78b9C7s3QcvNZ2fu3fvjjr1Kt++fVM8evXaZTp6+Mg/LOn5MfaQnD13iqWbjP/SZkOpVG7cuLFRo0Y7d+6cOHHio0eP5syZY5Dn8uXL8+bNK1269NixY4cNG/b27VtSBPGQg4NDRETE7NmzBw8eTCV8+umnv/zyS2hoaDo/PZ1OyoxB17Rtx8bChYtOm/qHq4vrvv27SAiCihRbuXxLzx79SSBmzZ5O2bp17du2TWc/v1wH9p/+slUHOosSly5fQJbh10NH6xf4+MmjYcO/iomNmfX7oh/G/3z37q0hQ3vTM12+XCX3bO7i0yBy9OgBSqlU8WN6On+fNa1EiTITJvz87Yjxb968pqeNmYoqgcXHSW8sBilXRuIYZIUOHtpboVBMmfz79GlzlArlqNFDYmJi6NDM36fSfWnRvM3KFVtrVK8zbvzwQ4f3UzodnfjT6NjYWPoNJ038NSAgP53y+rVm9r2U92ve/N83b/57wvifR383MUcOvxEjB5IeiR996PC+yMgI+txvho29fPn8okVz0r7U9H9uvXqfVShfefqMH5m2HU4bdes0rP5pbV1Rxh6SMqXLMxuhYMGCFSpUoDZOcHBwkyZNDh8+HB8fr5+B0ufOnduqVasyZcpQzpYtW16/fj0sLEw8Spk7dOhAeaiEunXr0q90586ddH70h5sY5KEUMuifo+vw8PAc2H+YuLtjx6bSpcsNHvQtbWfP7t2tS9+pP0/o2L47bRucRX+pbpNYGBS4b99OB6UDSYOnp2b6+WFfj2nXoenRYwdr1qhbq1b9w0f2f9UvcU0heg7q1GlIdaB48VKLFq719w8gAab0hPj470YPoReXp4ctLHKQPsgOy1BHykePHpBQtvyiHYk17Y4bO/nCxbOks1QPd+/ZRgb5501bUvpnjZrR+3bpsvmkFM7OzgvmrXZxcRF/+eBiJTdvWXfp8nk6ZHC/6Ldd+/dyusuUwjSDCz+Jiop89fol1W3adXV169Sxh3gZx44funjpXNqXmv7PJUgmunRruWPnZvoiJCK//ZLMMUkPg7GHhNkI+ivUUsOBKvyzZ88CAt7PI03fhSwLMhOoiaFbHpXsCN2ylUWLFhU3xLZJ+ntzW8oHUTSouLihVqsvX7nQuVMv3aFy5SpRIj0idLNTnhhUJDhl4pUrF4oVKyE+K0SuXLnz5PGnEkggatast2Xr+pu3rtNDTy7Px48fjvhmHNP+ZE+fPv5j9vRr1y/rnDdv37w2TSCkGeZkGXRSklx6eWWfPPX7enU/K1umQsmSZcqVrUjpZG2RcVGpYhVdTjq6c9cWUU+pni9YOOv8hTOvXr0Uj759+0aXU3e/7t/TvJToNom7pMsTxk/TZStVsqxu29PDKy42ln2IdH4uQUZo9279yH5RJSSMGjUxZfvc2EOSIbKw04n+QAmSTvpr4I/8999/qfVB3oo+ffqQuXH27NlRo0bpZzDZw2opgXBMWmmEnjwSvIV/zaZ/+hnoVZb6iakNGomICL9+4yq1G5OVoDU46VEmS+Tw4f10748cPZAjR0567in92LFDo8d+3aF9tz69BxUqVOT0mf/IH8FMRaJhzgw6Kek5++2X+dt3bKLWBN0OEtmunXuTiU4/Lx0dOKiHQX76hWOiowcN6Vm+XOUxoyaRUUbPWb0GH+vn0d0vsRBnJ+dUP1q040TS87CSZyqdnyvyRYu2i5f8SY2mVMMWxh6SDMBlpUDoy4HYJBRlQgc5F0qUKNG7d++U+TNJOnpSZm4iHfom5HqtX69x9eT2Qp7cGViHxNvHt1SpsuSz0E+kFxHTPm1kQFJzg7wb1Lakd6N4lJwgdAolirvi42syvEKQYJiTZTzMSQZ/v76D6Zc8e/Yk2QiTJo8NzF/QxzcH0xjqo/LmTbY2cs6cubZuW08ST44AsvZZ8ne4AW5umvc2vfaZOTh4aG86P1eEnKO5c+elV9G8+TPFxqw+xh6SDKDpJpVlr4iLFy/qtsl9QGorRih0hIeH58yZk5zEYruJ4h3MTKTHbs5sKK1QoaDwiHCyZsV/JUuU8fH2zZkzAwtVFSpYJDT0OXmVdIVk9/IWG7dE7Zr1Hzy4R27tW7dv6O59WNi7HL7vl3g9ouejMgG1ipNgmDOj/SDIZUiiwLSqXbVq9e/HTaFH7ebNa/55A0QjVvfz5g8sGBhQgJSdfkZ3dw+xljKNr3G/scLJJ02lkVND3CVPGEUrdu/exkwi/Z9L3L9/d8nSeeSZ+ubrMeSquHr1Uso8qT4ktsKrV682bNhA9Z8cDTt27KhRo4bB6GxqVpw5c4bCnORRopxiYkhICMs0HxYIrZMyU3WjV48Bx44dJB8SuR6ouUsRx6HD+tL7gWlbxdTCPHr0IPnP0iihVasOdC7FPsi+opwUkOves83de7fFoyVKlCa5oaBpwYKF8+cvKCYWLhRE8S2KjdFPRjFUMfF5yDNmR2R0pkyqdVOnTZgz91eKCtHPuGLlIvpxSK9JCLp26UNeSdEZQbWRYka//jaZaZ68InSDqAFPOf87eZzsDvIEkVinLJxa/lTxKIpBGkQ/O4WQ6IkNDi7JTCL9n0sPxo+TRtWt0yi4WAmyGevUbkBmUcpRz6k+JLZCw4YNr1271rhx4169epFvsl+/fgYZunTpQsGLadOmNW3alEKYFOkMCgoaM2bMgQMHWOawRkcpum3z5q6gx5EqdkxMdInipX/8YYYogR9/VI3cV2PGDevSuTdFp4yV4OHusXDBmtWrl/Tp15Feg+QJ+2bYGNEVL1KzRj1yoesaFET37l+RuTt6zNDo6GhqoJKx+uzZk29H/m/Udz+yjKOZ9l56Pu+MWhDU8B465Dtqq9NvRbsVK3w0Y/pcsbZQvJkMvZWrF1NVpMYC3aOvv9ZELqm+PXhwl7Tjl19/osDBiOHfkzG/ctXi8PCw1l92NCh/0P9GkKxMnzGR3nUk0BO+n6az8jJK+j+XnquQ589mTP9T3B3Qf1iHTs2WLV9g0CBlqT0ktgJ59AycjiJr1qwRNyhaMXLkSP1DM2fO1G03atRIt00vg127drF08+HFe5f8cJ9eU60G52cyZtWUex4+yrZf52NSYsHou25eDk16Seuq7JIl399u3DN3gRJuzMKQG+Xdu2S9yFq3bt28efP27dunfSJVZDKmTIvdurm56VpzBqSjoxQ6/Gt+BIHnJThprcTHB9gX0u7WnmVjMbBmjBZOghOGqFVMgrNUpBPyd3w3arCxo8uXbdJ1e5E5a9euTU82C80HkQ4fhMBgQmhWoJBgPwhbNu7IM7Vy5VZjR92zuTOQERy1MHODSWttGRsfzQkVMCOZ8UGkwYdtEp7neAn2MgaaMaaCA5YUBlqyzgchTevauvBKTuHApIYqgYuXXvct+4Rac1Z5TyqVSh8fH5ZxTp069d9//w0YYMp4gjRCmekRCDyCTJ0gqOKZ1NAsnAPjzjpQa84q70mT41KVtTCTSOND0zdhDIJpkkSaa46DLCHL1ubkEMQQe1JK711NDR8lvMxAS5atzalWCVjeTVBLb85zuiq6NbAggJZs2bLlypWLmZt0vIC4LJuNTzpoe4sxqaGZtBYCAbR8pIWZG/i4bBj0ggc6sswHoXDgpThXinVROnAOjpITU4WSLgwSbw04Bc8pJf1TW8gH8eHv7ObuyASbmd7TQlAcxzWb5PyBTi4OTA2BsAY8J/jldGUSxkI+iA8/XkHlskVFxjF5ExOlKl8rO5MY+YJc3r3+8OyvIJNcOPiWLDUXbyZlyAExZMgQZm4+LBAlqmZzdFbsX5nelTbsj23zn7hnV/rlN/9ImEzyaQsfcp+e3v2GAUty48y7YpWkvlpClvkgiG7jAt+GRG+f/5TJjLhotuH3hxwTOo4MYJKk18T8ty+827vM/E8GIB5cjVkx6W6V4PIDsQAAEABJREFURt6ftpC2/WAxHwSX/p7Uy396FPY6TqHg4+MMJyHgtLMl6EJuomtdUzDHdOtyUaL4UZz2f7T9PoXTXIZuV8zE6c3Q8b5AvV5b74/yiR+tKUHzH6f7cpoQrXgWryld86HazFzSBejQZRMnv6ANcs2qVYKPn3PrYXmZtFn648PIsATyo8UlvzX6P6km5MFrvlGyDMl/Df2bopcp2b0Q4TlOrU3Sz89T+UlTh7y/ufz7Uad6d5zpnr3kianmNHhaEjd4ReLXMTikewwMvj5L7YnSu05Of94ThaM2RiRwRSpkq906B5M8JBDHjx83eyuDy9hQCxU7f/Rt+JsULgmDn5+xVB63JLHQ3DjNhqBL0fa0SJZZuxyoWnfqjes3HZ2dCuQPfP9Z2mcgqWAucUfzSNA3UicV8v7bUYGCuPxq4oWJn6+REKbRJl48i2e82CWK43n37I5lPvVgNkJcBLt8IjwyIjp58nuFFldYFQw6Tmh/jcQfKnWF0P7anGC4AON7VdZ7BzDtj2zwyboXAjOoyizp9qelEHr3SPeCSbya91Va/+UjKob2dZDsu1ApPNN7wMRnUEj2QXpfUqlU+ORxCSovacekFeBsYizWtGnTAgIC2rRpwwAAqRGhxeyBDNsIkiUkJCgx6gAA42TZWAwpQAIhrukMAEiVrBuLIQFgQQCQNrIeiwGBACBtyAHx9Kn5OyJAIACwB8gH8dtvvzFzgyYGAPaAu7u7wZLfZsE2al18fDwEAoA0yMyclGmAJgYA9gB8EBAIAIwCHwQEAgCjyN0HgY5SAKQBfBCwIAAwCnwQEAgAjAIfBAQCAKPABwEfBABGgQ8CFgQARoEPAgIBgFHgg4BAAGAUWfsgIBAApI3cfRBwUgKQBrL2QahUKoVC7sv/AZAG8vVBkPkAdQAgbeTrg4iPjy9evDgDABhHvj4Inudv3LjBAADGka8PguIX1MpgAADjWMgHYQMCQQ4ItVptEyuAAZBVyLofhGhEINIJgDFk3Q+CpIFclQwAYARZ94OAGwKAtJH1WAwIBABpAx8EBAIAo8jaBwGBACBt4IOAQABgFPggIBAAGAU+CAgEAEaBDwICAYBR4IOAQABgFPggIBAAGAU+CAgEAEaBDwICAYBRLOSD4KQ8jLpevXokDRzHvX371s3NzdHRked5Stm4cSMDAOixf//+PXv2TJkyhZkVSTcxsmXL9ujRI3E7Li6O/qrV6nbt2jEAQHIs5IOQdBOjRYsWBtPV0k/Qtm1bBgBIDjkgBg0axMyNpAWiffv2/v7++ikVKlTIly8fAwAkR479IMjd0KpVK3I9iLt+fn4dO3ZkAIAUyHROSvI4BAYGitvlypUrUqQIAwCkwEI+CElHMUS2bt06depUclj+/PPPJUqUYAAAa/EBgXh8M/rQhhdRYaq4GLWYQrm598eT9jjtpi5VECg2+X6f/IyqxJwCU3PJzRbKqLkETmACl7zw98UKglpzSFMol/zytVkEvXKobHXyLDwT1IZXqL+rOVHzH5fi2yVlUDAKsGbzcmzcNa97DgaABCEfRFhYmNmNiLQE4ubpyH/WhfjkcsoZ4CqoVIkncO9PoTrFJZ2tv21QG3kFp1YLiSmGFVUjJe8rcGIt1535vrZzvPZzk18sSQovnp50YZps6mSZqHoLqhQSRqfpsumpjOEFiFfhwAvx7NmDmDeh0S36+ecq4MQAkBjW7gexf9WLOxcjOowsyIAeKybdLV/Tu3IjLwaAlLC2D2L2N3c6jSzEsGhuckLux+5d8aTfVOgmkAWpRzF2LgpxdlNCHVLil9/JwZE/uv4tA0BKWLUfRNjreGdXyEPqODhwr15EMQCkhFXng4iJSlCrGUiV2Fh1dKSKASAlZD0fBAAgbWQ9HwQAIG2sOxbjfccAYAjHM57nGABSwrpzUop9lkCqqKGdQHJY1QdBL0nIgzGE1HpbApC1WNUHIaiZgCiGMQToA5Acsl4XQ1KQAcHjZwMSw6o+CKoAgoBGRuoI0h8hD+SHVX0QajVDLTAOh/YXkBroByEVtGFOBoCksKoPQqHkFAo0MQCwGazqg1AlCBiLYQxqX+DHAVLDuuticBbsBxEVFTVp8tjGTasPHzHg7t3btepUvHTpPMs06zesrlPP/G2wlHC8gCYGkBrWXRfDcG43c3Lp8vm9e3d069q3d6//scxx796dtu2biNvFg0t26tiTWR5BzcGCAFLDfvpBREVF0t+6dRoVLhzEMseNm1d128HBJbt26c0sDzkpFbAggMSwqg9C4cBxGVxMu1mLOp079jx89J+LF89t3vSPh7vHrt1bt2xdf+/e7QIFCteuVb/lF+04jluw8I8VKxdR/hYt61Wq+HHfPoP1C0n1FPHQv/8e+e33KS9ehBYuFNS8eetGDT9ftHju0mUL6BA1Ur7qN4TnFbPnzNi/96SY/9ixQ0uWznvw8J6np1fhwkUHDRzh55eL0pt/UZeMl3fv3tJRFxeXShWrDOg/zMfHl6Ub8kGoYEEAiWFVH4QqXlCpMtbIcHBw2LZjI1XFaVP/cHVx3bd/15Sp44OKFFu5fEvPHv3XrV85a/Z0ykbbY8f8RBsb1++dOmWWfgnGTmFadRgzbliP7v0n/zSzWrVaU6dNoMxUz9u26UzV/sD+01+26qBf1Okz/439/pv69RuvXb1j3JjJISHPfp05WXeda9Ys5Xl+08b9Sxatp/bO4iV/MgBsHAv5IIxOGJPRflL0qvfw8BzYf5i4u2PHptKlyw0e9C1tZ8/u3a1L36k/T+jYvjttGyshjVPIWKj+ae16dRvRIbI7IiMjxHaKMf5aNIfyt2rZnrbJgviq39Bh33x1/cbVYkWLU0revPk6duiuyZfNnSyImzevsYx9U4Hj0IsMSAsLrYthtDFtwmjvokHFxQ21Wn35ygWqe7pD5cpVosSLl84ZOzeNU+jvnbu3ihV7v6ZW3z6DPm/akhnnbvL84oVdv35F3A0KCtYdcnf3ILlhGUEQOPRDB1Lj7Nmzy5YtY+bGnFPO6VbZjYuLi4+PX/jXbPqnn+HNm9fGzk3jlJiYGNIIJydnlj5ISmNjY/Xzu7q6siTnKNMaOywTaJyUmNAXSAxnLczcGHVSMpXpVjRdKNXJ+vUaV69eRz89T25/E05xcnIil0H63/PizxQTE61LidRKg493BjyRaaBxUmLOWiAxLDQWw0hPyvjM9qQsVCgoPCK8XNmK4i5ZB8+ePcmZ08+EU+iFX7RocfIm6nLOXzCLLI7+Xw1NtRylUlk0KPjKlYu6FHG7YCGsDA7sFuv6IBIXsjWdXj0GHDt2cMfOzdQ6uHTp/IQfRg4d1pdqtWmnNGva6tSpf9esXXbu/OnNW9atWr2kQIFClO7vH/Dq1cujRw8+evRAv6gWzdscPXZw/fpVYeFhdAqFP8uXq1SkcFFmLuCCABLD6nNSZq4SlCpVdt7cFStWLvpz3kyy9ksUL/3jDzOosWDaKQ0aNAkLf7dk6bzIyEgfH9/evQZ+1qgZpX/8UbVSJctSBLRL597kbtQVRQHOFy9D1/y9jAKlFAetWOHjXj0HMDOCIAaQGFZdm3PJD/epidFqcH4GUrB66j1PH2XrofkYAPYO+gxnGAEGBJAeVh2LwfEcB+kwBiatBdLDuj4IppndnYHU4Bh6UgLJYdU5KQU1pqQ0isDQkxJIDqvOScnzmlYGA6mBOSmBBLGqD0Izq7UaJkTqYMo5IEGs7oMARuA4rFsKJIeV1+bkeBgQRhEgEEBqWHldDEFAsN8IWiclA0BSWNUHoVm8F3XAGOgHAaQHfBAAAKNY1QcBALAtrNsPgmPoam0MBccpoKtAYljVB+GSzcFBiUqQOrwj5+LmwACQEhbyQaQuELkLuUVFZnBhDNkQF60qVNqDASAlrDofBDH/u7slq/mU/MSTAT2Obnzx/F5kt/H5GQAywKinodekgucPvTy//zUDSRxa+/LRzQioA5AgFvJBcGkN21SxBePuqVTM2UURG2NkImcu2fQpHKfpI8BpSk3W2ZDjki8InHQWl2LyFY4XBLVmNgpBrTs3WWnirvhB70vQFag9Xe9zBPK4ikWJWfSv0PBqNWuaJ25rfLTqxGvjeaZ0VMREJTg4KnpMCGQASI/9+/fv2bNnypQpzKyk6YlUsJ4/FrhwJPzh9YjIsFR6F2uqHCfozxxBdUmtpr+CWr+WagcvCEy/zidV7yQh0CkFz3NqtSCWk5iZF8LehfM87+aWTZtBU7hBCbpdOl0Q3oue+NFUlOZqtDn1r1A8l1dwau0c/xoVI7XQjlKjbJr+UNpvoVAwVzeHoLI5ilRyYQBIEmv7ICTFzz//7O/v37ZtWwYAsCK20dshISFBibArAMaxaj8IqREfH+/ggK4HABhF1mMxYEEAkDayHosBgQAgbaw8H4S0gEAAkDay9kGQQMAHAUAawAcBCwIAo8AHAYEAwChy90EoFAoGADCC3H0QsCAASANZ+yDQUQqAtIEPAhYEAEZBPwgIBABGgQ8CAgGAUeCDgA8CAKPABwELAgCjwAcBgQDAKPBBQCAAMArGYkAgADCKrH0QcFICkDay9kGoVCpYEACkgXx9EKQOGKkFQNpYdW1OSUEOCA8Pj2vXrjEAgBFkvS7G+fPnp0+fHhcX16RJk6ZNm3p5eTEAAGOvXr1auHBhcHAw1QtmAWxDIETu3LmzTUvJkiUbN25ct25dBoAsiYyMvHDhQtWqVTdv3hwbG9uyZUsLNcNtSSB0HD16dPv27fS3iZYSJUowAGTDw4cPO3fuPHz48M8++4xZGJsUCJGYmJitW7eSQREVFSU2Pby9vRkAdsqsWbMOHz68du3ad+/eeXp6MqtgwwKh4969eyQTJBbUEqOmR/369RkAdgGF8KgR8dFHH+XNm3fVqlX0eJPDnlkRexAIHceOHaOmx6FDh8iaIJuCXBUMANskJCTEz8/vm2++Ibt46NChTk5OLCuwK4EQIZ8NWROkFOHh4aKTwtfXlwFgI1y6dGnkyJEkDTVq1GBZjR0KhI779++TTJBYFClShGyzhg0bMgCkCjWTHz161K9fv8uXL+fIkYPMByYB7FkgdJw4cYJ+/f3791PTg5SiTJkyDABpQPZCqVKlbt68uXLlSopNFCxYkEkJWQiESHx8vNj0ePPmjRj1IJ1mAGQF4gCCrl27urm5/fHHH0yqyEggdFAYWYx6kFqTUjRq1IgBYC0uXry4YMGCgQMHUsv38ePH/v7+TMLIUSB0/Pfff6QUe/fuFZseZcuWZQBYhuvXr4eFhVWuXHn16tUBAQFVq1ZltoCsBUIkISGBZIKaHi9fviSZILGQiH8I2AExMTHOzs779u1bvHjxqFGjgoODmU0BgXgP+ZDFqEdgYGBjLQwAUyFpGDt2LPkapk+fTraDlTs4mQsIRCqcPHmSlGLnzp1i06N8+fIMgPRBboWNGzeSi4EMUnI31K5dm9kyEOwYpYAAABAASURBVAijqNVqMeoREhIidrjKnTs3AyA14uLiyEzw9fXt37//J5980r59e2YXQCA+zJMnT8Rh5nnz5hWVggGgx/r166kdsWrVKmqcMvsCApEBTp8+TQYFKQVpBDU9KlasyIBciYqKWrp0qZubW6dOnS5fvmyvA38gEKYgGhRkWYgGBVkWDMiGf//9t0qVKocOHbp161a7du1II5j9AoEwnefPn4sTUuTMmVPsmsnzhnN8NmjQgCLe48aNY8DGoZpCVkOtWrX69u3bvXt3Jg8gEGbg3LlzYtfMhg0bklLoL0/w8ccfK5XKFi1afP311wzYJhs2bFi2bNnatWvJb+3g4JDyNWDHQCDMyY4dO0gpHjx4IDY98uXLV6FCBY7jyAolt3afPn0YsB327dtHcasSJUqsXLny008/pbvJ5AcEwvxQWFTnpKB3jpjo4eHRu3fvtm3bMiBtQkNDqc3466+/UhNyxIgR2bNnZzIGAmFBRPNBt0uPHQXJ0UFTsjx+/HjYsGHNmjUj1yNWexSBQFgK8js8evRIt1unxDceLnldHD2cnZ31nzwSEIM7wHNMLaaQtmg3eKWgTuCSZUo6pI/CQVDFc4aJCqZSGebkNLedS1mOQiGoVIYlaC+AqRO02XkmqA3KEpiQyimasxRMrUrtFPE8XhDUqZ9IvwnPp34lqXyF90mGv0kqeVJDLcTGJER/XDO/b9HXdELhwoUZSAICYSkoeBETE0MWRE7PIo3LTXB2c/DycYqNNaysVBPUyeuJgmcqbY3SaYdSySUkJLtNqT76SgcuId7wbiqVLCHBIO29ahjIk0LJVCkya9M5lfYCyD2nVhtcv2GKQWnGMqRxIl0Vx3NqVVpPpkLBqVQGv4mh1KYqjilxdFQkxLOIN7FKB77beHvr6ZRJIBCWgloTgYGBOVxLxjws1qSXv1dORwakzY6/nsVGxHceE8BAEhAIyzJ3xN1abfzzFII62Abb5z+lFl6bYZKexMWayCiia332r36ldOSgDjZElSZ+r0NiGUgCAmFBXj6LcXaFJ9yW8M6tIKfrg2txDGhRMmAxYiLijfnhgGQhv2ZsbDz5LhmAQACQCh+OjcoFCAQAhnBw3CcBgQAgGRzienpAIABIhqDpp8WACAQCgBTAhkgCAgGAIdAHHRAIAAxBC0MHBMKCaBqzeNZsEFgQOiAQFoTc4fCI2yJQdR0QCAuC58w2ETCCUQcEwoLgKbNNOI6HticCgQDAEBgQOjCa04JwvAWdlFFRUZMmj23ctPrwEQPu3r1dq07FS5fOs0yzfsPqOvUqM3kD+0EHBMKCCGoLvosuXT6/d++Obl379u71P5Y57t2707Z94oKjxYNLdurYk0mYjZvW/jTFsgsRwYDQgSaGrRIVFUl/69Zp5OWVnSwIlglu3Lyq2w4OLkn/mIS5ceMqsyScwDg1JCIRCIS0aNaiTueOPQ8f/efixXObN/3j4e6xa/fWLVvX37t3u0CBwrVr1W/5RTuO4xYs/GPFykWUv0XLepUqfty3z2D9QlI9RTz0779Hfvt9yosXoYULBTVv3rpRw88XLZ67dNkCOkSNlK/6DeF5xew5M/bvPSnmP3bs0JKl8x48vOfp6VW4cNFBA0f4+eWi9OZf1CXj5d27t3TUxcWlUsUqA/oP8/HxTeOrkYr16NX2p4m//jzjRxK1BfNWpVF+o8bVunTu3bZNZ/HcqdMm3Llz88+5ywcP7X3hwllK2bNnO+0GFSl25cpFKuH69SueXtmrfPwpnSUuljnu++EKhcLPL/fqNUvHfz+1+qe1WfoQOCYo0MhIBE0MaeHg4LBtx0aqKtOm/uHq4rpv/64pU8dTNVi5fEvPHv3XrV85a/Z0ykbbY8f8RBsb1++dOmWWfgnGTmFadRgzbliP7v0n/zSzWrVaVOsoM9VzqodULQ/sP/1lqw76RZ0+89/Y77+pX7/x2tU7xo2ZHBLy7NeZk3XXuWbNUp7nN23cv2TRemrvLF7y5we/Gv1dunxBm9advh46Ou3yjfHrjHlk4NApdLX0HR8/eTRs+FcxsTGzfl/0w/if7969NWRo7wTtNN70cXfv3aZ/E3+YUbpUOZYhYEAkAQvCgiiUHJeOadf1oVe9h4fnwP7DxN0dOzaVLl1u8KBvaTt7du9uXfpO/XlCx/bdadtYCWmcQsYCvUjr1W1Eh8juiIyMENspxvhr0RzK36ple9qmN/xX/YYO++ar6zeuFitanFLy5s3XsYN2Ddts7mRB3Lx5jX3oq4mfq5OhtMtPD/v27XRQOpA00Om0O+zrMe06ND167GDNGnXp454/fzp39jJnZ2eWUWBAJAELwoKoEgSVKsMvo6JBidVDrVZfvnKB6p7uULlylSjx4qVzxs5N4xT6e+furWLFSugO9e0z6POmLZlx7ibPL14YGfPiblBQsO6Qu7sHyQ1LB0FFgtNZfnq4cuUClSCqA5ErV+48efx1v09gQAFT1IHBgngPLAgLk/EwhqNj4myIcXFx8fHxC/+aTf/0M7x589rYuWmcEhMTQxrh5JTeChMREREbG6uf39XVlSU5R1mSRZBRHJ2c0ll++i4ynCwO8p7oJ755/crgs4DJQCAsTCY6QtDbj+pM/XqNq1evo5+eJ7e/Cac4OTmRyyCd73mxKPobExOtS4nUVl0fb19mDjJUvkqdelPN28e3VKmy5EbRT/T08GLATEAgJE2hQkHhEeHlyia+Ick6ePbsSc6cfiacQi/8okWLkzdRl3P+gllkcfT/amiq5SiVyqJBwRQj0KWI2wULFWHmIO3yHR2doqOjdIcePXqQaiGFChbZs3d7mdLlSfvElPv37/r7Y2ksswEfhAWhh5bL3A/cq8eAY8cO7ti5mVoHly6dn/DDyKHD+lKtNu2UZk1bnTr175q1y86dP715y7pVq5cUKFCI0qlGvXr18ujRgwb1sEXzNuTwW79+VVh4GJ1C4c/y5SoVKVyUmYk0yi9evNShw/upGULby5YvfPkyVHcWOUevXbt89twpaje1atWBviaFaagBRRf/57yZ3Xu2ocgFA2YCFoQFUatTX9g6/ZD9PG/uihUrF9GjT9Z4ieKlf/xhhlOaTes0TmnQoElY+LslS+dFRkb6+Pj27jXws0bNKP3jj6qVKlmWIqBdOvcmd6OuKIomvngZuubvZVQDKQ5ascLHvXoOYOYjjfIH9B82ffqPTZvVJEODwqJ1ajc8ezaxa0bTxl9QxOSb4f2nTP69YoWPFi5Ys3r1kj79Oj58eJ8clt8MG0PhTwbMBNbmtCBLfrhPGtFqcH4GbIfF42836JwrqGw2BmBBAJAa6AiRCATCgmgiGHJ60sjl8d2owcaOLl+2SddhQepAH5KAQFgQToOMWnAa98e8lcaO2ow6MHSUeg8EwoJo/DuCvF5GuXPlYcCOgEBYEO2ktXgZARsGAmFBMO29LcJheU49IBAWBNPe2yKCuDwn0AKBAAAYBQIBgCGY9V4HBMKCcDyZqmhj2B6YklIHBMKCCGohk2MxAMhaIBAAAKNAIAAARoFAWBBnV4eEODRnbQwHB87JyYEBLZgwxoL45nGKiUpgwHZ4/UwlqLnAYEcGtEAgLEiddr7xcepnd+IYsBGObwvx9oM6vAcCYVkads69f/Xjt6HQCBtg56KQuOj4NsP8GUgCM0pZnFeP4//+/ZGTi8Ld2zE+NkWLg9N0/je8CWJHHUEzpaUuUMpzRuLz3Psxo5r8QuJoZY6nIGsqPX54BdOfI5rnNVPj6Rel/6FimZSs/9G6DNoZNzkhtcvSnMU4tfaQODGn7hTa4PhkZyX7RMqsTn5hnN7466Sj+teQuKHkhASBMnBqTXbdSYkZOEEz9l7/GpIyODop4hOEsFdxSiXXfXx+BvSAQFiJ7X89f/M8PjoipUBQ5U4xOIgSBc1MEvo1x6Devs/LvdcXcQyBuMvzgjpVgVAK6oT36cmL1XxmSoEwGLdOVVelElQJCY5ODprhJql9ilYUEg+lFAh9UWNGviav4NTaZYeSfUH+fZlCakqh+SztEBjdWUkZBJ7n1Po5kzIonJirm0OBYLePGmdnIDkQCGAKV65cmTp16pIlSxiwaxDmBKbg6elZt25dBuwdWBAAAKMgigFMITQ09MiRIwzYOxAIYAq3b99et24dA/YOfBDAFHLkyFG9enUG7B34IAAARkETA5jC48ePT548yYC9A4EApnDx4sXt27czYO/ABwFMIW/evB999BED9g58EAAAo6CJAUyBwpwXLlxgwN6BQABTOHHixIEDBxiwd+CDAKZQuHBhf3/Mm2D/wAcBADAKmhjAFK5cuXL9+nUG7B0IBDCFf/75Bx2l5AB8EMAUihcv7uHhwYC9Ax8EAMAoaGIAUzh37tzdu3cZsHcgEMAUtm3bdvnyZQbsHfgggCmULVu2YMGCDNg78EEAAIyCJgYwhf/+++/JkycM2DsQCGAK27dvv3XrFgP2DnwQwBQqVKjg5+fHgL0DHwQAwChoYgBTOH369IMHDxiwdyAQwBR279599uxZBuwd+CCAKVSqVMnHx4cBewc+CACAUdDEAKZw6dIlhDnlAJoYwBQOHz7s6upapEgRBuwaCAQwhbJly3Icx4C9Ax8EAMAo8EEAU7h+/Tq5IRiwd9DEAKZw5syZ0NDQUqVKMWDXQCCAKQQHB+fKlYsBewc+CACAUeCDAKZw9+7d06dPM2DvQCCAKVy7dm3r1q0M2DvwQQBTKFSokFqtZsDegQ8CZICOHTteuXKF4zSPDc/z4sOTJ0+ebdu2MWCPoIkBMkDv3r29vLxIGhQKBckEbdDfqlWrMmCnQCBABqhevXqxYsX0U/Lmzfvll18yYKdAIEDG6Nmzp7e3t263ZMmSGLJlx0AgQMaoUKGCrgNl7ty527dvz4D9AoEAGaZr164kDbRRtGhRsiAYsF8QxZA6Rza9ff08KjZKpUvhFUz9fo9xCiaoxHROezMFQS/+yPEs2S5H/2NpZUjaFTdS5hcPPXr0KCI80j/AP5trNk2KkhMSDB+k9yXTa4g2eIGpUx8hrsnJxDzav8YvjK6F0ddUpVaIgrm4OQSV9giq7MqAmYBASJdLh8KO7XipcOAdHbnY6Pd1heeZ2kiVJnGg2IJa/f6eUg03uMMpJUM/g25X3KC/9E+tTr2E95kVgqAyrPwGRaUlENpkgzypX5j2eoTUemBQREWp+aFUDk589/H5GTAHEAiJculo2PGtL+t19s/h78hARji64eWjm2G9f8LawmYAPggp8uKh6uiWF+2/Kwh1MIFqX/gGBnstHHufgUwDgZAie1Y+9c3jwoCpVG3mnRAnXD4RyUDmgEBIkajwhNwF4GnLFE6uirsXwxnIHBisJUXiYtWMx1CoTKGKV0dHxDOQOSAQUkRQCwL0IXNQ5EVQ4UfMLBAIAIBRIBAAAKNAIKSItv8ilqUBWQ+iGFJE03kNHdgyh0D/QWMzDSwIKcJ17fCaAAAQAElEQVQxBnnIJJx23AbIJBAIKaIdcwBA1gOBkCQcFAJIAgiEFIE4ZB6Op3/4ITMLBEKKaJ2UDGQKTTsNP2JmQRQD2CcUxRAEWBCZBRaEFBHw6ss8pA5q/IyZBRaEFOHt2guxfsPqOvUqM2ALwIKQIvbdSap4cMlOHXsyYAtAIIC1CQ4uSf8YsAUgEHbCif+OrVmz9PqNK97eviVLlundc6CPj++161e+6t9l9h9LgouVELN17NS8atUaX/UbsnHT2mXLF0ydPGvUmCGvXr0MDCzw9ZBRb9+++Wny2ARVQqWKVYYO+c7LKzud0vyLul279Hn8+OH6DasopcrHnw7oP2zS5DHHjh3Kly+wY/vu9es3pmwRERF/r1t+8tS/9+/f8fH2pU/p3q2fs7MzHRr3/XCFQuHnl3v1mqXjv5/64kXo7Dkz9u89SSWMHvu1wRdZtmSDv39AQkLCwr9mn/jvaGjo85Ily7Zo1vrjj6sxYHXgg5AmGXPA37x1feR3g8qVq7T4r3X/Gzj8zp2bU6Z+n/YpDg4OERHhi5f++fPU2Vs3H4yPj580eezOXVsWzF+9YtnmS5fPr1m7TJdz9ZolAQH5d+883rNHf8ozZGjvOrUb7t19olbNetOm/xAeoZm4acPG1StXLW7TutOkib/26TPo4KG9S5bO05Vw995t+jfxhxmlS5XTXQMJ2Yzpc3X/ChUqkssvt49PDjo08/ep69avbNG8zcoVW2tUrzNu/PBDh/ezDMEJAp7uTAMLQopo1sXNSP7Ll87Tu7pjh+48z/v55SpWtDjVxg+eRaLQpXNvsgJo+6PKn1ANn/nrAm9vH9otW6YCqYwuZ5HCxT5v2pI2atao9/P0H0uUKE3SQLu1atZfumzBwwf3KKX1lx2pJpMlknhJly+cPHW8T+//iV/n+fOnc2cvEw0KHZ6eXuXKVhS3N29Z9+TJo1kzF7m4uMTGxu7es619u67ih37WqBmVtnTZfCqfpR+B4zBfTKaBQEgRQciYn7JkqbIxMTEjRw2uWOGjKlWq++fNp6t4aZM/MHFueFdX1+zZvUV1IFxcXENCn+uykfkgbri5uWnOyl9Il43+hoeHMa2ZcOr0v5OnjLt95yY1ECiFCtSVEBhQwEAd9Ll9++asP34e9d2PZETQ7s2b1+Li4qiZo8tAgkWWS2RkpHgB6UGzogd6UmYaCIQ9EFSk2OSfZh4+vH/e/N9nz/mlQvnK5DUgA/6DJ3J6s05wxmegMDhEdkrKPPTRO3ZsosYFVWyyYhYs/GPHzs26o45OTswIYeFho8cObfb5lzVr1BVTIrRtloGDehjkpPT0C4T2utEPIrNAIOyEjypXpX/duvY9c+Y/8iZ+N2rwhvV7U2YjBySzAIIgbN22vlXL9k0atxBTxEqeHn788TvyX/brO1iX4uOrcUN8PXRU3rz59HOKTtN0XxJLdYU+kCEgEJJEYBlyUp4/fyY2LpYEwtc3R4MGTXLlyjN4aO/nIc+cHDXv7ejoKDEbBRpevnzBLAC5M6Kjo319c4q71EA4/u/h9JxIfk1ylyycv5rCHLpE/7wBTlqLQ9dQevPmNWmQk3EzJCUcJ3AKeCkzC35BKaIZiZgRgbh85cL344dv3baB4pRXr10mdyMpBUUEyAHpns2dTH2qXeQXmDx1nLu7B7MAjo6O5KcgN8GTp4/fvXs79ecJpUqWJd8EeQ3SOOvChbPzF8xq26YzacS586fFf6GhIeQQoSYSeSUvXTpPWkPxi2HDv/r1t8ksI1AcSFChiZFZYEFIEY2HMiMeeIogkDSQn2/GL5Oortau1eCXGfOUSs3NHTPmp99mTqldtxJJRp/eg16/fmWh1VjHjJr0x+zpXbu1ImfkV/2Gli1b8eTJ4y1a1l2yeL2xUyhUQX//mD1DP3FA/2Etv2hLqlGoUNDK1YvPnj3p5patRPHSX389mgGrg8V7pcisobfL1PAuW9ObAVNZPfW+uxfX9ptABjIBLAhJgvAckAYQCEkCqy7TaPpBKCC0mQUCIUWwJkbm0YY5IbSZBQIhRfBcmwfobKaBQEgSKESm4TAzuDlAPwhgn6ipjaGGQmQWWBDAPuGwPpk5gEAAAIwCgZAisIyBRIAPQpJAITKNZkoNhIszDSwIKYLu75mH04znxO+YWSAQAACjQCAAAEaBQEgRR0deifkUM4ejC+fk6sBA5oBASBEHF8WrFxaZG04+xMcI2XM6M5A5EMWQIgWKZ3t+N4IBU3nxOCE+TlXzS0yokVkgEFKk5pc+ji6KjbMeMWASe5Y+LF/Dh4FMgxmlpMvGWU9fh8Tm8HfzzuOYEJ9miyOtXsXvj3GaeeAFlt4+yGlmEzhxUvmkMlM5lFSMpjeC/mNmWC6nPUWXluywoOsTwnHvn9VkWZJ2FDyvVvHP7ke/CY36vFfevIXRvjADEAhJc2jd67tXw+NjVHExad0mqoJ0G41UaP06ps3GJfaz0NY4Y65Qg1ounm/wAUJS3eeSX0yyFO3HCfp9v/QzaMVB+1m6krn3U3onfS3tNi8I7wdf6RWYlJ8+yNGZz+bpULdd7hz5FAyYAwgEMIVr165NmjRp2bJlDNg1iGIAU0hISBBnzQb2De4xMIX4+HgHB/QysH8gEMAUYEHIBNxjYAoQCJmAewxMAQIhE3CPgSmQQMAHIQcgEMAUYEHIBNxjYAoQCJmAewxMAQIhE3CPgSlAIGQC7jEwhfj4eAiEHMA9BqYAC0Im4B4DU4BAyATcY2AKEAiZgHsMTAGDtWQCppwDpgALQibgHgNTgEDIBNxjYAoQCJmAewxMAQIhE3CPgSnASSkTIBDAFGBByATcY2AKEAiZgHsMTAECIRNwj4EpZM+e3cnJiQF7BwIBTOHdu3fR0dEM2DsQCGAK1L6gVgYD9g4EApgCBEImQCCAKUAgZAIEApgCBEImQCCAKUAgZAIEApgCBEImQCCAKUAgZAIEApgCBEImYEYpYAoQCJkAgQCmAIGQCWhiAFOAQMgECAQwBQiETIBAAFOAQMgECAQwBQiETIBAAFOAQMgECAQwBQiETOAEQWAApI/GjRs/e/aM4zi1Wk1/xURfX989e/YwYI+gHwTIAD179syWLRtJg0Kh4LXQC6Z06dIM2CkQCJABWrRoERgYqJ/i5+fXrl07BuwUCATIGJ07d3Z1ddXtFipUqEKFCgzYKRAIkDHq1atXpEgRcdvT0xPmg30DgQAZpmvXru7u7rRRuHDhatWqMWC/IIph/0SFs0c3IsPfxMXFqWiX5ykGIYh/OXoAGD0AFJAQmGaHU2s2tLuaRDqqFsQEbYr2nyZl1+5dz58+r1GrRoH8BeiooNY8SprCOW0J2meKZ9ptLWLIQ/ewiZ+uu0IFr3ByVmTP5RQYjLU2pAUEwm7Zvez545sxMdFUSdWJN1rQ1FKOIg9qURk48T9Nbk31TkxKPD/x/7VncfoFiylUnsBxfKJuaJM1f8S84jNF5qmavS+Ne7/LaU5/Xyiv1H68mqnUKoWSd8/uUKaaV+nqHgxkNRAIO2Tl1EdvQuIUjryLu3P23Nk8crkyG0EVx14+ehsWGhkbEcfxXLEKHnXa5WAg64BA2BXb/wq5dyXC2c0hX+lcTm4KZsuE3n77+kmYOkHdckCAX34HBrICCIT9sHDc/fhYoVi1AGbbypCMZ9dfv3r0LqicZ/1OMCWyAAiEnTB3+F1XL6eAcrmYPXL94MNilTxqtvJhwLpAIOyBOcPvZMvulq+sPb9jr/7zwC/QueWAPAxYEfSDsHnmfXfX08/dvtWBKF478OWT2N1LQxiwIhAI22btjMe8QpmnuCxs76LVA25fjAi5F8eAtYBA2DD3r0S/eBZXuGpeJht88nlu+vMxA9YCAmHD7F0V4pUzG5MTuYKyC4zbuyKUAasAgbBVrp2MSIhV5S0pO8e+Tz6vOxcjGLAKEAhb5dSeV87uzkyqnL+0b9iYjyIi3zBzk6OgB0Xezh8IY8DyQCBslYh3CX6FvZkscXR1uHLiHQOWBwJhk1w/GckY55rdkckSdx/X8LfxDFgezGptk9y5FK5wsKC43394cc+BBY8eX83mlj24aLX6tXo6O7tR+rI135EwlS/TcM2GCbGxUYH5SjVuMCAwX0nxrG27fj99YYeTo2u50g1y+gYwi5GzsFfo/bcMWB5YEDZJ+JsEhYOlxP3lq0d/Lh4YHx87oPeCLu2nPAu5NeevfiqVZpJ7nlc+eHTpzPmdg/ounjT2kNLBcfWGCeJZx0+uP35y3ReNvxnUZ5FP9jx7DyxkloQjlbwQyYCFgUDYJFHhCRzHLMTZC7uUCoeu7ab45cifK2fBL5uNevLsxuVrh8SjZDi0aTHaxzuvQqEsX7rBi5cPKIXSj/67tnSJOqVL1nZ19ahUvknhghWZReGE16FoZVgcCIRtQvLAW2oQDbUv8vkXd3PzEne9s+f28fa/9+C8uJszR34np8QJJpydNRPPRUWHCYLw8vUjv5wFdIX45ynGLArHqeLVDFgY+CBsErVKLagsZUJEx0Q8enKVgpT6iWHhr8QNzSxSKYiJjVSrVTrhIBwdXZglEVSCk7MdDWuXKhAImySblzLstaXen+7uPgUCyzao3Vs/0c3NM41TnJ3ceF4RHx+jS4mNi2IWJndB6XYDsRsgEDaJj5/z29BwZhny+BU5c2FHwfzleD7RWHgeejeHT1pRCY7jsnvlvv/wUo1PElOu3TjGLEZshMZjmis/Zri1OPBB2CSlqnomJFjKgqhetZ1ard6y85e4uJjQFw+27Z41fVb7ZyG30z6rTMm6l64eOH9pH23/c2Tpg8eXmcV4/Sjc0RmPrjXAr2yT5CroyCv41w8tEuejMMSwASsdHVx+ndtl6szWd++f/bL5qA86HevW6PZRhWabdkwn5wWZD583Gsz05rk3LxFvIn1ywXywBphRylZZM/1x2BtVkU/8mfy4vO9ei3558xa2rB8UMFgQtsvnPfPGRcmxI8DDiy8ofgF1sA5wUtoqLp6cu4/D3ZNPC1ZOfZrG0Bf3Z87rYeRsLmlxG0OomdC04f+Y+Rg9sU6q6RQWJetVoUjlCSxZrEbblmOZESJeRtZokZMBq4Amhm3zx9DbxaoHKpxSsQRVqoTIyNQHLMTGRun3WdDHwdHZxdmck9CEhb00diheFeegSGW8mYODk4uLe6qn3D35jAkJ3b/Pz4BVgAVh25So6nX12KPitQNTHqKXs4eHL8tqzHgNUa9io8JiBkwvzIC1gA/CtqnZytfT1+HWcVlM03jv/LMWfSw4SBSkBE0Me2DTnKfP7scG17TnynNl372mvfIGFINv0qpAIOyEZZMeRoULRavbYdTz1aOI5zdeNOvr718EfautDQTCftj+1/P7VyO9crvntaNlMm4efZwQm9B2WH7vXBialQVAIOyKFw/jN859rFKxbD6u+UplvYfSZKLexj+5EhITGZc9p1PHkfA7ZBkQg5hVPAAAAPdJREFUCDvk351vLh99GxerVioVjm4O2bK7uHo6Obg5MoX+veaYmmkmlVDrppbQ7xzBcQITuKRdTR5xSz8/PT28wGmHhKh5xotjQyifZoNXa/7jBE4sJDGnmtPOc8MllSyI6QqmUMWpoiLiI19FRb+LJl3geC6bp6JV/3xu3jAcshIIhN3yLlR1dMuL0MexMdEJqnjNsAgutVut0QFmOLWEQBU5xYOhyScYSUnSFoHnOLWQLEW7qS8+yQoX0/nEWSYUPOfmpQws6lK9pZ0vNWorQCAAAEZBRykAgFEgEAAAo0AgAABGgUAAAIwCgQAAGAUCAQAwyv8BAAD//4Hrt8oAAAAGSURBVAMAsIgIFJY1MbkAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a734bbc8",
   "metadata": {},
   "source": [
    "Finally, invoke the graph with all state variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "18f7951c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:\n",
      "{'query': 'vulnerabilities in LLMs', 'original_plan': {}, 'plan': [], 'results': {'arxiv': []}, 'reflection': None, 'reflection_notes': '', 'summary': '', 'relevant_docs': []}\n",
      "\n",
      ">> GENERATING INITIAL PLAN...\n",
      "PLAN GENERATED, total 1 queries to be searched:\n",
      "[{'tool': 'arxiv_search', 'purpose': 'To identify academic papers discussing vulnerabilities and security issues in Large Language Models (LLMs).', 'query': {'search_terms': ['LLM vulnerabilities', 'Large Language Model security'], 'additional_focus': ['attack', 'risk', 'exploit', 'safety', 'threat', 'misuse', 'adversarial']}, 'rationale': \"The search terms directly target the core concepts of the user query. 'LLM vulnerabilities' and 'Large Language Model security' are common phrases in the field. The 'additional_focus' terms are included to broaden the search to cover various facets and types of vulnerabilities, such as different attack vectors, risks, and safety concerns, ensuring a comprehensive initial scan of relevant literature.\"}]\n",
      "\n",
      ">> SEARCHING ARXIV ...\n",
      "url: http://export.arxiv.org/api/query?search_query=%28ti%3A%22LLM%20vulnerabilities%22%20OR%20abs%3A%22LLM%20vulnerabilities%22%20OR%20ti%3A%22Large%20Language%20Model%20security%22%20OR%20abs%3A%22Large%20Language%20Model%20security%22%29%20AND%20%28attack%20OR%20risk%20OR%20exploit%20OR%20adversarial%29&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      "url: http://export.arxiv.org/api/query?search_query=%28ti%3A%22LLM%20vulnerabilities%22%20OR%20abs%3A%22LLM%20vulnerabilities%22%29%20AND%20%28safety%20OR%20threat%20OR%20misuse%29%20AND%20cat%3Acs.CL&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      "url: http://export.arxiv.org/api/query?search_query=%28ti%3A%22Large%20Language%20Model%20security%22%20OR%20abs%3A%22Large%20Language%20Model%20security%22%29%20AND%20%28attack%20OR%20risk%20OR%20exploit%20OR%20safety%20OR%20threat%20OR%20misuse%20OR%20adversarial%29&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      ">> 0 SEARCHES LEFT...\n",
      "\n",
      ">> RETRIEVING RELEVANT PAPERS...\n",
      ">> LOADED 13 PAPERS\n",
      "MIN, MAX, MEAN\n",
      "\n",
      "0.7151068065082268 0.8078882103354944 0.750840208055084 \n",
      "\n",
      ">> USING 6 / 13 PAPERS FOR REFLECTION...\n",
      ">> CURRENT PAPERS ARE NOT SUFFICIENT...\n",
      ">> >> The collected papers are highly relevant and cover a good range of LLM vulnerabilities, including prompt injection, jailbreaking, adversarial attacks, data poisoning, unauthorized data exposure (data leakage), and vulnerabilities specific to multimodal LLMs (image inputs). Several papers also focus on evaluation methodologies and benchmarks, which is valuable. \n",
      "\n",
      "However, a key area explicitly mentioned in the 'analysis_focus' that is not prominently addressed in the retrieved summaries is 'hallucination.' While some adversarial attacks might indirectly lead to or exacerbate hallucination, none of the papers specifically focus on hallucination as a distinct vulnerability, its mechanisms, evaluation, or mitigation strategies in the same way they do for prompt injection or jailbreaking.\n",
      "\n",
      "Therefore, to fully address the 'analysis_focus,' further search is needed to specifically target papers discussing LLM hallucination as a security or safety vulnerability.\n",
      "\n",
      "**Suggested New Search Directions:**\n",
      "*   \"LLM hallucination vulnerability\"\n",
      "*   \"mitigating LLM hallucination\"\n",
      "*   \"evaluating LLM factual consistency\"\n",
      "*   \"LLM safety hallucination\"\n",
      "\n",
      "State:\n",
      "{'query': 'vulnerabilities in LLMs', 'original_plan': {'plan': [], 'reflection': {'purpose': 'To evaluate the effectiveness and comprehensiveness of the initial search results.', 'analysis_focus': ['Relevance of top search results to the user query.', 'Types of vulnerabilities identified (e.g., prompt injection, data leakage, adversarial attacks, hallucination).', 'Identification of any major areas or specific types of vulnerabilities that might be missing from the results.', 'Need for further refinement of search terms or additional search queries.'], 'rationale': 'This reflection step is crucial for an iterative research process. It allows for assessing whether the initial search strategy effectively captured the breadth and depth of information on LLM vulnerabilities. Based on this analysis, subsequent steps can be planned to either narrow down, broaden, or pivot the research direction.'}}, 'plan': [], 'results': {'arxiv': []}, 'reflection': False, 'reflection_notes': 'The collected papers are highly relevant and cover a good range of LLM vulnerabilities, including prompt injection, jailbreaking, adversarial attacks, data poisoning, unauthorized data exposure (data leakage), and vulnerabilities specific to multimodal LLMs (image inputs). Several papers also focus on evaluation methodologies and benchmarks, which is valuable. \\n\\nHowever, a key area explicitly mentioned in the \\'analysis_focus\\' that is not prominently addressed in the retrieved summaries is \\'hallucination.\\' While some adversarial attacks might indirectly lead to or exacerbate hallucination, none of the papers specifically focus on hallucination as a distinct vulnerability, its mechanisms, evaluation, or mitigation strategies in the same way they do for prompt injection or jailbreaking.\\n\\nTherefore, to fully address the \\'analysis_focus,\\' further search is needed to specifically target papers discussing LLM hallucination as a security or safety vulnerability.\\n\\n**Suggested New Search Directions:**\\n*   \"LLM hallucination vulnerability\"\\n*   \"mitigating LLM hallucination\"\\n*   \"evaluating LLM factual consistency\"\\n*   \"LLM safety hallucination\"', 'summary': '', 'relevant_docs': [\"Title: Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI\\n  Agents\\nSummary:\\nAI agents powered by large language models (LLMs) are being deployed at\\nscale, yet we lack a systematic understanding of how the choice of backbone LLM\\naffects agent security. The non-deterministic sequential nature of AI agents\\ncomplicates security modeling, while the integration of traditional software\\nwith AI components entangles novel LLM vulnerabilities with conventional\\nsecurity risks. Existing frameworks only partially address these challenges as\\nthey either capture specific vulnerabilities only or require modeling of\\ncomplete agents. To address these limitations, we introduce threat snapshots: a\\nframework that isolates specific states in an agent's execution flow where LLM\\nvulnerabilities manifest, enabling the systematic identification and\\ncategorization of security risks that propagate from the LLM to the agent\\nlevel. We apply this framework to construct the $\\\\operatorname{b}^3$ benchmark,\\na security benchmark based on 194331 unique crowdsourced adversarial attacks.\\nWe then evaluate 31 popular LLMs with it, revealing, among other insights, that\\nenhanced reasoning capabilities improve security, while model size does not\\ncorrelate with security. We release our benchmark, dataset, and evaluation code\\nto facilitate widespread adoption by LLM providers and practitioners, offering\\nguidance for agent developers and incentivizing model developers to prioritize\\nbackbone security improvements.\\nLink: http://arxiv.org/abs/2510.22620v1\", 'Title: Towards Reliable and Practical LLM Security Evaluations via Bayesian\\n  Modelling\\nSummary:\\nBefore adopting a new large language model (LLM) architecture, it is critical\\nto understand vulnerabilities accurately. Existing evaluations can be difficult\\nto trust, often drawing conclusions from LLMs that are not meaningfully\\ncomparable, relying on heuristic inputs or employing metrics that fail to\\ncapture the inherent uncertainty. In this paper, we propose a principled and\\npractical end-to-end framework for evaluating LLM vulnerabilities to prompt\\ninjection attacks. First, we propose practical approaches to experimental\\ndesign, tackling unfair LLM comparisons by considering two practitioner\\nscenarios: when training an LLM and when deploying a pre-trained LLM. Second,\\nwe address the analysis of experiments and propose a Bayesian hierarchical\\nmodel with embedding-space clustering. This model is designed to improve\\nuncertainty quantification in the common scenario that LLM outputs are not\\ndeterministic, test prompts are designed imperfectly, and practitioners only\\nhave a limited amount of compute to evaluate vulnerabilities. We show the\\nimproved inferential capabilities of the model in several prompt injection\\nattack settings. Finally, we demonstrate the pipeline to evaluate the security\\nof Transformer versus Mamba architectures. Our findings show that consideration\\nof output variability can suggest less definitive findings. However, for some\\nattacks, we find notably increased Transformer and Mamba-variant\\nvulnerabilities across LLMs with the same training data or mathematical\\nability.\\nLink: http://arxiv.org/abs/2510.05709v1', \"Title: GuardVal: Dynamic Large Language Model Jailbreak Evaluation for\\n  Comprehensive Safety Testing\\nSummary:\\nJailbreak attacks reveal critical vulnerabilities in Large Language Models\\n(LLMs) by causing them to generate harmful or unethical content. Evaluating\\nthese threats is particularly challenging due to the evolving nature of LLMs\\nand the sophistication required in effectively probing their vulnerabilities.\\nCurrent benchmarks and evaluation methods struggle to fully address these\\nchallenges, leaving gaps in the assessment of LLM vulnerabilities. In this\\npaper, we review existing jailbreak evaluation practices and identify three\\nassumed desiderata for an effective jailbreak evaluation protocol. To address\\nthese challenges, we introduce GuardVal, a new evaluation protocol that\\ndynamically generates and refines jailbreak prompts based on the defender LLM's\\nstate, providing a more accurate assessment of defender LLMs' capacity to\\nhandle safety-critical situations. Moreover, we propose a new optimization\\nmethod that prevents stagnation during prompt refinement, ensuring the\\ngeneration of increasingly effective jailbreak prompts that expose deeper\\nweaknesses in the defender LLMs. We apply this protocol to a diverse set of\\nmodels, from Mistral-7b to GPT-4, across 10 safety domains. Our findings\\nhighlight distinct behavioral patterns among the models, offering a\\ncomprehensive view of their robustness. Furthermore, our evaluation process\\ndeepens the understanding of LLM behavior, leading to insights that can inform\\nfuture research and drive the development of more secure models.\\nLink: http://arxiv.org/abs/2507.07735v1\", 'Title: SecReEvalBench: A Multi-turned Security Resilience Evaluation Benchmark\\n  for Large Language Models\\nSummary:\\nThe increasing deployment of large language models in security-sensitive\\ndomains necessitates rigorous evaluation of their resilience against\\nadversarial prompt-based attacks. While previous benchmarks have focused on\\nsecurity evaluations with limited and predefined attack domains, such as\\ncybersecurity attacks, they often lack a comprehensive assessment of\\nintent-driven adversarial prompts and the consideration of real-life\\nscenario-based multi-turn attacks. To address this gap, we present\\nSecReEvalBench, the Security Resilience Evaluation Benchmark, which defines\\nfour novel metrics: Prompt Attack Resilience Score, Prompt Attack Refusal Logic\\nScore, Chain-Based Attack Resilience Score and Chain-Based Attack Rejection\\nTime Score. Moreover, SecReEvalBench employs six questioning sequences for\\nmodel assessment: one-off attack, successive attack, successive reverse attack,\\nalternative attack, sequential ascending attack with escalating threat levels\\nand sequential descending attack with diminishing threat levels. In addition,\\nwe introduce a dataset customized for the benchmark, which incorporates both\\nneutral and malicious prompts, categorised across seven security domains and\\nsixteen attack techniques. In applying this benchmark, we systematically\\nevaluate five state-of-the-art open-weighted large language models, Llama 3.1,\\nGemma 2, Mistral v0.3, DeepSeek-R1 and Qwen 3. Our findings offer critical\\ninsights into the strengths and weaknesses of modern large language models in\\ndefending against evolving adversarial threats. The SecReEvalBench dataset is\\npublicly available at\\nhttps://kaggle.com/datasets/5a7ee22cf9dab6c93b55a73f630f6c9b42e936351b0ae98fbae6ddaca7fe248d,\\nwhich provides a groundwork for advancing research in large language model\\nsecurity.\\nLink: http://arxiv.org/abs/2505.07584v3', \"Title: Blockchain for Large Language Model Security and Safety: A Holistic\\n  Survey\\nSummary:\\nWith the growing development and deployment of large language models (LLMs)\\nin both industrial and academic fields, their security and safety concerns have\\nbecome increasingly critical. However, recent studies indicate that LLMs face\\nnumerous vulnerabilities, including data poisoning, prompt injections, and\\nunauthorized data exposure, which conventional methods have struggled to\\naddress fully. In parallel, blockchain technology, known for its data\\nimmutability and decentralized structure, offers a promising foundation for\\nsafeguarding LLMs. In this survey, we aim to comprehensively assess how to\\nleverage blockchain technology to enhance LLMs' security and safety. Besides,\\nwe propose a new taxonomy of blockchain for large language models (BC4LLMs) to\\nsystematically categorize related works in this emerging field. Our analysis\\nincludes novel frameworks and definitions to delineate security and safety in\\nthe context of BC4LLMs, highlighting potential research directions and\\nchallenges at this intersection. Through this study, we aim to stimulate\\ntargeted advancements in blockchain-integrated LLM security.\\nLink: http://arxiv.org/abs/2407.20181v2\", \"Title: Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in\\n  Multimodal Large Language Model Security\\nSummary:\\nMultimodal Large Language Models (MLLMs) demonstrate remarkable capabilities\\nthat increasingly influence various aspects of our daily lives, constantly\\ndefining the new boundary of Artificial General Intelligence (AGI). Image\\nmodalities, enriched with profound semantic information and a more continuous\\nmathematical nature compared to other modalities, greatly enhance the\\nfunctionalities of MLLMs when integrated. However, this integration serves as a\\ndouble-edged sword, providing attackers with expansive vulnerabilities to\\nexploit for highly covert and harmful attacks. The pursuit of reliable AI\\nsystems like powerful MLLMs has emerged as a pivotal area of contemporary\\nresearch. In this paper, we endeavor to demostrate the multifaceted risks\\nassociated with the incorporation of image modalities into MLLMs. Initially, we\\ndelineate the foundational components and training processes of MLLMs.\\nSubsequently, we construct a threat model, outlining the security\\nvulnerabilities intrinsic to MLLMs. Moreover, we analyze and summarize existing\\nscholarly discourses on MLLMs' attack and defense mechanisms, culminating in\\nsuggestions for the future research on MLLM security. Through this\\ncomprehensive analysis, we aim to deepen the academic understanding of MLLM\\nsecurity challenges and propel forward the development of trustworthy MLLM\\nsystems.\\nLink: http://arxiv.org/abs/2404.05264v2\"]}\n",
      "\n",
      ">> GENERATING NEW PLAN...\n",
      "PLAN GENERATED, total 1 queries to be searched:\n",
      "[{'tool': 'arxiv_search', 'purpose': 'To specifically identify academic papers discussing LLM hallucination as a distinct vulnerability, including its mechanisms, evaluation, and mitigation strategies.', 'query': {'search_terms': ['LLM hallucination vulnerability'], 'additional_focus': ['mitigation', 'evaluation', 'factual consistency', 'safety', 'mechanisms']}, 'rationale': \"The previous search results lacked prominent coverage of 'hallucination' as a direct LLM vulnerability. This targeted search uses specific keywords derived from the reflection notes to fill this gap, focusing on hallucination's nature as a vulnerability, its causes, methods for detection/evaluation, and strategies for mitigation, to ensure comprehensive coverage of LLM vulnerabilities.\"}]\n",
      "\n",
      ">> SEARCHING ARXIV ...\n",
      "url: http://export.arxiv.org/api/query?search_query=%28ti%3A%22LLM%20hallucination%20vulnerability%22%20OR%20abs%3A%22LLM%20hallucination%20vulnerability%22%29%20AND%20%28abs%3Amitigation%20OR%20abs%3Aevaluation%20OR%20abs%3A%22factual%20consistency%22%29%20AND%20cat%3Acs.CL&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      "url: http://export.arxiv.org/api/query?search_query=abs%3A%22LLM%20hallucination%20vulnerability%22%20AND%20%28abs%3Amitigation%20OR%20abs%3Aevaluation%20OR%20abs%3A%22factual%20consistency%22%20OR%20abs%3Asafety%20OR%20abs%3Amechanisms%29%20AND%20cat%3Acs.CL&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      "url: http://export.arxiv.org/api/query?search_query=%28ti%3A%22LLM%20hallucination%20vulnerability%22%20OR%20abs%3A%22LLM%20hallucination%20vulnerability%22%29%20AND%20%28abs%3Asafety%20OR%20abs%3Amechanisms%29%20AND%20cat%3Acs.CL&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      ">> 0 SEARCHES LEFT...\n",
      "\n",
      ">> RETRIEVING RELEVANT PAPERS...\n",
      ">> LOADED 0 PAPERS\n",
      ">> CURRENT PAPERS ARE NOT SUFFICIENT...\n",
      ">> >> The retrieved papers are not sufficient to fulfill the current research plan, as they do not adequately address the 'hallucination' aspect of LLM vulnerabilities. The search was specifically targeted at 'hallucination' to address a gap, but the results primarily focus on general LLM security, adversarial attacks (such as prompt injection, jailbreaking, and multi-turn attacks), and their evaluation benchmarks. None of the paper titles or summaries explicitly mention 'hallucination' as a core topic. Therefore, the retrieved papers do not provide insights into:\n",
      "\n",
      "1.  **Relevance to LLM hallucination as a vulnerability:** The papers are relevant to LLM security but not specifically to hallucination.\n",
      "2.  **Hallucination mechanisms, evaluation, and mitigation strategies:** There is no discussion of the underlying causes of hallucination, specific methods to evaluate its presence and severity, or strategies to mitigate it.\n",
      "3.  **Distinct perspective on hallucination:** The papers focus on adversarial attacks, which are distinct from hallucination. Hallucination often arises from the model's inherent limitations or uncertainty, rather than direct malicious prompting.\n",
      "\n",
      "**Missing aspects:** The current set of papers lacks any direct discussion of LLM hallucination. To address the research plan, papers are needed that:\n",
      "*   Define and characterize LLM hallucination.\n",
      "*   Explore the mechanisms or causes behind hallucination.\n",
      "*   Propose specific methodologies and metrics for evaluating hallucination.\n",
      "*   Discuss various strategies and techniques for mitigating hallucination.\n",
      "*   Differentiate hallucination from other types of LLM vulnerabilities like prompt injection or jailbreaking.\n",
      "\n",
      "**New directions to search:**\n",
      "*   Refine the search query to explicitly include terms like 'LLM hallucination mechanism', 'hallucination detection LLM', 'hallucination mitigation LLM', 'factual consistency LLM', 'truthfulness LLM', 'reliable LLM generation'.\n",
      "*   Look for survey papers specifically on 'LLM hallucination' to gain a comprehensive understanding.\n",
      "*   Include terms that distinguish hallucination from adversarial attacks, e.g., 'intrinsic LLM errors vs adversarial attacks' or 'non-adversarial LLM vulnerabilities'.\n",
      "\n",
      "State:\n",
      "{'query': 'vulnerabilities in LLMs', 'original_plan': {'plan': [], 'reflection': {'purpose': \"To evaluate whether the targeted search for 'hallucination' successfully captured relevant papers and adequately addressed the identified gap in the previous search results.\", 'analysis_focus': ['Relevance of top search results to LLM hallucination as a vulnerability.', 'Identification of papers specifically discussing hallucination mechanisms, evaluation methodologies, and mitigation strategies.', 'Assessment of whether the new results provide a distinct perspective on hallucination compared to general adversarial attacks.', 'Determination if further refinement or additional searches are needed to fully cover the hallucination aspect.'], 'rationale': \"This reflection is essential to confirm that the refined search strategy effectively covered the missing 'hallucination' vulnerability. It ensures that the research process is iterative and adaptive, guaranteeing that all critical aspects of LLM vulnerabilities are addressed comprehensively.\"}}, 'plan': [], 'results': {'arxiv': []}, 'reflection': False, 'reflection_notes': \"The retrieved papers are not sufficient to fulfill the current research plan, as they do not adequately address the 'hallucination' aspect of LLM vulnerabilities. The search was specifically targeted at 'hallucination' to address a gap, but the results primarily focus on general LLM security, adversarial attacks (such as prompt injection, jailbreaking, and multi-turn attacks), and their evaluation benchmarks. None of the paper titles or summaries explicitly mention 'hallucination' as a core topic. Therefore, the retrieved papers do not provide insights into:\\n\\n1.  **Relevance to LLM hallucination as a vulnerability:** The papers are relevant to LLM security but not specifically to hallucination.\\n2.  **Hallucination mechanisms, evaluation, and mitigation strategies:** There is no discussion of the underlying causes of hallucination, specific methods to evaluate its presence and severity, or strategies to mitigate it.\\n3.  **Distinct perspective on hallucination:** The papers focus on adversarial attacks, which are distinct from hallucination. Hallucination often arises from the model's inherent limitations or uncertainty, rather than direct malicious prompting.\\n\\n**Missing aspects:** The current set of papers lacks any direct discussion of LLM hallucination. To address the research plan, papers are needed that:\\n*   Define and characterize LLM hallucination.\\n*   Explore the mechanisms or causes behind hallucination.\\n*   Propose specific methodologies and metrics for evaluating hallucination.\\n*   Discuss various strategies and techniques for mitigating hallucination.\\n*   Differentiate hallucination from other types of LLM vulnerabilities like prompt injection or jailbreaking.\\n\\n**New directions to search:**\\n*   Refine the search query to explicitly include terms like 'LLM hallucination mechanism', 'hallucination detection LLM', 'hallucination mitigation LLM', 'factual consistency LLM', 'truthfulness LLM', 'reliable LLM generation'.\\n*   Look for survey papers specifically on 'LLM hallucination' to gain a comprehensive understanding.\\n*   Include terms that distinguish hallucination from adversarial attacks, e.g., 'intrinsic LLM errors vs adversarial attacks' or 'non-adversarial LLM vulnerabilities'.\", 'summary': '', 'relevant_docs': [\"Title: Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI\\n  Agents\\nSummary:\\nAI agents powered by large language models (LLMs) are being deployed at\\nscale, yet we lack a systematic understanding of how the choice of backbone LLM\\naffects agent security. The non-deterministic sequential nature of AI agents\\ncomplicates security modeling, while the integration of traditional software\\nwith AI components entangles novel LLM vulnerabilities with conventional\\nsecurity risks. Existing frameworks only partially address these challenges as\\nthey either capture specific vulnerabilities only or require modeling of\\ncomplete agents. To address these limitations, we introduce threat snapshots: a\\nframework that isolates specific states in an agent's execution flow where LLM\\nvulnerabilities manifest, enabling the systematic identification and\\ncategorization of security risks that propagate from the LLM to the agent\\nlevel. We apply this framework to construct the $\\\\operatorname{b}^3$ benchmark,\\na security benchmark based on 194331 unique crowdsourced adversarial attacks.\\nWe then evaluate 31 popular LLMs with it, revealing, among other insights, that\\nenhanced reasoning capabilities improve security, while model size does not\\ncorrelate with security. We release our benchmark, dataset, and evaluation code\\nto facilitate widespread adoption by LLM providers and practitioners, offering\\nguidance for agent developers and incentivizing model developers to prioritize\\nbackbone security improvements.\\nLink: http://arxiv.org/abs/2510.22620v1\", 'Title: Towards Reliable and Practical LLM Security Evaluations via Bayesian\\n  Modelling\\nSummary:\\nBefore adopting a new large language model (LLM) architecture, it is critical\\nto understand vulnerabilities accurately. Existing evaluations can be difficult\\nto trust, often drawing conclusions from LLMs that are not meaningfully\\ncomparable, relying on heuristic inputs or employing metrics that fail to\\ncapture the inherent uncertainty. In this paper, we propose a principled and\\npractical end-to-end framework for evaluating LLM vulnerabilities to prompt\\ninjection attacks. First, we propose practical approaches to experimental\\ndesign, tackling unfair LLM comparisons by considering two practitioner\\nscenarios: when training an LLM and when deploying a pre-trained LLM. Second,\\nwe address the analysis of experiments and propose a Bayesian hierarchical\\nmodel with embedding-space clustering. This model is designed to improve\\nuncertainty quantification in the common scenario that LLM outputs are not\\ndeterministic, test prompts are designed imperfectly, and practitioners only\\nhave a limited amount of compute to evaluate vulnerabilities. We show the\\nimproved inferential capabilities of the model in several prompt injection\\nattack settings. Finally, we demonstrate the pipeline to evaluate the security\\nof Transformer versus Mamba architectures. Our findings show that consideration\\nof output variability can suggest less definitive findings. However, for some\\nattacks, we find notably increased Transformer and Mamba-variant\\nvulnerabilities across LLMs with the same training data or mathematical\\nability.\\nLink: http://arxiv.org/abs/2510.05709v1', \"Title: GuardVal: Dynamic Large Language Model Jailbreak Evaluation for\\n  Comprehensive Safety Testing\\nSummary:\\nJailbreak attacks reveal critical vulnerabilities in Large Language Models\\n(LLMs) by causing them to generate harmful or unethical content. Evaluating\\nthese threats is particularly challenging due to the evolving nature of LLMs\\nand the sophistication required in effectively probing their vulnerabilities.\\nCurrent benchmarks and evaluation methods struggle to fully address these\\nchallenges, leaving gaps in the assessment of LLM vulnerabilities. In this\\npaper, we review existing jailbreak evaluation practices and identify three\\nassumed desiderata for an effective jailbreak evaluation protocol. To address\\nthese challenges, we introduce GuardVal, a new evaluation protocol that\\ndynamically generates and refines jailbreak prompts based on the defender LLM's\\nstate, providing a more accurate assessment of defender LLMs' capacity to\\nhandle safety-critical situations. Moreover, we propose a new optimization\\nmethod that prevents stagnation during prompt refinement, ensuring the\\ngeneration of increasingly effective jailbreak prompts that expose deeper\\nweaknesses in the defender LLMs. We apply this protocol to a diverse set of\\nmodels, from Mistral-7b to GPT-4, across 10 safety domains. Our findings\\nhighlight distinct behavioral patterns among the models, offering a\\ncomprehensive view of their robustness. Furthermore, our evaluation process\\ndeepens the understanding of LLM behavior, leading to insights that can inform\\nfuture research and drive the development of more secure models.\\nLink: http://arxiv.org/abs/2507.07735v1\", 'Title: SecReEvalBench: A Multi-turned Security Resilience Evaluation Benchmark\\n  for Large Language Models\\nSummary:\\nThe increasing deployment of large language models in security-sensitive\\ndomains necessitates rigorous evaluation of their resilience against\\nadversarial prompt-based attacks. While previous benchmarks have focused on\\nsecurity evaluations with limited and predefined attack domains, such as\\ncybersecurity attacks, they often lack a comprehensive assessment of\\nintent-driven adversarial prompts and the consideration of real-life\\nscenario-based multi-turn attacks. To address this gap, we present\\nSecReEvalBench, the Security Resilience Evaluation Benchmark, which defines\\nfour novel metrics: Prompt Attack Resilience Score, Prompt Attack Refusal Logic\\nScore, Chain-Based Attack Resilience Score and Chain-Based Attack Rejection\\nTime Score. Moreover, SecReEvalBench employs six questioning sequences for\\nmodel assessment: one-off attack, successive attack, successive reverse attack,\\nalternative attack, sequential ascending attack with escalating threat levels\\nand sequential descending attack with diminishing threat levels. In addition,\\nwe introduce a dataset customized for the benchmark, which incorporates both\\nneutral and malicious prompts, categorised across seven security domains and\\nsixteen attack techniques. In applying this benchmark, we systematically\\nevaluate five state-of-the-art open-weighted large language models, Llama 3.1,\\nGemma 2, Mistral v0.3, DeepSeek-R1 and Qwen 3. Our findings offer critical\\ninsights into the strengths and weaknesses of modern large language models in\\ndefending against evolving adversarial threats. The SecReEvalBench dataset is\\npublicly available at\\nhttps://kaggle.com/datasets/5a7ee22cf9dab6c93b55a73f630f6c9b42e936351b0ae98fbae6ddaca7fe248d,\\nwhich provides a groundwork for advancing research in large language model\\nsecurity.\\nLink: http://arxiv.org/abs/2505.07584v3', \"Title: Blockchain for Large Language Model Security and Safety: A Holistic\\n  Survey\\nSummary:\\nWith the growing development and deployment of large language models (LLMs)\\nin both industrial and academic fields, their security and safety concerns have\\nbecome increasingly critical. However, recent studies indicate that LLMs face\\nnumerous vulnerabilities, including data poisoning, prompt injections, and\\nunauthorized data exposure, which conventional methods have struggled to\\naddress fully. In parallel, blockchain technology, known for its data\\nimmutability and decentralized structure, offers a promising foundation for\\nsafeguarding LLMs. In this survey, we aim to comprehensively assess how to\\nleverage blockchain technology to enhance LLMs' security and safety. Besides,\\nwe propose a new taxonomy of blockchain for large language models (BC4LLMs) to\\nsystematically categorize related works in this emerging field. Our analysis\\nincludes novel frameworks and definitions to delineate security and safety in\\nthe context of BC4LLMs, highlighting potential research directions and\\nchallenges at this intersection. Through this study, we aim to stimulate\\ntargeted advancements in blockchain-integrated LLM security.\\nLink: http://arxiv.org/abs/2407.20181v2\", \"Title: Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in\\n  Multimodal Large Language Model Security\\nSummary:\\nMultimodal Large Language Models (MLLMs) demonstrate remarkable capabilities\\nthat increasingly influence various aspects of our daily lives, constantly\\ndefining the new boundary of Artificial General Intelligence (AGI). Image\\nmodalities, enriched with profound semantic information and a more continuous\\nmathematical nature compared to other modalities, greatly enhance the\\nfunctionalities of MLLMs when integrated. However, this integration serves as a\\ndouble-edged sword, providing attackers with expansive vulnerabilities to\\nexploit for highly covert and harmful attacks. The pursuit of reliable AI\\nsystems like powerful MLLMs has emerged as a pivotal area of contemporary\\nresearch. In this paper, we endeavor to demostrate the multifaceted risks\\nassociated with the incorporation of image modalities into MLLMs. Initially, we\\ndelineate the foundational components and training processes of MLLMs.\\nSubsequently, we construct a threat model, outlining the security\\nvulnerabilities intrinsic to MLLMs. Moreover, we analyze and summarize existing\\nscholarly discourses on MLLMs' attack and defense mechanisms, culminating in\\nsuggestions for the future research on MLLM security. Through this\\ncomprehensive analysis, we aim to deepen the academic understanding of MLLM\\nsecurity challenges and propel forward the development of trustworthy MLLM\\nsystems.\\nLink: http://arxiv.org/abs/2404.05264v2\"]}\n",
      "\n",
      ">> GENERATING NEW PLAN...\n",
      "PLAN GENERATED, total 4 queries to be searched:\n",
      "[{'tool': 'arxiv_search', 'purpose': 'To gain a comprehensive and foundational understanding of LLM hallucination, including its definitions, types, characteristics, and an overview of the current research landscape. Survey papers are prioritized for this broad understanding.', 'query': {'search_terms': ['LLM hallucination survey', 'LLM factual error review'], 'additional_focus': ['causes', 'detection', 'mitigation', 'truthfulness', 'reliability']}, 'rationale': 'This step directly addresses the need for foundational understanding and survey papers on LLM hallucination, aiming to provide a comprehensive starting point as suggested in the reflection notes.'}, {'tool': 'arxiv_search', 'purpose': 'To specifically identify academic papers that delve into the underlying mechanisms, intrinsic causes, and theoretical frameworks explaining why LLMs generate hallucinatory content.', 'query': {'search_terms': ['LLM hallucination mechanism', 'causes of LLM hallucination', 'intrinsic LLM errors'], 'additional_focus': ['factual inconsistency', 'knowledge gaps', 'reliability issues', 'generation process']}, 'rationale': 'This query directly targets the missing aspect of understanding hallucination mechanisms and causes, using specific terms identified in the reflection notes to differentiate from general adversarial attacks.'}, {'tool': 'arxiv_search', 'purpose': 'To find research on methodologies, metrics, and benchmarks specifically developed for detecting, quantifying, and evaluating the presence and severity of hallucination in LLM outputs.', 'query': {'search_terms': ['LLM hallucination detection', 'LLM hallucination evaluation', 'factual consistency metrics LLM'], 'additional_focus': ['truthfulness assessment', 'reliability measurement', 'benchmarks', 'quantification']}, 'rationale': \"This step focuses on the 'evaluation methodologies and metrics' identified as missing, using precise search terms to find papers on how hallucination is measured and assessed.\"}, {'tool': 'arxiv_search', 'purpose': 'To discover and categorize various strategies, techniques, and architectural approaches proposed in academic literature for mitigating, reducing, or preventing hallucination in LLMs.', 'query': {'search_terms': ['LLM hallucination mitigation', 'reducing LLM factual errors', 'LLM truthfulness improvement'], 'additional_focus': ['reliable generation', 'knowledge integration', 'post-hoc correction', 'factual accuracy']}, 'rationale': \"This query directly addresses the 'mitigation strategies' identified as missing, using action-oriented terms to find solutions and techniques for improving LLM factual consistency.\"}]\n",
      "\n",
      ">> SEARCHING ARXIV ...\n",
      "url: http://export.arxiv.org/api/query?search_query=%28ti%3A%22LLM%20hallucination%22%20OR%20abs%3A%22LLM%20hallucination%22%29%20AND%20%28survey%20OR%20review%29%20AND%20%28causes%20OR%20detection%20OR%20mitigation%20OR%20truthfulness%20OR%20reliability%29%20AND%20cat%3Acs.CL&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      "url: http://export.arxiv.org/api/query?search_query=%28ti%3A%22LLM%20factual%20error%22%20OR%20abs%3A%22LLM%20factual%20error%22%29%20AND%20%28review%20OR%20survey%29%20AND%20%28causes%20OR%20detection%20OR%20mitigation%20OR%20truthfulness%20OR%20reliability%29%20AND%20cat%3Acs.CL&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      "url: http://export.arxiv.org/api/query?search_query=%28ti%3A%22LLM%20hallucination%22%20OR%20abs%3A%22LLM%20hallucination%22%20OR%20ti%3A%22LLM%20factual%20error%22%20OR%20abs%3A%22LLM%20factual%20error%22%29%20AND%20%28survey%20OR%20review%29%20AND%20%28detection%20OR%20mitigation%20OR%20reliability%29%20AND%20cat%3Acs.CL&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      ">> 3 SEARCHES LEFT...\n",
      "\n",
      ">> SEARCHING ARXIV ...\n",
      "url: http://export.arxiv.org/api/query?search_query=%28ti%3A%22LLM%20hallucination%20mechanism%22%20OR%20abs%3A%22LLM%20hallucination%20mechanism%22%20OR%20ti%3A%22causes%20of%20LLM%20hallucination%22%20OR%20abs%3A%22causes%20of%20LLM%20hallucination%22%29%20AND%20%28ti%3A%22factual%20inconsistency%22%20OR%20abs%3A%22factual%20inconsistency%22%20OR%20ti%3A%22generation%20process%22%20OR%20abs%3A%22generation%20process%22%29&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      "url: http://export.arxiv.org/api/query?search_query=%28ti%3A%22intrinsic%20LLM%20errors%22%20OR%20abs%3A%22intrinsic%20LLM%20errors%22%29%20AND%20%28ti%3A%22reliability%20issues%22%20OR%20abs%3A%22reliability%20issues%22%20OR%20ti%3A%22knowledge%20gaps%22%20OR%20abs%3A%22knowledge%20gaps%22%29&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      "url: http://export.arxiv.org/api/query?search_query=cat%3Acs.CL%20AND%20%28ti%3A%22LLM%20hallucination%20mechanism%22%20OR%20abs%3A%22LLM%20hallucination%20mechanism%22%20OR%20ti%3A%22causes%20of%20LLM%20hallucination%22%20OR%20abs%3A%22causes%20of%20LLM%20hallucination%22%20OR%20ti%3A%22intrinsic%20LLM%20errors%22%20OR%20abs%3A%22intrinsic%20LLM%20errors%22%29%20AND%20%28ti%3A%22factual%20inconsistency%22%20OR%20abs%3A%22factual%20inconsistency%22%20OR%20ti%3A%22knowledge%20gaps%22%20OR%20abs%3A%22knowledge%20gaps%22%20OR%20ti%3A%22reliability%20issues%22%20OR%20abs%3A%22reliability%20issues%22%20OR%20ti%3A%22generation%20process%22%20OR%20abs%3A%22generation%20process%22%29&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      ">> 2 SEARCHES LEFT...\n",
      "\n",
      ">> SEARCHING ARXIV ...\n",
      "url: http://export.arxiv.org/api/query?search_query=%28all%3A%22LLM%20hallucination%20detection%22%20OR%20all%3A%22LLM%20hallucination%20evaluation%22%20OR%20all%3A%22factual%20consistency%20metrics%20LLM%22%29%20AND%20%28abs%3A%22truthfulness%20assessment%22%20OR%20abs%3A%22reliability%20measurement%22%20OR%20abs%3Abenchmarks%29&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      "url: http://export.arxiv.org/api/query?search_query=%28all%3A%22LLM%20hallucination%20evaluation%22%20OR%20all%3A%22factual%20consistency%20metrics%20LLM%22%29%20AND%20%28abs%3Aquantification%20OR%20abs%3Abenchmarks%20OR%20abs%3A%22reliability%20measurement%22%29%20AND%20cat%3Acs.CL&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      "url: http://export.arxiv.org/api/query?search_query=%28all%3A%22LLM%20hallucination%20detection%22%20OR%20all%3A%22LLM%20hallucination%20evaluation%22%29%20AND%20%28abs%3A%22truthfulness%20assessment%22%20OR%20abs%3A%22reliability%20measurement%22%20OR%20abs%3Aquantification%29%20AND%20cat%3Acs.CL&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      ">> 1 SEARCHES LEFT...\n",
      "\n",
      ">> SEARCHING ARXIV ...\n",
      "url: http://export.arxiv.org/api/query?search_query=%28abs%3A%22LLM%20hallucination%20mitigation%22%20OR%20abs%3A%22reducing%20LLM%20factual%20errors%22%20OR%20abs%3A%22LLM%20truthfulness%20improvement%22%29%20AND%20abs%3A%22factual%20accuracy%22%20AND%20cat%3Acs.CL&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      "url: http://export.arxiv.org/api/query?search_query=%28abs%3A%22LLM%20hallucination%20mitigation%22%20OR%20abs%3A%22reducing%20LLM%20factual%20errors%22%20OR%20abs%3A%22LLM%20truthfulness%20improvement%22%29%20AND%20abs%3A%22knowledge%20integration%22%20AND%20cat%3Acs.CL&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      "url: http://export.arxiv.org/api/query?search_query=%28abs%3A%22LLM%20hallucination%20mitigation%22%20OR%20abs%3A%22reducing%20LLM%20factual%20errors%22%20OR%20abs%3A%22LLM%20truthfulness%20improvement%22%29%20AND%20%28abs%3A%22post-hoc%20correction%22%20OR%20abs%3A%22reliable%20generation%22%29%20AND%20cat%3Acs.CL&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      ">> 0 SEARCHES LEFT...\n",
      "\n",
      ">> RETRIEVING RELEVANT PAPERS...\n",
      ">> LOADED 21 PAPERS\n",
      "MIN, MAX, MEAN\n",
      "\n",
      "0.7273650721929907 0.8899799306430386 0.7961817315239772 \n",
      "\n",
      ">> USING 8 / 21 PAPERS FOR REFLECTION...\n",
      ">> CURRENT PAPERS ARE SUFFICIENT...\n",
      ">> >> The retrieved papers are highly sufficient to fulfill the current research plan. Multiple comprehensive survey papers explicitly focusing on 'LLM Hallucination' (e.g., 'Large Language Models Hallucination: A Comprehensive Survey', 'A comprehensive taxonomy of hallucinations in Large Language Models', 'Loki's Dance of Illusions') directly address all key aspects of the analysis focus:\n",
      "\n",
      "1.  **Relevance to LLM hallucination:** The core topic of LLM hallucination is thoroughly covered by these dedicated surveys.\n",
      "2.  **Mechanisms, causes, and theoretical frameworks:** These surveys explicitly mention analyzing root causes, underlying mechanisms, and theoretical frameworks behind hallucination.\n",
      "3.  **Methodologies and metrics for evaluation:** The surveys review existing evaluation benchmarks and metrics. Additionally, 'GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework' provides a concrete, specialized methodology for hallucination evaluation.\n",
      "4.  **Mitigation strategies:** The surveys outline diverse mitigation strategies and solutions for LLM hallucination.\n",
      "5.  **Differentiation from other LLM vulnerabilities:** By providing clear definitions and dedicated discussions of hallucination, these papers inherently differentiate it from other LLM vulnerabilities. While some retrieved papers discuss other vulnerabilities like prompt injection or jailbreaking, the strong focus and comprehensive nature of the hallucination-specific papers ensure the core problem is addressed distinctly.\n",
      "\n",
      "The presence of several recent, comprehensive survey papers on hallucination, along with a specific evaluation framework, provides a robust foundation for a thorough understanding of this vulnerability as outlined in the research plan.\n",
      "\n",
      ">> GENERATING SUMMARY...\n",
      ">> SUMMARIZED RESULTS !!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# class AgentState(TypedDict):\n",
    "#     query: str # The user query\n",
    "#     original_plan: Dict[str, str] # Entire plan returned by the planner, with rationale and reflection\n",
    "#     plan: List[Dict] # List of plan dicts with tool name, search parameters and reasoning\n",
    "#     results: Dict # Result of each search tool\n",
    "#     reflection: bool # To determine whether the findings are enough to summarize\n",
    "#     reflection_notes: str # LLM's reasoning notes for the reflection\n",
    "#     summary: str # Final summary\n",
    "\n",
    "# app.invoke({\"query\":\"latest advancements in image generation models\"})\n",
    "\n",
    "# query = \"report on advancement of LLMs\"\n",
    "# query = \"detecting attacks on voice based systems\"\n",
    "query = \"vulnerabilities in LLMs\"\n",
    "\n",
    "final_state = app.invoke({\n",
    "    \"query\": query,\n",
    "    \"original_plan\": {},\n",
    "    \"plan\": [],\n",
    "    \"results\": {\"arxiv\":[]},\n",
    "    \"reflection\": None,\n",
    "    \"reflection_notes\": \"\",\n",
    "    \"summary\": \"\",\n",
    "    \"relevant_docs\": []\n",
    "},config={\"recursion_limit\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "b2b4affc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Summary of Vulnerabilities in Large Language Models\n",
       "\n",
       "The widespread deployment of Large Language Models (LLMs) and AI agents powered by them necessitates a deep understanding of their security vulnerabilities. Recent research highlights several critical areas of concern, ranging from direct adversarial attacks to inherent reliability issues like hallucinations, and specialized risks in multimodal and agentic systems.\n",
       "\n",
       "### Key Vulnerability Categories\n",
       "\n",
       "1.  **Adversarial Prompt Attacks and Jailbreaks**:\n",
       "    *   LLMs are highly susceptible to prompt injection attacks, where malicious inputs can override safety mechanisms or elicit harmful content. \"Towards Reliable and Practical LLM Security Evaluations via Bayesian Modelling\" (http://arxiv.org/abs/2510.05709v1) specifically focuses on evaluating these vulnerabilities, emphasizing the need for robust experimental design and uncertainty quantification.\n",
       "    *   **Jailbreak attacks** are a specific type of prompt attack designed to bypass safety filters and make LLMs generate unethical or unsafe outputs. \"GuardVal: Dynamic Large Language Model Jailbreak Evaluation for Comprehensive Safety Testing\" (http://arxiv.org/abs/2507.07735v1) introduces a dynamic protocol to generate and refine such prompts, revealing distinct robustness levels across models like Mistral-7b and GPT-4.\n",
       "    *   **Multi-turn and intent-driven attacks** represent an evolving threat, requiring more sophisticated evaluation. \"SecReEvalBench: A Multi-turned Security Resilience Evaluation Benchmark for Large Language Models\" (http://arxiv.org/abs/2505.07584v3) addresses this by defining novel metrics and questioning sequences to assess LLM resilience against chained adversarial interactions.\n",
       "\n",
       "2.  **Hallucinations**:\n",
       "    *   Hallucination, defined as the generation of fluent but factually inaccurate or fabricated content, is a pervasive and critical challenge for LLMs, undermining their reliability and trustworthiness. Multiple surveys, including \"Large Language Models Hallucination: A Comprehensive Survey\" (http://arxiv.org/abs/2510.06265v2), \"A comprehensive taxonomy of hallucinations in Large Language Models\" (http://arxiv.org/abs/2508.01781v1), and \"Loki's Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models\" (http://arxiv.org/abs/2507.02870v1), provide extensive taxonomies of hallucination types (e.g., intrinsic vs. extrinsic, factual errors, logical inconsistencies) and analyze their root causes across the entire LLM development lifecycle (data, model architecture, prompt). These papers highlight the potential for substantial economic, legal, and health risks due to misinformation.\n",
       "    *   The theoretical inevitability of hallucinations in computable LLMs is underscored, emphasizing the need for robust detection and mitigation rather than complete elimination.\n",
       "\n",
       "3.  **Multimodal Specific Vulnerabilities**:\n",
       "    *   The integration of diverse modalities, particularly image inputs, in Multimodal Large Language Models (MLLMs) introduces new security challenges. \"Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security\" (http://arxiv.org/abs/2404.05264v2) details how the rich semantic information and continuous mathematical nature of image modalities can be exploited by attackers for highly covert and harmful attacks, necessitating a specialized threat model for MLLMs.\n",
       "\n",
       "4.  **Agent-Level Vulnerabilities**:\n",
       "    *   When LLMs serve as \"backbones\" for AI agents, their inherent vulnerabilities can propagate and entangle with conventional software security risks. \"Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents\" (http://arxiv.org/abs/2510.22620v1) introduces the \"threat snapshots\" framework to isolate and systematically categorize these security risks at the agent level. Their evaluation of 31 popular LLMs revealed that enhanced reasoning capabilities can improve security, while model size does not necessarily correlate.\n",
       "\n",
       "### Evaluation and Mitigation Strategies\n",
       "\n",
       "Addressing these vulnerabilities requires sophisticated evaluation methodologies and novel mitigation approaches:\n",
       "\n",
       "*   **Advanced Evaluation Frameworks**: Researchers are developing new benchmarks and frameworks to systematically assess LLM security. These include:\n",
       "    *   **Threat snapshots** for AI agents (http://arxiv.org/abs/2510.22620v1).\n",
       "    *   **Bayesian hierarchical models** for robust prompt injection evaluations, accounting for output variability and limited compute (http://arxiv.org/abs/2510.05709v1).\n",
       "    *   **Dynamic jailbreak evaluation protocols** like GuardVal, which adapt prompts based on the defender's state (http://arxiv.org/abs/2507.07735v1).\n",
       "    *   **Multi-turn resilience benchmarks** such as SecReEvalBench, which consider complex attack sequences and diverse security domains (http://arxiv.org/abs/2505.07584v3).\n",
       "    *   **Knowledge-Graph Based Hallucination Evaluation**: \"GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework\" (http://arxiv.org/abs/2407.10793v1) offers a method to provide explainable decisions by identifying specific information prone to hallucination within a knowledge graph, demonstrating improved accuracy over raw natural language inference models.\n",
       "\n",
       "*   **Mitigation Approaches**:\n",
       "    *   **Architectural improvements and training data considerations** are crucial for reducing hallucinations, as discussed in the comprehensive surveys (e.g., http://arxiv.org/abs/2510.06265v2).\n",
       "    *   **Hallucination Correction**: GraphEval also explores \"GraphCorrect,\" a method leveraging knowledge graph structures to rectify hallucinations, showing promising results (http://arxiv.org/abs/2407.10793v1).\n",
       "    *   **Blockchain Integration**: \"Blockchain for Large Language Model Security and Safety: A Holistic Survey\" (http://arxiv.org/abs/2407.20181v2) proposes leveraging blockchain technology's data immutability and decentralized structure as a promising foundation to enhance LLMs' security against issues like data poisoning, prompt injections, and unauthorized data exposure.\n",
       "\n",
       "In conclusion, LLM vulnerabilities are complex and multifaceted, requiring continuous research into robust evaluation, detection, and mitigation strategies. The inherent nature of some issues, such as hallucination, suggests that ongoing human oversight will remain critical for responsible and reliable LLM deployment in sensitive applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "display(Markdown(final_state[\"summary\"]))\n",
    "# print(final_state[\"summary\"].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "c2d213a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary from a previous run\n",
    "# summary = '## Report on the Advancement of Large Language Models (LLMs)\\n\\nLarge Language Models (LLMs) continue to rapidly evolve, demonstrating remarkable progress across diverse applications while simultaneously revealing critical areas for improvement in efficiency, safety, and reliability. Recent research highlights significant advancements in architectural design, multimodal capabilities, and domain-specific applications, alongside a growing focus on understanding and mitigating inherent vulnerabilities.\\n\\n### Architectural Innovations and Efficiency\\n\\nThe foundational Transformer architecture, while powerful, presents substantial computational and memory demands. Current research is heavily focused on optimizing LLMs for speed and resource efficiency, particularly for large-scale training and edge deployment. A comprehensive survey, \"Speed Always Wins: A Survey on Efficient Architectures for Large Language Models\" [http://arxiv.org/abs/2508.09834v1], systematically examines innovative designs such as linear and sparse sequence modeling, efficient full attention variants, sparse mixture-of-experts (MoE), and hybrid architectures.\\n\\nKey developments in this area include:\\n*   **Long-Context Modeling:** New approaches tackle the quadratic cost of attention for longer contexts. \"Long-Context Modeling with Dynamic Hierarchical Sparse Attention for On-Device LLMs\" [http://arxiv.org/abs/2510.24606v1] introduces Dynamic Hierarchical Sparse Attention (DHSA), a data-driven framework that dynamically predicts attention sparsity, reducing prefill latency by 20-60% and peak memory usage by 35% without sacrificing accuracy. Similarly, \"H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for Efficient Long-Context LLM Inference\" [http://arxiv.org/abs/2508.16653v1] proposes a hybrid bonding-based accelerator with algorithm-hardware co-design to improve energy and latency efficiency for long-context LLM inference at the edge.\\n*   **Quantization:** To alleviate deployment barriers from enormous parameter counts, advancements in post-training quantization (PTQ) are crucial. \"SBVR: Summation of BitVector Representation for Efficient LLM Quantization\" [http://arxiv.org/abs/2509.18172v1] introduces a novel method that enables Gaussian-like code representation in a hardware-friendly manner, achieving 2.21x-3.04x speedup. For ultra-efficient edge inference, \"BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter 1.58-bit LLM Inference\" [http://arxiv.org/abs/2509.08542v1] presents a CiROM-based accelerator co-designed with 1.58-bit quantization, offering significant area efficiency. A systematic characterization framework, qMeter, is developed in \"Systematic Characterization of LLM Quantization\" [http://arxiv.org/abs/2508.16712v1] to understand the performance, energy, and quality tradeoffs of various quantization methods.\\n*   **Inference Systems:** For heterogeneous GPU clusters, \"Cronus: Efficient LLM inference on Heterogeneous GPU Clusters via Partially Disaggregated Prefill\" [http://arxiv.org/abs/2509.17357v1] proposes a novel system that dynamically balances workloads, improving throughput and reducing latency.\\n*   **Causal Reasoning:** Beyond efficiency, \"Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning\" [http://arxiv.org/abs/2506.07501v1] introduces GoCE, a method to strengthen transformers\\' ability to capture long-range causal dependencies, enhancing reasoning capabilities.\\n\\n### Enhanced Capabilities and Multimodality\\n\\nLLMs are increasingly moving beyond text-only processing, integrating multiple modalities and demonstrating more sophisticated reasoning and interaction capabilities.\\n*   **Multimodal Reasoning and Embodied AI:** Multimodal Large Language Models (MLLMs) are advancing rapidly. \"Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs\" [http://arxiv.org/abs/2510.24514v1] introduces an internal visual scratchpad for MLLMs, enabling visual planning and imagination to improve reasoning. \"BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning\" [http://arxiv.org/abs/2510.24161v1] presents a unified multimodal spatial foundation model capable of generalizing across digital/physical spaces, tasks, and robot embodiments. For proactive robot manipulation, \"RoboOmni: Proactive Robot Manipulation in Omni-modal Context\" [http://arxiv.org/abs/2510.23763v1] leverages end-to-end omni-modal LLMs to infer user intentions from dialogue, sounds, and visual cues.\\n*   **Fine-grained Understanding:** \"PixelRefer: A Unified Framework for Spatio-Temporal Object Referring with Arbitrary Granularity\" [http://arxiv.org/abs/2510.23603v1] introduces a region-level MLLM for advanced fine-grained, object-centric understanding across images and videos.\\n*   **Emotion and Aesthetics:** \"Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier\" [http://arxiv.org/abs/2510.23506v1] enhances MLLMs\\' emotion understanding and their ability to provide faithful, consistent explanations. In code generation, \"Code Aesthetics with Agentic Reward Feedback\" [http://arxiv.org/abs/2510.23272v1] proposes a pipeline using agentic reward feedback to significantly improve the aesthetic quality of LLM-generated code, with AesCoder-4B surpassing GPT-4o and GPT-4.1.\\n\\n### Critical Challenges: Safety, Alignment, and Reliability\\n\\nDespite impressive capabilities, LLMs face significant challenges related to safety, alignment with human preferences, and overall reliability.\\n*   **Vulnerabilities and Jailbreaks:** LLMs remain vulnerable to sophisticated attacks. \"Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models\" [http://arxiv.org/abs/2510.22085v1] demonstrates automated generation of narrative-based jailbreak prompts with high success rates against state-of-the-art models. \"Path Drift in Large Reasoning Models: How First-Person Commitments Override Safety\" [http://arxiv.org/abs/2510.10013v1] identifies \"Path Drift\" in long Chain-of-Thought reasoning, where models deviate from safety constraints due to goal-driven reasoning. \"Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment\" [http://arxiv.org/abs/2509.10546v1] reveals critical vulnerabilities in financial LLMs to \"Risk-Concealment Attacks,\" bypassing safeguards with high success rates. Furthermore, \"A Simple and Efficient Jailbreak Method Exploiting LLMs\\' Helpfulness\" [http://arxiv.org/abs/2509.14297v1] shows how transforming harmful requests into learning-style questions can bypass safety mechanisms.\\n*   **Defenses:** To counter these threats, \"Proactive defense against LLM Jailbreak\" [http://arxiv.org/abs/2510.05052v1] introduces ProAct, a proactive defense framework that misleads attackers with \"spurious responses,\" significantly reducing attack success rates.\\n*   **Alignment and Reward Modeling:** Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) are key for alignment. \"Improving LLM Safety and Helpfulness using SFT and DPO\" [http://arxiv.org/abs/2509.09055v1] finds that a combined SFT+DPO approach outperforms individual methods for safety and helpfulness. \"Alignment is Localized: A Causal Probe into Preference Layers\" [http://arxiv.org/abs/2510.16167v1] provides insights into RLHF, showing that alignment is spatially localized in mid-layer activations. For better reward modeling, \"Think Twice: Branch-and-Rethink Reasoning Reward Model\" [http://arxiv.org/abs/2510.23596v1] introduces BR-RM, a two-turn reward model that reduces \"judgment diffusion\" and improves error sensitivity. \"LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling\" [http://arxiv.org/abs/2510.06915v1] addresses the fragility of current reward models in long-context scenarios. To optimize the efficiency of reward model training, \"Reviving The Classics: Active Reward Modeling in Large Language Model Alignment\" [http://arxiv.org/abs/2502.04354v1] proposes Fisher information-based active learning strategies. \"Learning from Failures: Understanding LLM Alignment through Failure-Aware Inverse RL\" [http://arxiv.org/abs/2510.06092v1] enhances Inverse RL by focusing on misclassified examples to better extract latent reward signals.\\n*   **Reliability and Bias:** LLMs as research assistants exhibit \"consistent reliability failures\" in tasks like citation retrieval and content extraction, as revealed by the PaperAsk benchmark (\"PaperAsk: A Benchmark for Reliability Evaluation of LLMs in Paper Search and Reading\" [http://arxiv.org/abs/2510.22242v1]). Concerns about bias persist, with \"The power of Prompts: Evaluating and Mitigating Gender Bias in MT with LLMs\" [http://arxiv.org/abs/2407.18786v1] demonstrating prompt engineering\\'s ability to reduce gender bias in machine translation. Multilingual contexts pose unique safety challenges, as LLMs are more prone to generating unsafe responses in lower-resource languages (\"The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts\" [http://arxiv.org/abs/2401.13136v1]). This is further complicated by the potential for persona-targeted disinformation, where personalization strategies increase jailbreak likelihood and persuasiveness (\"A Multilingual, Large-Scale Study of the Interplay between LLM Safeguards, Personalisation, and Disinformation\" [http://arxiv.org/abs/2510.12993v1]).\\n*   **Interpretability and Unintended Behaviors:** Understanding LLM internal workings is critical. \"Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework\" [http://arxiv.org/abs/2509.18127v2] proposes a framework for interpreting safety-related features. \"Empirical Investigation of Latent Representational Dynamics in Large Language Models\" [http://arxiv.org/abs/2505.20340v2] introduces DMET to characterize latent dynamics and link them to text quality. Other issues include \"mode collapse,\" where aligned models lose the ability to assume diverse perspectives (\"Detecting Mode Collapse in Language Models via Narration\" [http://arxiv.org/abs/2402.04477v1]), and \"word overuse\" induced by human feedback (\"Word Overuse and Alignment in Large Language Models\" [http://arxiv.org/abs/2508.01930v1]). LLMs can also exhibit deceptive behavior, with \"Evaluating & Reducing Deceptive Dialogue From Language Models with Multi-turn RL\" [http://arxiv.org/abs/2510.14318v1] showing multi-turn RL can reduce deceptiveness by 77.6%.\\n\\n### Applications and Domain-Specific Advancements\\n\\nLLMs are being adapted and specialized for a growing array of domain-specific tasks and low-resource contexts:\\n*   **Healthcare and Clinical NLP:** LLMs show strong potential in healthcare. \"Exploring Scaling Laws for EHR Foundation Models\" [http://arxiv.org/abs/2505.22964v2] identifies scaling patterns for EHR foundation models, analogous to LLMs, to guide resource-efficient training. For extracting information from clinical notes, LLM-based approaches \"Automated Extraction of Fluoropyrimidine Treatment and Treatment-Related Toxicities from Clinical Notes Using Natural Language Processing\" [http://arxiv.org/abs/2510.20727v1] outperformed traditional and deep learning methods. However, \"Open (Clinical) LLMs are Sensitive to Instruction Phrasings\" [http://arxiv.org/abs/2407.09429v1] warns that domain-specific LLMs can be brittle to natural variations in clinical instructions, impacting performance and fairness. While \"When Raw Data Prevails: Are Large Language Model Embeddings Effective in Numerical Data Representation for Medical Machine Learning Applications?\" [http://arxiv.org/abs/2408.11854v2] suggests raw data still prevails for numerical EHR data, zero-shot LLM embeddings show competitive results.\\n*   **Low-Resource Languages:** Efforts are being made to extend LLM capabilities to underrepresented languages, such as the \"SentiMaithili: A Benchmark Dataset for Sentiment and Reason Generation for the Low-Resource Maithili Language\" [http://arxiv.org/abs/2510.22160v1], which provides a valuable resource for interpretable sentiment analysis.\\n*   **Specialized NLP Tasks:** LLMs, particularly fine-tuned transformer models, demonstrate superior performance in tasks like hope speech detection (\"Hope Speech Detection in Social Media English Corpora\" [http://arxiv.org/abs/2510.23585v1]) and Czech anaphora resolution, where fine-tuned models significantly outperform prompt-based approaches (\"Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution\" [http://arxiv.org/abs/2506.18091v1]).\\n*   **Scientific Discovery:** \"Protein Large Language Models: A Comprehensive Survey\" [http://arxiv.org/abs/2502.17504v2] highlights the transformative role of Protein LLMs in accelerating protein structure prediction, function annotation, and design.\\n\\n### Conclusion\\n\\nThe advancement of LLMs is characterized by a dynamic interplay between expanding capabilities and addressing critical limitations. Research is pushing the boundaries of architectural efficiency, enabling long-context processing and deployment on constrained hardware through novel quantization and inference strategies. Multimodal LLMs are evolving into sophisticated agents capable of visual planning, embodied control, and nuanced emotional understanding. Simultaneously, a significant body of work is dedicated to understanding and mitigating the inherent challenges of LLMs, particularly concerning safety, alignment, reliability in scholarly tasks, and biases across languages and domains. The development of robust benchmarks, proactive defense mechanisms against jailbreaks, and advanced reward modeling techniques are crucial for fostering trustworthy and responsible LLM deployment. As LLMs become more integrated into specialized domains like healthcare and scientific research, the focus on domain adaptation, interpretability, and addressing specific vulnerabilities will continue to drive future advancements.'\n",
    "\n",
    "# display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "21d5b77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'vulnerabilities in LLMs', 'original_plan': {'plan': [], 'reflection': {'purpose': 'To evaluate whether the refined search strategy successfully identified papers specifically addressing LLM hallucination, its mechanisms, evaluation, and mitigation, thereby filling the identified gaps from the previous iteration.', 'analysis_focus': ['Relevance of top search results to LLM hallucination as a core, distinct topic.', 'Identification of papers discussing hallucination mechanisms, underlying causes, and theoretical frameworks.', 'Assessment of methodologies and metrics proposed for evaluating LLM hallucination.', 'Identification of diverse strategies and techniques for mitigating LLM hallucination.', 'Confirmation that results differentiate hallucination from other LLM vulnerabilities like adversarial attacks.', 'Determination if further refinement or additional searches are needed to comprehensively cover LLM hallucination and its nuances.'], 'rationale': 'This reflection is crucial to confirm that the iterative search process has now effectively captured the specific and nuanced aspects of LLM hallucination. It ensures the research is thorough and addresses the core problem identified in the previous reflection, leading to a complete understanding of this vulnerability rather than general LLM security.'}}, 'plan': [], 'results': {'arxiv': []}, 'reflection': True, 'reflection_notes': \"The retrieved papers are highly sufficient to fulfill the current research plan. Multiple comprehensive survey papers explicitly focusing on 'LLM Hallucination' (e.g., 'Large Language Models Hallucination: A Comprehensive Survey', 'A comprehensive taxonomy of hallucinations in Large Language Models', 'Loki's Dance of Illusions') directly address all key aspects of the analysis focus:\\n\\n1.  **Relevance to LLM hallucination:** The core topic of LLM hallucination is thoroughly covered by these dedicated surveys.\\n2.  **Mechanisms, causes, and theoretical frameworks:** These surveys explicitly mention analyzing root causes, underlying mechanisms, and theoretical frameworks behind hallucination.\\n3.  **Methodologies and metrics for evaluation:** The surveys review existing evaluation benchmarks and metrics. Additionally, 'GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework' provides a concrete, specialized methodology for hallucination evaluation.\\n4.  **Mitigation strategies:** The surveys outline diverse mitigation strategies and solutions for LLM hallucination.\\n5.  **Differentiation from other LLM vulnerabilities:** By providing clear definitions and dedicated discussions of hallucination, these papers inherently differentiate it from other LLM vulnerabilities. While some retrieved papers discuss other vulnerabilities like prompt injection or jailbreaking, the strong focus and comprehensive nature of the hallucination-specific papers ensure the core problem is addressed distinctly.\\n\\nThe presence of several recent, comprehensive survey papers on hallucination, along with a specific evaluation framework, provides a robust foundation for a thorough understanding of this vulnerability as outlined in the research plan.\", 'summary': '## Summary of Vulnerabilities in Large Language Models\\n\\nThe widespread deployment of Large Language Models (LLMs) and AI agents powered by them necessitates a deep understanding of their security vulnerabilities. Recent research highlights several critical areas of concern, ranging from direct adversarial attacks to inherent reliability issues like hallucinations, and specialized risks in multimodal and agentic systems.\\n\\n### Key Vulnerability Categories\\n\\n1.  **Adversarial Prompt Attacks and Jailbreaks**:\\n    *   LLMs are highly susceptible to prompt injection attacks, where malicious inputs can override safety mechanisms or elicit harmful content. \"Towards Reliable and Practical LLM Security Evaluations via Bayesian Modelling\" (http://arxiv.org/abs/2510.05709v1) specifically focuses on evaluating these vulnerabilities, emphasizing the need for robust experimental design and uncertainty quantification.\\n    *   **Jailbreak attacks** are a specific type of prompt attack designed to bypass safety filters and make LLMs generate unethical or unsafe outputs. \"GuardVal: Dynamic Large Language Model Jailbreak Evaluation for Comprehensive Safety Testing\" (http://arxiv.org/abs/2507.07735v1) introduces a dynamic protocol to generate and refine such prompts, revealing distinct robustness levels across models like Mistral-7b and GPT-4.\\n    *   **Multi-turn and intent-driven attacks** represent an evolving threat, requiring more sophisticated evaluation. \"SecReEvalBench: A Multi-turned Security Resilience Evaluation Benchmark for Large Language Models\" (http://arxiv.org/abs/2505.07584v3) addresses this by defining novel metrics and questioning sequences to assess LLM resilience against chained adversarial interactions.\\n\\n2.  **Hallucinations**:\\n    *   Hallucination, defined as the generation of fluent but factually inaccurate or fabricated content, is a pervasive and critical challenge for LLMs, undermining their reliability and trustworthiness. Multiple surveys, including \"Large Language Models Hallucination: A Comprehensive Survey\" (http://arxiv.org/abs/2510.06265v2), \"A comprehensive taxonomy of hallucinations in Large Language Models\" (http://arxiv.org/abs/2508.01781v1), and \"Loki\\'s Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models\" (http://arxiv.org/abs/2507.02870v1), provide extensive taxonomies of hallucination types (e.g., intrinsic vs. extrinsic, factual errors, logical inconsistencies) and analyze their root causes across the entire LLM development lifecycle (data, model architecture, prompt). These papers highlight the potential for substantial economic, legal, and health risks due to misinformation.\\n    *   The theoretical inevitability of hallucinations in computable LLMs is underscored, emphasizing the need for robust detection and mitigation rather than complete elimination.\\n\\n3.  **Multimodal Specific Vulnerabilities**:\\n    *   The integration of diverse modalities, particularly image inputs, in Multimodal Large Language Models (MLLMs) introduces new security challenges. \"Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security\" (http://arxiv.org/abs/2404.05264v2) details how the rich semantic information and continuous mathematical nature of image modalities can be exploited by attackers for highly covert and harmful attacks, necessitating a specialized threat model for MLLMs.\\n\\n4.  **Agent-Level Vulnerabilities**:\\n    *   When LLMs serve as \"backbones\" for AI agents, their inherent vulnerabilities can propagate and entangle with conventional software security risks. \"Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI Agents\" (http://arxiv.org/abs/2510.22620v1) introduces the \"threat snapshots\" framework to isolate and systematically categorize these security risks at the agent level. Their evaluation of 31 popular LLMs revealed that enhanced reasoning capabilities can improve security, while model size does not necessarily correlate.\\n\\n### Evaluation and Mitigation Strategies\\n\\nAddressing these vulnerabilities requires sophisticated evaluation methodologies and novel mitigation approaches:\\n\\n*   **Advanced Evaluation Frameworks**: Researchers are developing new benchmarks and frameworks to systematically assess LLM security. These include:\\n    *   **Threat snapshots** for AI agents (http://arxiv.org/abs/2510.22620v1).\\n    *   **Bayesian hierarchical models** for robust prompt injection evaluations, accounting for output variability and limited compute (http://arxiv.org/abs/2510.05709v1).\\n    *   **Dynamic jailbreak evaluation protocols** like GuardVal, which adapt prompts based on the defender\\'s state (http://arxiv.org/abs/2507.07735v1).\\n    *   **Multi-turn resilience benchmarks** such as SecReEvalBench, which consider complex attack sequences and diverse security domains (http://arxiv.org/abs/2505.07584v3).\\n    *   **Knowledge-Graph Based Hallucination Evaluation**: \"GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation Framework\" (http://arxiv.org/abs/2407.10793v1) offers a method to provide explainable decisions by identifying specific information prone to hallucination within a knowledge graph, demonstrating improved accuracy over raw natural language inference models.\\n\\n*   **Mitigation Approaches**:\\n    *   **Architectural improvements and training data considerations** are crucial for reducing hallucinations, as discussed in the comprehensive surveys (e.g., http://arxiv.org/abs/2510.06265v2).\\n    *   **Hallucination Correction**: GraphEval also explores \"GraphCorrect,\" a method leveraging knowledge graph structures to rectify hallucinations, showing promising results (http://arxiv.org/abs/2407.10793v1).\\n    *   **Blockchain Integration**: \"Blockchain for Large Language Model Security and Safety: A Holistic Survey\" (http://arxiv.org/abs/2407.20181v2) proposes leveraging blockchain technology\\'s data immutability and decentralized structure as a promising foundation to enhance LLMs\\' security against issues like data poisoning, prompt injections, and unauthorized data exposure.\\n\\nIn conclusion, LLM vulnerabilities are complex and multifaceted, requiring continuous research into robust evaluation, detection, and mitigation strategies. The inherent nature of some issues, such as hallucination, suggests that ongoing human oversight will remain critical for responsible and reliable LLM deployment in sensitive applications.', 'relevant_docs': [\"Title: Breaking Agent Backbones: Evaluating the Security of Backbone LLMs in AI\\n  Agents\\nSummary:\\nAI agents powered by large language models (LLMs) are being deployed at\\nscale, yet we lack a systematic understanding of how the choice of backbone LLM\\naffects agent security. The non-deterministic sequential nature of AI agents\\ncomplicates security modeling, while the integration of traditional software\\nwith AI components entangles novel LLM vulnerabilities with conventional\\nsecurity risks. Existing frameworks only partially address these challenges as\\nthey either capture specific vulnerabilities only or require modeling of\\ncomplete agents. To address these limitations, we introduce threat snapshots: a\\nframework that isolates specific states in an agent's execution flow where LLM\\nvulnerabilities manifest, enabling the systematic identification and\\ncategorization of security risks that propagate from the LLM to the agent\\nlevel. We apply this framework to construct the $\\\\operatorname{b}^3$ benchmark,\\na security benchmark based on 194331 unique crowdsourced adversarial attacks.\\nWe then evaluate 31 popular LLMs with it, revealing, among other insights, that\\nenhanced reasoning capabilities improve security, while model size does not\\ncorrelate with security. We release our benchmark, dataset, and evaluation code\\nto facilitate widespread adoption by LLM providers and practitioners, offering\\nguidance for agent developers and incentivizing model developers to prioritize\\nbackbone security improvements.\\nLink: http://arxiv.org/abs/2510.22620v1\", 'Title: Towards Reliable and Practical LLM Security Evaluations via Bayesian\\n  Modelling\\nSummary:\\nBefore adopting a new large language model (LLM) architecture, it is critical\\nto understand vulnerabilities accurately. Existing evaluations can be difficult\\nto trust, often drawing conclusions from LLMs that are not meaningfully\\ncomparable, relying on heuristic inputs or employing metrics that fail to\\ncapture the inherent uncertainty. In this paper, we propose a principled and\\npractical end-to-end framework for evaluating LLM vulnerabilities to prompt\\ninjection attacks. First, we propose practical approaches to experimental\\ndesign, tackling unfair LLM comparisons by considering two practitioner\\nscenarios: when training an LLM and when deploying a pre-trained LLM. Second,\\nwe address the analysis of experiments and propose a Bayesian hierarchical\\nmodel with embedding-space clustering. This model is designed to improve\\nuncertainty quantification in the common scenario that LLM outputs are not\\ndeterministic, test prompts are designed imperfectly, and practitioners only\\nhave a limited amount of compute to evaluate vulnerabilities. We show the\\nimproved inferential capabilities of the model in several prompt injection\\nattack settings. Finally, we demonstrate the pipeline to evaluate the security\\nof Transformer versus Mamba architectures. Our findings show that consideration\\nof output variability can suggest less definitive findings. However, for some\\nattacks, we find notably increased Transformer and Mamba-variant\\nvulnerabilities across LLMs with the same training data or mathematical\\nability.\\nLink: http://arxiv.org/abs/2510.05709v1', \"Title: GuardVal: Dynamic Large Language Model Jailbreak Evaluation for\\n  Comprehensive Safety Testing\\nSummary:\\nJailbreak attacks reveal critical vulnerabilities in Large Language Models\\n(LLMs) by causing them to generate harmful or unethical content. Evaluating\\nthese threats is particularly challenging due to the evolving nature of LLMs\\nand the sophistication required in effectively probing their vulnerabilities.\\nCurrent benchmarks and evaluation methods struggle to fully address these\\nchallenges, leaving gaps in the assessment of LLM vulnerabilities. In this\\npaper, we review existing jailbreak evaluation practices and identify three\\nassumed desiderata for an effective jailbreak evaluation protocol. To address\\nthese challenges, we introduce GuardVal, a new evaluation protocol that\\ndynamically generates and refines jailbreak prompts based on the defender LLM's\\nstate, providing a more accurate assessment of defender LLMs' capacity to\\nhandle safety-critical situations. Moreover, we propose a new optimization\\nmethod that prevents stagnation during prompt refinement, ensuring the\\ngeneration of increasingly effective jailbreak prompts that expose deeper\\nweaknesses in the defender LLMs. We apply this protocol to a diverse set of\\nmodels, from Mistral-7b to GPT-4, across 10 safety domains. Our findings\\nhighlight distinct behavioral patterns among the models, offering a\\ncomprehensive view of their robustness. Furthermore, our evaluation process\\ndeepens the understanding of LLM behavior, leading to insights that can inform\\nfuture research and drive the development of more secure models.\\nLink: http://arxiv.org/abs/2507.07735v1\", 'Title: SecReEvalBench: A Multi-turned Security Resilience Evaluation Benchmark\\n  for Large Language Models\\nSummary:\\nThe increasing deployment of large language models in security-sensitive\\ndomains necessitates rigorous evaluation of their resilience against\\nadversarial prompt-based attacks. While previous benchmarks have focused on\\nsecurity evaluations with limited and predefined attack domains, such as\\ncybersecurity attacks, they often lack a comprehensive assessment of\\nintent-driven adversarial prompts and the consideration of real-life\\nscenario-based multi-turn attacks. To address this gap, we present\\nSecReEvalBench, the Security Resilience Evaluation Benchmark, which defines\\nfour novel metrics: Prompt Attack Resilience Score, Prompt Attack Refusal Logic\\nScore, Chain-Based Attack Resilience Score and Chain-Based Attack Rejection\\nTime Score. Moreover, SecReEvalBench employs six questioning sequences for\\nmodel assessment: one-off attack, successive attack, successive reverse attack,\\nalternative attack, sequential ascending attack with escalating threat levels\\nand sequential descending attack with diminishing threat levels. In addition,\\nwe introduce a dataset customized for the benchmark, which incorporates both\\nneutral and malicious prompts, categorised across seven security domains and\\nsixteen attack techniques. In applying this benchmark, we systematically\\nevaluate five state-of-the-art open-weighted large language models, Llama 3.1,\\nGemma 2, Mistral v0.3, DeepSeek-R1 and Qwen 3. Our findings offer critical\\ninsights into the strengths and weaknesses of modern large language models in\\ndefending against evolving adversarial threats. The SecReEvalBench dataset is\\npublicly available at\\nhttps://kaggle.com/datasets/5a7ee22cf9dab6c93b55a73f630f6c9b42e936351b0ae98fbae6ddaca7fe248d,\\nwhich provides a groundwork for advancing research in large language model\\nsecurity.\\nLink: http://arxiv.org/abs/2505.07584v3', \"Title: Blockchain for Large Language Model Security and Safety: A Holistic\\n  Survey\\nSummary:\\nWith the growing development and deployment of large language models (LLMs)\\nin both industrial and academic fields, their security and safety concerns have\\nbecome increasingly critical. However, recent studies indicate that LLMs face\\nnumerous vulnerabilities, including data poisoning, prompt injections, and\\nunauthorized data exposure, which conventional methods have struggled to\\naddress fully. In parallel, blockchain technology, known for its data\\nimmutability and decentralized structure, offers a promising foundation for\\nsafeguarding LLMs. In this survey, we aim to comprehensively assess how to\\nleverage blockchain technology to enhance LLMs' security and safety. Besides,\\nwe propose a new taxonomy of blockchain for large language models (BC4LLMs) to\\nsystematically categorize related works in this emerging field. Our analysis\\nincludes novel frameworks and definitions to delineate security and safety in\\nthe context of BC4LLMs, highlighting potential research directions and\\nchallenges at this intersection. Through this study, we aim to stimulate\\ntargeted advancements in blockchain-integrated LLM security.\\nLink: http://arxiv.org/abs/2407.20181v2\", \"Title: Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in\\n  Multimodal Large Language Model Security\\nSummary:\\nMultimodal Large Language Models (MLLMs) demonstrate remarkable capabilities\\nthat increasingly influence various aspects of our daily lives, constantly\\ndefining the new boundary of Artificial General Intelligence (AGI). Image\\nmodalities, enriched with profound semantic information and a more continuous\\nmathematical nature compared to other modalities, greatly enhance the\\nfunctionalities of MLLMs when integrated. However, this integration serves as a\\ndouble-edged sword, providing attackers with expansive vulnerabilities to\\nexploit for highly covert and harmful attacks. The pursuit of reliable AI\\nsystems like powerful MLLMs has emerged as a pivotal area of contemporary\\nresearch. In this paper, we endeavor to demostrate the multifaceted risks\\nassociated with the incorporation of image modalities into MLLMs. Initially, we\\ndelineate the foundational components and training processes of MLLMs.\\nSubsequently, we construct a threat model, outlining the security\\nvulnerabilities intrinsic to MLLMs. Moreover, we analyze and summarize existing\\nscholarly discourses on MLLMs' attack and defense mechanisms, culminating in\\nsuggestions for the future research on MLLM security. Through this\\ncomprehensive analysis, we aim to deepen the academic understanding of MLLM\\nsecurity challenges and propel forward the development of trustworthy MLLM\\nsystems.\\nLink: http://arxiv.org/abs/2404.05264v2\", 'Title: Large Language Models Hallucination: A Comprehensive Survey\\nSummary:\\nLarge language models (LLMs) have transformed natural language processing,\\nachieving remarkable performance across diverse tasks. However, their\\nimpressive fluency often comes at the cost of producing false or fabricated\\ninformation, a phenomenon known as hallucination. Hallucination refers to the\\ngeneration of content by an LLM that is fluent and syntactically correct but\\nfactually inaccurate or unsupported by external evidence. Hallucinations\\nundermine the reliability and trustworthiness of LLMs, especially in domains\\nrequiring factual accuracy. This survey provides a comprehensive review of\\nresearch on hallucination in LLMs, with a focus on causes, detection, and\\nmitigation. We first present a taxonomy of hallucination types and analyze\\ntheir root causes across the entire LLM development lifecycle, from data\\ncollection and architecture design to inference. We further examine how\\nhallucinations emerge in key natural language generation tasks. Building on\\nthis foundation, we introduce a structured taxonomy of detection approaches and\\nanother taxonomy of mitigation strategies. We also analyze the strengths and\\nlimitations of current detection and mitigation approaches and review existing\\nevaluation benchmarks and metrics used to quantify LLMs hallucinations.\\nFinally, we outline key open challenges and promising directions for future\\nresearch, providing a foundation for the development of more truthful and\\ntrustworthy LLMs.\\nLink: http://arxiv.org/abs/2510.06265v2', 'Title: A comprehensive taxonomy of hallucinations in Large Language Models\\nSummary:\\nLarge language models (LLMs) have revolutionized natural language processing,\\nyet their propensity for hallucination, generating plausible but factually\\nincorrect or fabricated content, remains a critical challenge. This report\\nprovides a comprehensive taxonomy of LLM hallucinations, beginning with a\\nformal definition and a theoretical framework that posits its inherent\\ninevitability in computable LLMs, irrespective of architecture or training. It\\nexplores core distinctions, differentiating between intrinsic (contradicting\\ninput context) and extrinsic (inconsistent with training data or reality), as\\nwell as factuality (absolute correctness) and faithfulness (adherence to\\ninput). The report then details specific manifestations, including factual\\nerrors, contextual and logical inconsistencies, temporal disorientation,\\nethical violations, and task-specific hallucinations across domains like code\\ngeneration and multimodal applications. It analyzes the underlying causes,\\ncategorizing them into data-related issues, model-related factors, and\\nprompt-related influences. Furthermore, the report examines cognitive and human\\nfactors influencing hallucination perception, surveys evaluation benchmarks and\\nmetrics for detection, and outlines architectural and systemic mitigation\\nstrategies. Finally, it introduces web-based resources for monitoring LLM\\nreleases and performance. This report underscores the complex, multifaceted\\nnature of LLM hallucinations and emphasizes that, given their theoretical\\ninevitability, future efforts must focus on robust detection, mitigation, and\\ncontinuous human oversight for responsible and reliable deployment in critical\\napplications.\\nLink: http://arxiv.org/abs/2508.01781v1', 'Title: Loki\\'s Dance of Illusions: A Comprehensive Survey of Hallucination in\\n  Large Language Models\\nSummary:\\nEdgar Allan Poe noted, \"Truth often lurks in the shadow of error,\"\\nhighlighting the deep complexity intrinsic to the interplay between truth and\\nfalsehood, notably under conditions of cognitive and informational asymmetry.\\nThis dynamic is strikingly evident in large language models (LLMs). Despite\\ntheir impressive linguistic generation capabilities, LLMs sometimes produce\\ninformation that appears factually accurate but is, in reality, fabricated, an\\nissue often referred to as \\'hallucinations\\'. The prevalence of these\\nhallucinations can mislead users, affecting their judgments and decisions. In\\nsectors such as finance, law, and healthcare, such misinformation risks causing\\nsubstantial economic losses, legal disputes, and health risks, with\\nwide-ranging consequences.In our research, we have methodically categorized,\\nanalyzed the causes, detection methods, and solutions related to LLM\\nhallucinations. Our efforts have particularly focused on understanding the\\nroots of hallucinations and evaluating the efficacy of current strategies in\\nrevealing the underlying logic, thereby paving the way for the development of\\ninnovative and potent approaches. By examining why certain measures are\\neffective against hallucinations, our study aims to foster a comprehensive\\napproach to tackling this issue within the domain of LLMs.\\nLink: http://arxiv.org/abs/2507.02870v1', 'Title: Large Language Models Hallucination: A Comprehensive Survey\\nSummary:\\nLarge language models (LLMs) have transformed natural language processing,\\nachieving remarkable performance across diverse tasks. However, their\\nimpressive fluency often comes at the cost of producing false or fabricated\\ninformation, a phenomenon known as hallucination. Hallucination refers to the\\ngeneration of content by an LLM that is fluent and syntactically correct but\\nfactually inaccurate or unsupported by external evidence. Hallucinations\\nundermine the reliability and trustworthiness of LLMs, especially in domains\\nrequiring factual accuracy. This survey provides a comprehensive review of\\nresearch on hallucination in LLMs, with a focus on causes, detection, and\\nmitigation. We first present a taxonomy of hallucination types and analyze\\ntheir root causes across the entire LLM development lifecycle, from data\\ncollection and architecture design to inference. We further examine how\\nhallucinations emerge in key natural language generation tasks. Building on\\nthis foundation, we introduce a structured taxonomy of detection approaches and\\nanother taxonomy of mitigation strategies. We also analyze the strengths and\\nlimitations of current detection and mitigation approaches and review existing\\nevaluation benchmarks and metrics used to quantify LLMs hallucinations.\\nFinally, we outline key open challenges and promising directions for future\\nresearch, providing a foundation for the development of more truthful and\\ntrustworthy LLMs.\\nLink: http://arxiv.org/abs/2510.06265v2', 'Title: A comprehensive taxonomy of hallucinations in Large Language Models\\nSummary:\\nLarge language models (LLMs) have revolutionized natural language processing,\\nyet their propensity for hallucination, generating plausible but factually\\nincorrect or fabricated content, remains a critical challenge. This report\\nprovides a comprehensive taxonomy of LLM hallucinations, beginning with a\\nformal definition and a theoretical framework that posits its inherent\\ninevitability in computable LLMs, irrespective of architecture or training. It\\nexplores core distinctions, differentiating between intrinsic (contradicting\\ninput context) and extrinsic (inconsistent with training data or reality), as\\nwell as factuality (absolute correctness) and faithfulness (adherence to\\ninput). The report then details specific manifestations, including factual\\nerrors, contextual and logical inconsistencies, temporal disorientation,\\nethical violations, and task-specific hallucinations across domains like code\\ngeneration and multimodal applications. It analyzes the underlying causes,\\ncategorizing them into data-related issues, model-related factors, and\\nprompt-related influences. Furthermore, the report examines cognitive and human\\nfactors influencing hallucination perception, surveys evaluation benchmarks and\\nmetrics for detection, and outlines architectural and systemic mitigation\\nstrategies. Finally, it introduces web-based resources for monitoring LLM\\nreleases and performance. This report underscores the complex, multifaceted\\nnature of LLM hallucinations and emphasizes that, given their theoretical\\ninevitability, future efforts must focus on robust detection, mitigation, and\\ncontinuous human oversight for responsible and reliable deployment in critical\\napplications.\\nLink: http://arxiv.org/abs/2508.01781v1', 'Title: Loki\\'s Dance of Illusions: A Comprehensive Survey of Hallucination in\\n  Large Language Models\\nSummary:\\nEdgar Allan Poe noted, \"Truth often lurks in the shadow of error,\"\\nhighlighting the deep complexity intrinsic to the interplay between truth and\\nfalsehood, notably under conditions of cognitive and informational asymmetry.\\nThis dynamic is strikingly evident in large language models (LLMs). Despite\\ntheir impressive linguistic generation capabilities, LLMs sometimes produce\\ninformation that appears factually accurate but is, in reality, fabricated, an\\nissue often referred to as \\'hallucinations\\'. The prevalence of these\\nhallucinations can mislead users, affecting their judgments and decisions. In\\nsectors such as finance, law, and healthcare, such misinformation risks causing\\nsubstantial economic losses, legal disputes, and health risks, with\\nwide-ranging consequences.In our research, we have methodically categorized,\\nanalyzed the causes, detection methods, and solutions related to LLM\\nhallucinations. Our efforts have particularly focused on understanding the\\nroots of hallucinations and evaluating the efficacy of current strategies in\\nrevealing the underlying logic, thereby paving the way for the development of\\ninnovative and potent approaches. By examining why certain measures are\\neffective against hallucinations, our study aims to foster a comprehensive\\napproach to tackling this issue within the domain of LLMs.\\nLink: http://arxiv.org/abs/2507.02870v1', 'Title: GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation\\n  Framework\\nSummary:\\nMethods to evaluate Large Language Model (LLM) responses and detect\\ninconsistencies, also known as hallucinations, with respect to the provided\\nknowledge, are becoming increasingly important for LLM applications. Current\\nmetrics fall short in their ability to provide explainable decisions,\\nsystematically check all pieces of information in the response, and are often\\ntoo computationally expensive to be used in practice. We present GraphEval: a\\nhallucination evaluation framework based on representing information in\\nKnowledge Graph (KG) structures. Our method identifies the specific triples in\\nthe KG that are prone to hallucinations and hence provides more insight into\\nwhere in the response a hallucination has occurred, if at all, than previous\\nmethods. Furthermore, using our approach in conjunction with state-of-the-art\\nnatural language inference (NLI) models leads to an improvement in balanced\\naccuracy on various hallucination benchmarks, compared to using the raw NLI\\nmodels. Lastly, we explore the use of GraphEval for hallucination correction by\\nleveraging the structure of the KG, a method we name GraphCorrect, and\\ndemonstrate that the majority of hallucinations can indeed be rectified.\\nLink: http://arxiv.org/abs/2407.10793v1', 'Title: GraphEval: A Knowledge-Graph Based LLM Hallucination Evaluation\\n  Framework\\nSummary:\\nMethods to evaluate Large Language Model (LLM) responses and detect\\ninconsistencies, also known as hallucinations, with respect to the provided\\nknowledge, are becoming increasingly important for LLM applications. Current\\nmetrics fall short in their ability to provide explainable decisions,\\nsystematically check all pieces of information in the response, and are often\\ntoo computationally expensive to be used in practice. We present GraphEval: a\\nhallucination evaluation framework based on representing information in\\nKnowledge Graph (KG) structures. Our method identifies the specific triples in\\nthe KG that are prone to hallucinations and hence provides more insight into\\nwhere in the response a hallucination has occurred, if at all, than previous\\nmethods. Furthermore, using our approach in conjunction with state-of-the-art\\nnatural language inference (NLI) models leads to an improvement in balanced\\naccuracy on various hallucination benchmarks, compared to using the raw NLI\\nmodels. Lastly, we explore the use of GraphEval for hallucination correction by\\nleveraging the structure of the KG, a method we name GraphCorrect, and\\ndemonstrate that the majority of hallucinations can indeed be rectified.\\nLink: http://arxiv.org/abs/2407.10793v1']}\n"
     ]
    }
   ],
   "source": [
    "print(final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b33070",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
