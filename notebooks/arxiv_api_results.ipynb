{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0321c4fb",
   "metadata": {},
   "source": [
    "Understand arxiv api results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6954e844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\\n  <link href=\"http://arxiv.org/api/query?search_query%3Dall%3Aelectron%26id_list%3D%26start%3D0%26max_results%3D1\" rel=\"self\" type=\"application/atom+xml\"/>\\n  <title type=\"html\">ArXiv Query: search_query=all:electron&amp;id_list=&amp;start=0&amp;max_results=1</title>\\n  <id>http://arxiv.org/api/cHxbiOdZaP56ODnBPIenZhzg5f8</id>\\n  <updated>2025-10-23T00:00:00-04:00</updated>\\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">231429</opensearch:totalResults>\\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:itemsPerPage>\\n  <entry>\\n    <id>http://arxiv.org/abs/cond-mat/0102536v1</id>\\n    <updated>2001-02-28T20:12:09Z</updated>\\n    <published>2001-02-28T20:12:09Z</published>\\n    <title>Impact of Electron-Electron Cusp on Configuration Interaction Energies</title>\\n    <summary>  The effect of the electron-electron cusp on the convergence of configuration\\ninteraction (CI) wave functions is examined. By analogy with the\\npseudopotential approach for electron-ion interactions, an effective\\nelectron-electron interaction is developed which closely reproduces the\\nscattering of the Coulomb interaction but is smooth and finite at zero\\nelectron-electron separation. The exact many-electron wave function for this\\nsmooth effective interaction has no cusp at zero electron-electron separation.\\nWe perform CI and quantum Monte Carlo calculations for He and Be atoms, both\\nwith the Coulomb electron-electron interaction and with the smooth effective\\nelectron-electron interaction. We find that convergence of the CI expansion of\\nthe wave function for the smooth electron-electron interaction is not\\nsignificantly improved compared with that for the divergent Coulomb interaction\\nfor energy differences on the order of 1 mHartree. This shows that, contrary to\\npopular belief, description of the electron-electron cusp is not a limiting\\nfactor, to within chemical accuracy, for CI calculations.\\n</summary>\\n    <author>\\n      <name>David Prendergast</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Department of Physics</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>M. Nolan</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NMRC, University College, Cork, Ireland</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Claudia Filippi</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Department of Physics</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>Stephen Fahy</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Department of Physics</arxiv:affiliation>\\n    </author>\\n    <author>\\n      <name>J. C. Greer</name>\\n      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">NMRC, University College, Cork, Ireland</arxiv:affiliation>\\n    </author>\\n    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1063/1.1383585</arxiv:doi>\\n    <link title=\"doi\" href=\"http://dx.doi.org/10.1063/1.1383585\" rel=\"related\"/>\\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">11 pages, 6 figures, 3 tables, LaTeX209, submitted to The Journal of\\n  Chemical Physics</arxiv:comment>\\n    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">J. Chem. Phys. 115, 1626 (2001)</arxiv:journal_ref>\\n    <link href=\"http://arxiv.org/abs/cond-mat/0102536v1\" rel=\"alternate\" type=\"text/html\"/>\\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/cond-mat/0102536v1\" rel=\"related\" type=\"application/pdf\"/>\\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cond-mat.str-el\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n    <category term=\"cond-mat.str-el\" scheme=\"http://arxiv.org/schemas/atom\"/>\\n  </entry>\\n</feed>\\n'\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as libreq\n",
    "with libreq.urlopen('http://export.arxiv.org/api/query?search_query=all:electron&start=0&max_results=1') as url:\n",
    "    r = url.read()\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3810cae",
   "metadata": {},
   "source": [
    "Let's see the output of the planner stage, to determine the kind of queries to pass to the arxiv tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cda19a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31e38392",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "    You are an expert research agent planner.\n",
    "\n",
    "    You have access to the following tools:\n",
    "    - arxiv_search: for academic papers\n",
    "    - web_search: for general sources\n",
    "\n",
    "    Your job is to take a user query and return a structured execution plan in STRICT JSON format.\n",
    "    Do not include any text outside of the JSON. Do not explain your reasoning in prose. \n",
    "\n",
    "    The JSON must follow this schema exactly:\n",
    "\n",
    "    {\n",
    "    \"plan\": [\n",
    "        {\n",
    "        \"tool\": \"<tool_name>\",\n",
    "        \"purpose\": \"<why this step is included>\",\n",
    "        \"query\": {\n",
    "            \"search_terms\": [\"<list of exact search terms>\"],\n",
    "            \"additional_focus\": [\"<list of optional focus keywords>\"]\n",
    "        },\n",
    "        \"rationale\": \"<why these parameters were chosen>\"\n",
    "        }\n",
    "    ],\n",
    "    \"reflection\": {\n",
    "        \"purpose\": \"<why reflection is needed>\",\n",
    "        \"analysis_focus\": [\"<list of aspects to check>\"],\n",
    "        \"rationale\": \"<why this reflection matters>\"\n",
    "    }\n",
    "    }\n",
    "\n",
    "    Return only valid JSON. Do not include markdown formatting, explanations, or extra text.\n",
    "    \"\"\"\n",
    "\n",
    "# user_query = \"write a report on evolution of LLMs\"\n",
    "user_query = \"comparison of different image generation models\"\n",
    "response = llm.invoke(system_prompt + \"\\n\" + user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5aff8f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"plan\": [\n",
      "    {\n",
      "    \"tool\": \"web_search\",\n",
      "    \"purpose\": \"To get a general overview of the major types of image generation models, their evolution, and common criteria used for comparison. This helps in identifying key models and categories before diving into deeper research.\",\n",
      "    \"query\": {\n",
      "        \"search_terms\": [\"image generation models comparison overview\", \"types of generative models for images\"],\n",
      "        \"additional_focus\": [\"GANs\", \"VAEs\", \"Diffusion Models\", \"autoregressive models\", \"strengths\", \"weaknesses\", \"applications\"]\n",
      "    },\n",
      "    \"rationale\": \"Starting with a broad web search provides a foundational understanding, identifies prominent models and model types, and common comparison points, which guides subsequent, more focused searches.\"\n",
      "    },\n",
      "    {\n",
      "    \"tool\": \"arxiv_search\",\n",
      "    \"purpose\": \"To find academic papers, surveys, or review articles that specifically compare different image generation models, focusing on technical details, performance metrics, and architectural differences.\",\n",
      "    \"query\": {\n",
      "        \"search_terms\": [\"survey image generation models\", \"comparison generative models image synthesis\", \"review diffusion models GANs VAEs\"],\n",
      "        \"additional_focus\": [\"performance metrics\", \"FID\", \"IS\", \"perceptual quality\", \"diversity\", \"computational cost\", \"architectures\", \"benchmarking\"]\n",
      "    },\n",
      "    \"rationale\": \"Arxiv is crucial for accessing peer-reviewed research that provides rigorous and detailed comparisons, often including quantitative evaluations and in-depth technical analyses of various models.\"\n",
      "    },\n",
      "    {\n",
      "    \"tool\": \"web_search\",\n",
      "    \"purpose\": \"To gather information on more recent and practical comparisons, including user experience, specific model benchmarks (e.g., DALL-E vs. Stable Diffusion), and current industry trends or applications.\",\n",
      "    \"query\": {\n",
      "        \"search_terms\": [\"best image generation models comparison\", \"DALL-E vs Stable Diffusion vs Midjourney\", \"image generation model benchmarks 2023\"],\n",
      "        \"additional_focus\": [\"quality\", \"speed\", \"cost\", \"open source vs proprietary\", \"real-world applications\", \"user perspective\"]\n",
      "    },\n",
      "    \"rationale\": \"While academic papers provide theoretical depth, web searches can offer up-to-date practical comparisons, often with visual examples, user feedback, and insights into the latest commercial or open-source offerings that might not yet be extensively covered in academic surveys.\"\n",
      "    }\n",
      "],\n",
      "\"reflection\": {\n",
      "    \"purpose\": \"To ensure that the gathered information provides a comprehensive and balanced comparison of image generation models, covering various aspects from technical details to practical implications.\",\n",
      "    \"analysis_focus\": [\n",
      "        \"Major model categories (GANs, VAEs, Diffusion Models, Autoregressive) are sufficiently covered.\",\n",
      "        \"Key models within each category are identified and compared.\",\n",
      "        \"Technical differences and architectural nuances are understood.\",\n",
      "        \"Performance metrics and evaluation criteria are clear.\",\n",
      "        \"Strengths and weaknesses of each model type are highlighted.\",\n",
      "        \"Recent advancements and popular commercial/open-source models are included.\",\n",
      "        \"Practical considerations like speed, cost, and ease of use are addressed.\"\n",
      "    ],\n",
      "    \"rationale\": \"A thorough comparison requires understanding not just *what* the models are, but *how* they work, *how* they are evaluated, and their practical implications. This reflection ensures that the research plan addresses all these critical dimensions for a complete comparison.\"\n",
      "}\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19af5d46",
   "metadata": {},
   "source": [
    "Now that we have the plan output, let's take one arxiv tool call and make the LLM generate an arxiv search query. The goal of this step is query expansion, giving search terms to the LLM and getting eact search queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b50d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool_call = {\n",
    "#     \"tool\": \"arxiv_search\",\n",
    "#     \"purpose\": \"To identify the pivotal paper introducing the Transformer architecture, which is a cornerstone for modern LLMs.\",\n",
    "#     \"query\": {\n",
    "#         \"search_terms\": [\n",
    "#             \"Attention Is All You Need\"\n",
    "#         ],\n",
    "#         \"additional_focus\": [\n",
    "#             \"transformer architecture\",\n",
    "#             \"sequence transduction\"\n",
    "#         ]\n",
    "#     },\n",
    "#     \"rationale\": \"The Transformer paper marked a significant paradigm shift in neural network architectures for sequence modeling, directly leading to current LLMs.\"\n",
    "# }\n",
    "\n",
    "tool_call = {\n",
    "    \"tool\": \"arxiv_search\",\n",
    "    \"purpose\": \"To find academic papers, surveys, or review articles that specifically compare different image generation models, focusing on technical details, performance metrics, and architectural differences.\",\n",
    "    \"query\": {\n",
    "        \"search_terms\": [\"survey image generation models\", \"comparison generative models image synthesis\", \"review diffusion models GANs VAEs\"],\n",
    "        \"additional_focus\": [\"performance metrics\", \"FID\", \"IS\", \"perceptual quality\", \"diversity\", \"computational cost\", \"architectures\", \"benchmarking\"]\n",
    "    },\n",
    "    \"rationale\": \"Arxiv is crucial for accessing peer-reviewed research that provides rigorous and detailed comparisons, often including quantitative evaluations and in-depth technical analyses of various models.\"\n",
    "    }\n",
    "\n",
    "search_terms = tool_call[\"query\"][\"search_terms\"]\n",
    "additional_focus = tool_call[\"query\"][\"additional_focus\"]\n",
    "\n",
    "query_expansion_prompt = f\"\"\"\n",
    "You are an expert at constructing arxiv API queries. \n",
    "Given the following search terms and additional focus terms, generate efficient arXiv API queries.\n",
    "\n",
    "Requirements:\n",
    "- Always include the exact search_terms verbatim.\n",
    "- Incorporate additional_focus terms.\n",
    "- Use arXiv field prefixes where appropriate:\n",
    "  - ti: for title\n",
    "  - abs: for abstract\n",
    "  - cat:cs.CL for computational linguistics\n",
    "- Combine terms with AND/OR for precision.\n",
    "- Return 2-3 queries max.\n",
    "\n",
    "Search terms: {search_terms}\n",
    "Additional focus: {additional_focus}\n",
    "\n",
    "Return only a JSON list of objects. \n",
    "Each object must have:\n",
    "- \"search_query\": a valid arXiv API query string\n",
    "- \"max_results\": an integer (default 5)\n",
    "\n",
    "Do not include explanations, markdown, or extra text.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "queries = llm.invoke(query_expansion_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2612474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[\n",
      "  {\n",
      "    \"search_query\": \"ti:\\\"Attention Is All You Need\\\" AND (abs:\\\"transformer architecture\\\" OR abs:\\\"sequence transduction\\\") AND cat:cs.CL\",\n",
      "    \"max_results\": 5\n",
      "  },\n",
      "  {\n",
      "    \"search_query\": \"abs:\\\"transformer architecture\\\" AND abs:\\\"sequence transduction\\\" AND cat:cs.CL\",\n",
      "    \"max_results\": 5\n",
      "  },\n",
      "  {\n",
      "    \"search_query\": \"\\\"Attention Is All You Need\\\" AND abs:\\\"transformer architecture\\\" AND cat:cs.CL\",\n",
      "    \"max_results\": 5\n",
      "  }\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(queries.content[7:-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976003c6",
   "metadata": {},
   "source": [
    "Now that we have a structured JSON list of search_queries as a string, we first need to convert them into a json object. Once we have a json object, we'll iterate over and pass each search_query as one arxiv api call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a640fbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "[{'search_query': '((ti:\"survey image generation models\" OR abs:\"comparison generative models image synthesis\" OR ti:\"review diffusion models GANs VAEs\") AND (\"performance metrics\" OR FID OR IS OR \"perceptual quality\" OR diversity OR \"computational cost\" OR architectures OR benchmarking))', 'max_results': 5}, {'search_query': '(\"review diffusion models GANs VAEs\" OR (abs:\"diffusion models\" AND (GANs OR VAEs))) AND (abs:architectures OR abs:FID OR abs:IS OR abs:\"perceptual quality\" OR abs:diversity OR abs:\"computational cost\")', 'max_results': 5}, {'search_query': '((ti:\"survey image generation models\" OR abs:\"comparison generative models image synthesis\") AND (ti:benchmarking OR abs:benchmarking)) AND (\"performance metrics\" OR diversity OR \"computational cost\")', 'max_results': 5}]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "queries_json_string = queries.content[7:-3]\n",
    "# print(queries_json_string)\n",
    "print(type(queries_json_string))\n",
    "\n",
    "\n",
    "queries_json_dict = json.loads(queries_json_string)\n",
    "print(queries_json_dict)\n",
    "print(type(queries_json_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1099a7ba",
   "metadata": {},
   "source": [
    "For each query in the queries_json_dict, we will create a URL and pass it to the arxiv search api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "332e5078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://export.arxiv.org/api/query?search_query=%28%28ti%3A%22survey%20image%20generation%20models%22%20OR%20abs%3A%22comparison%20generative%20models%20image%20synthesis%22%20OR%20ti%3A%22review%20diffusion%20models%20GANs%20VAEs%22%29%20AND%20%28%22performance%20metrics%22%20OR%20FID%20OR%20IS%20OR%20%22perceptual%20quality%22%20OR%20diversity%20OR%20%22computational%20cost%22%20OR%20architectures%20OR%20benchmarking%29%29&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      "http://export.arxiv.org/api/query?search_query=%28%22review%20diffusion%20models%20GANs%20VAEs%22%20OR%20%28abs%3A%22diffusion%20models%22%20AND%20%28GANs%20OR%20VAEs%29%29%29%20AND%20%28abs%3Aarchitectures%20OR%20abs%3AFID%20OR%20abs%3AIS%20OR%20abs%3A%22perceptual%20quality%22%20OR%20abs%3Adiversity%20OR%20abs%3A%22computational%20cost%22%29&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      "http://export.arxiv.org/api/query?search_query=%28%28ti%3A%22survey%20image%20generation%20models%22%20OR%20abs%3A%22comparison%20generative%20models%20image%20synthesis%22%29%20AND%20%28ti%3Abenchmarking%20OR%20abs%3Abenchmarking%29%29%20AND%20%28%22performance%20metrics%22%20OR%20diversity%20OR%20%22computational%20cost%22%29&max_results=5&sortBy=submittedDate&sortOrder=descending\n",
      "[[], [{'id': 'http://arxiv.org/abs/2510.20792v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2510.20792v1', 'updated': '2025-10-23T17:54:17Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=10, tm_mday=23, tm_hour=17, tm_min=54, tm_sec=17, tm_wday=3, tm_yday=296, tm_isdst=0), 'published': '2025-10-23T17:54:17Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=10, tm_mday=23, tm_hour=17, tm_min=54, tm_sec=17, tm_wday=3, tm_yday=296, tm_isdst=0), 'title': 'BadGraph: A Backdoor Attack Against Latent Diffusion Model for\\n  Text-Guided Graph Generation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=%28%22review%20diffusion%20models%20GANs%20VAEs%22%20OR%20%28abs%3A%22diffusion%20models%22%20AND%20%28GANs%20OR%20VAEs%29%29%29%20AND%20%28abs%3Aarchitectures%20OR%20abs%3AFID%20OR%20abs%3AIS%20OR%20abs%3A%22perceptual%20quality%22%20OR%20abs%3Adiversity%20OR%20abs%3A%22computational%20cost%22%29&max_results=5&sortBy=submittedDate&sortOrder=descending', 'value': 'BadGraph: A Backdoor Attack Against Latent Diffusion Model for\\n  Text-Guided Graph Generation'}, 'summary': \"The rapid progress of graph generation has raised new security concerns,\\nparticularly regarding backdoor vulnerabilities. While prior work has explored\\nbackdoor attacks in image diffusion and unconditional graph generation,\\nconditional, especially text-guided graph generation remains largely\\nunexamined. This paper proposes BadGraph, a backdoor attack method targeting\\nlatent diffusion models for text-guided graph generation. BadGraph leverages\\ntextual triggers to poison training data, covertly implanting backdoors that\\ninduce attacker-specified subgraphs during inference when triggers appear,\\nwhile preserving normal performance on clean inputs. Extensive experiments on\\nfour benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the\\neffectiveness and stealth of the attack: less than 10% poisoning rate can\\nachieves 50% attack success rate, while 24% suffices for over 80% success rate,\\nwith negligible performance degradation on benign samples. Ablation studies\\nfurther reveal that the backdoor is implanted during VAE and diffusion training\\nrather than pretraining. These findings reveal the security vulnerabilities in\\nlatent diffusion models of text-guided graph generation, highlight the serious\\nrisks in models' applications such as drug discovery and underscore the need\\nfor robust defenses against the backdoor attack in such diffusion models.\", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=%28%22review%20diffusion%20models%20GANs%20VAEs%22%20OR%20%28abs%3A%22diffusion%20models%22%20AND%20%28GANs%20OR%20VAEs%29%29%29%20AND%20%28abs%3Aarchitectures%20OR%20abs%3AFID%20OR%20abs%3AIS%20OR%20abs%3A%22perceptual%20quality%22%20OR%20abs%3Adiversity%20OR%20abs%3A%22computational%20cost%22%29&max_results=5&sortBy=submittedDate&sortOrder=descending', 'value': \"The rapid progress of graph generation has raised new security concerns,\\nparticularly regarding backdoor vulnerabilities. While prior work has explored\\nbackdoor attacks in image diffusion and unconditional graph generation,\\nconditional, especially text-guided graph generation remains largely\\nunexamined. This paper proposes BadGraph, a backdoor attack method targeting\\nlatent diffusion models for text-guided graph generation. BadGraph leverages\\ntextual triggers to poison training data, covertly implanting backdoors that\\ninduce attacker-specified subgraphs during inference when triggers appear,\\nwhile preserving normal performance on clean inputs. Extensive experiments on\\nfour benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the\\neffectiveness and stealth of the attack: less than 10% poisoning rate can\\nachieves 50% attack success rate, while 24% suffices for over 80% success rate,\\nwith negligible performance degradation on benign samples. Ablation studies\\nfurther reveal that the backdoor is implanted during VAE and diffusion training\\nrather than pretraining. These findings reveal the security vulnerabilities in\\nlatent diffusion models of text-guided graph generation, highlight the serious\\nrisks in models' applications such as drug discovery and underscore the need\\nfor robust defenses against the backdoor attack in such diffusion models.\"}, 'authors': [{'name': 'Liang Ye'}, {'name': 'Shengqin Chen'}, {'name': 'Jiazhu Dai'}], 'author_detail': {'name': 'Jiazhu Dai'}, 'author': 'Jiazhu Dai', 'links': [{'href': 'http://arxiv.org/abs/2510.20792v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2510.20792v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'q-bio.BM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}, {'id': 'http://arxiv.org/abs/2510.19623v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2510.19623v1', 'updated': '2025-10-22T14:20:00Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=10, tm_mday=22, tm_hour=14, tm_min=20, tm_sec=0, tm_wday=2, tm_yday=295, tm_isdst=0), 'published': '2025-10-22T14:20:00Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=10, tm_mday=22, tm_hour=14, tm_min=20, tm_sec=0, tm_wday=2, tm_yday=295, tm_isdst=0), 'title': 'Learning and Simulating Building Evacuation Patterns for Enhanced Safety\\n  Design Using Generative Models', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=%28%22review%20diffusion%20models%20GANs%20VAEs%22%20OR%20%28abs%3A%22diffusion%20models%22%20AND%20%28GANs%20OR%20VAEs%29%29%29%20AND%20%28abs%3Aarchitectures%20OR%20abs%3AFID%20OR%20abs%3AIS%20OR%20abs%3A%22perceptual%20quality%22%20OR%20abs%3Adiversity%20OR%20abs%3A%22computational%20cost%22%29&max_results=5&sortBy=submittedDate&sortOrder=descending', 'value': 'Learning and Simulating Building Evacuation Patterns for Enhanced Safety\\n  Design Using Generative Models'}, 'summary': 'Evacuation simulation is essential for building safety design, ensuring\\nproperly planned evacuation routes. However, traditional evacuation simulation\\nrelies heavily on refined modeling with extensive parameters, making it\\nchallenging to adopt such methods in a rapid iteration process in early design\\nstages. Thus, this study proposes DiffEvac, a novel method to learn building\\nevacuation patterns based on Generative Models (GMs), for efficient evacuation\\nsimulation and enhanced safety design. Initially, a dataset of 399 diverse\\nfunctional layouts and corresponding evacuation heatmaps of buildings was\\nestablished. Then, a decoupled feature representation is proposed to embed\\nphysical features like layouts and occupant density for GMs. Finally, a\\ndiffusion model based on image prompts is proposed to learn evacuation patterns\\nfrom simulated evacuation heatmaps. Compared to existing research using\\nConditional GANs with RGB representation, DiffEvac achieves up to a 37.6%\\nimprovement in SSIM, 142% in PSNR, and delivers results 16 times faster,\\nthereby cutting simulation time to 2 minutes. Case studies further demonstrate\\nthat the proposed method not only significantly enhances the rapid design\\niteration and adjustment process with efficient evacuation simulation but also\\noffers new insights and technical pathways for future safety optimization in\\nintelligent building design. The research implication is that the approach\\nlowers the modeling burden, enables large-scale what-if exploration, and\\nfacilitates coupling with multi-objective design tools.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=%28%22review%20diffusion%20models%20GANs%20VAEs%22%20OR%20%28abs%3A%22diffusion%20models%22%20AND%20%28GANs%20OR%20VAEs%29%29%29%20AND%20%28abs%3Aarchitectures%20OR%20abs%3AFID%20OR%20abs%3AIS%20OR%20abs%3A%22perceptual%20quality%22%20OR%20abs%3Adiversity%20OR%20abs%3A%22computational%20cost%22%29&max_results=5&sortBy=submittedDate&sortOrder=descending', 'value': 'Evacuation simulation is essential for building safety design, ensuring\\nproperly planned evacuation routes. However, traditional evacuation simulation\\nrelies heavily on refined modeling with extensive parameters, making it\\nchallenging to adopt such methods in a rapid iteration process in early design\\nstages. Thus, this study proposes DiffEvac, a novel method to learn building\\nevacuation patterns based on Generative Models (GMs), for efficient evacuation\\nsimulation and enhanced safety design. Initially, a dataset of 399 diverse\\nfunctional layouts and corresponding evacuation heatmaps of buildings was\\nestablished. Then, a decoupled feature representation is proposed to embed\\nphysical features like layouts and occupant density for GMs. Finally, a\\ndiffusion model based on image prompts is proposed to learn evacuation patterns\\nfrom simulated evacuation heatmaps. Compared to existing research using\\nConditional GANs with RGB representation, DiffEvac achieves up to a 37.6%\\nimprovement in SSIM, 142% in PSNR, and delivers results 16 times faster,\\nthereby cutting simulation time to 2 minutes. Case studies further demonstrate\\nthat the proposed method not only significantly enhances the rapid design\\niteration and adjustment process with efficient evacuation simulation but also\\noffers new insights and technical pathways for future safety optimization in\\nintelligent building design. The research implication is that the approach\\nlowers the modeling burden, enables large-scale what-if exploration, and\\nfacilitates coupling with multi-objective design tools.'}, 'authors': [{'name': 'Jin Han'}, {'name': 'Zhe Zheng'}, {'name': 'Yi Gu'}, {'name': 'Jia-Rui Lin'}, {'name': 'Xin-Zheng Lu'}], 'author_detail': {'name': 'Xin-Zheng Lu'}, 'author': 'Xin-Zheng Lu', 'links': [{'href': 'http://arxiv.org/abs/2510.19623v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2510.19623v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}, {'id': 'http://arxiv.org/abs/2510.18777v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2510.18777v1', 'updated': '2025-10-21T16:25:19Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=10, tm_mday=21, tm_hour=16, tm_min=25, tm_sec=19, tm_wday=1, tm_yday=294, tm_isdst=0), 'published': '2025-10-21T16:25:19Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=10, tm_mday=21, tm_hour=16, tm_min=25, tm_sec=19, tm_wday=1, tm_yday=294, tm_isdst=0), 'title': 'A Frequentist Statistical Introduction to Variational Inference,\\n  Autoencoders, and Diffusion Models', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=%28%22review%20diffusion%20models%20GANs%20VAEs%22%20OR%20%28abs%3A%22diffusion%20models%22%20AND%20%28GANs%20OR%20VAEs%29%29%29%20AND%20%28abs%3Aarchitectures%20OR%20abs%3AFID%20OR%20abs%3AIS%20OR%20abs%3A%22perceptual%20quality%22%20OR%20abs%3Adiversity%20OR%20abs%3A%22computational%20cost%22%29&max_results=5&sortBy=submittedDate&sortOrder=descending', 'value': 'A Frequentist Statistical Introduction to Variational Inference,\\n  Autoencoders, and Diffusion Models'}, 'summary': 'While Variational Inference (VI) is central to modern generative models like\\nVariational Autoencoders (VAEs) and Denoising Diffusion Models (DDMs), its\\npedagogical treatment is split across disciplines. In statistics, VI is\\ntypically framed as a Bayesian method for posterior approximation. In machine\\nlearning, however, VAEs and DDMs are developed from a Frequentist viewpoint,\\nwhere VI is used to approximate a maximum likelihood estimator. This creates a\\nbarrier for statisticians, as the principles behind VAEs and DDMs are hard to\\ncontextualize without a corresponding Frequentist introduction to VI. This\\npaper provides that introduction: we explain the theory for VI, VAEs, and DDMs\\nfrom a purely Frequentist perspective, starting with the classical\\nExpectation-Maximization (EM) algorithm. We show how VI arises as a scalable\\nsolution for intractable E-steps and how VAEs and DDMs are natural,\\ndeep-learning-based extensions of this framework, thereby bridging the gap\\nbetween classical statistical inference and modern generative AI.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=%28%22review%20diffusion%20models%20GANs%20VAEs%22%20OR%20%28abs%3A%22diffusion%20models%22%20AND%20%28GANs%20OR%20VAEs%29%29%29%20AND%20%28abs%3Aarchitectures%20OR%20abs%3AFID%20OR%20abs%3AIS%20OR%20abs%3A%22perceptual%20quality%22%20OR%20abs%3Adiversity%20OR%20abs%3A%22computational%20cost%22%29&max_results=5&sortBy=submittedDate&sortOrder=descending', 'value': 'While Variational Inference (VI) is central to modern generative models like\\nVariational Autoencoders (VAEs) and Denoising Diffusion Models (DDMs), its\\npedagogical treatment is split across disciplines. In statistics, VI is\\ntypically framed as a Bayesian method for posterior approximation. In machine\\nlearning, however, VAEs and DDMs are developed from a Frequentist viewpoint,\\nwhere VI is used to approximate a maximum likelihood estimator. This creates a\\nbarrier for statisticians, as the principles behind VAEs and DDMs are hard to\\ncontextualize without a corresponding Frequentist introduction to VI. This\\npaper provides that introduction: we explain the theory for VI, VAEs, and DDMs\\nfrom a purely Frequentist perspective, starting with the classical\\nExpectation-Maximization (EM) algorithm. We show how VI arises as a scalable\\nsolution for intractable E-steps and how VAEs and DDMs are natural,\\ndeep-learning-based extensions of this framework, thereby bridging the gap\\nbetween classical statistical inference and modern generative AI.'}, 'authors': [{'name': 'Yen-Chi Chen'}], 'author_detail': {'name': 'Yen-Chi Chen'}, 'author': 'Yen-Chi Chen', 'arxiv_comment': 'This is an introduction paper. 28 pages, 2 figures', 'links': [{'href': 'http://arxiv.org/abs/2510.18777v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2510.18777v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'stat.ML', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.CO', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'stat.ME', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}, {'id': 'http://arxiv.org/abs/2510.18457v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2510.18457v1', 'updated': '2025-10-21T09:30:45Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=10, tm_mday=21, tm_hour=9, tm_min=30, tm_sec=45, tm_wday=1, tm_yday=294, tm_isdst=0), 'published': '2025-10-21T09:30:45Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=10, tm_mday=21, tm_hour=9, tm_min=30, tm_sec=45, tm_wday=1, tm_yday=294, tm_isdst=0), 'title': 'Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion\\n  Models', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=%28%22review%20diffusion%20models%20GANs%20VAEs%22%20OR%20%28abs%3A%22diffusion%20models%22%20AND%20%28GANs%20OR%20VAEs%29%29%29%20AND%20%28abs%3Aarchitectures%20OR%20abs%3AFID%20OR%20abs%3AIS%20OR%20abs%3A%22perceptual%20quality%22%20OR%20abs%3Adiversity%20OR%20abs%3A%22computational%20cost%22%29&max_results=5&sortBy=submittedDate&sortOrder=descending', 'value': 'Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion\\n  Models'}, 'summary': \"The performance of Latent Diffusion Models (LDMs) is critically dependent on\\nthe quality of their visual tokenizer. While recent works have explored\\nincorporating Vision Foundation Models (VFMs) via distillation, we identify a\\nfundamental flaw in this approach: it inevitably weakens the robustness of\\nalignment with the original VFM, causing the aligned latents to deviate\\nsemantically under distribution shifts. In this paper, we bypass distillation\\nby proposing a more direct approach: Vision Foundation Model Variational\\nAutoencoder (VFM-VAE). To resolve the inherent tension between the VFM's\\nsemantic focus and the need for pixel-level fidelity, we redesign the VFM-VAE\\ndecoder with Multi-Scale Latent Fusion and Progressive Resolution\\nReconstruction blocks, enabling high-quality reconstruction from spatially\\ncoarse VFM features. Furthermore, we provide a comprehensive analysis of\\nrepresentation dynamics during diffusion training, introducing the proposed\\nSE-CKNNA metric as a more precise tool for this diagnosis. This analysis allows\\nus to develop a joint tokenizer-diffusion alignment strategy that dramatically\\naccelerates convergence. Our innovations in tokenizer design and training\\nstrategy lead to superior performance and efficiency: our system reaches a gFID\\n(w/o CFG) of 2.20 in merely 80 epochs (a 10x speedup over prior tokenizers).\\nWith continued training to 640 epochs, it further attains a gFID (w/o CFG) of\\n1.62, establishing direct VFM integration as a superior paradigm for LDMs.\", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=%28%22review%20diffusion%20models%20GANs%20VAEs%22%20OR%20%28abs%3A%22diffusion%20models%22%20AND%20%28GANs%20OR%20VAEs%29%29%29%20AND%20%28abs%3Aarchitectures%20OR%20abs%3AFID%20OR%20abs%3AIS%20OR%20abs%3A%22perceptual%20quality%22%20OR%20abs%3Adiversity%20OR%20abs%3A%22computational%20cost%22%29&max_results=5&sortBy=submittedDate&sortOrder=descending', 'value': \"The performance of Latent Diffusion Models (LDMs) is critically dependent on\\nthe quality of their visual tokenizer. While recent works have explored\\nincorporating Vision Foundation Models (VFMs) via distillation, we identify a\\nfundamental flaw in this approach: it inevitably weakens the robustness of\\nalignment with the original VFM, causing the aligned latents to deviate\\nsemantically under distribution shifts. In this paper, we bypass distillation\\nby proposing a more direct approach: Vision Foundation Model Variational\\nAutoencoder (VFM-VAE). To resolve the inherent tension between the VFM's\\nsemantic focus and the need for pixel-level fidelity, we redesign the VFM-VAE\\ndecoder with Multi-Scale Latent Fusion and Progressive Resolution\\nReconstruction blocks, enabling high-quality reconstruction from spatially\\ncoarse VFM features. Furthermore, we provide a comprehensive analysis of\\nrepresentation dynamics during diffusion training, introducing the proposed\\nSE-CKNNA metric as a more precise tool for this diagnosis. This analysis allows\\nus to develop a joint tokenizer-diffusion alignment strategy that dramatically\\naccelerates convergence. Our innovations in tokenizer design and training\\nstrategy lead to superior performance and efficiency: our system reaches a gFID\\n(w/o CFG) of 2.20 in merely 80 epochs (a 10x speedup over prior tokenizers).\\nWith continued training to 640 epochs, it further attains a gFID (w/o CFG) of\\n1.62, establishing direct VFM integration as a superior paradigm for LDMs.\"}, 'authors': [{'name': 'Tianci Bi'}, {'name': 'Xiaoyi Zhang'}, {'name': 'Yan Lu'}, {'name': 'Nanning Zheng'}], 'author_detail': {'name': 'Nanning Zheng'}, 'author': 'Nanning Zheng', 'arxiv_comment': 'Code and models available at: https://github.com/tianciB/VFM-VAE', 'links': [{'href': 'http://arxiv.org/abs/2510.18457v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2510.18457v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}, {'id': 'http://arxiv.org/abs/2510.17383v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2510.17383v1', 'updated': '2025-10-20T10:20:42Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=10, tm_mday=20, tm_hour=10, tm_min=20, tm_sec=42, tm_wday=0, tm_yday=293, tm_isdst=0), 'published': '2025-10-20T10:20:42Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=10, tm_mday=20, tm_hour=10, tm_min=20, tm_sec=42, tm_wday=0, tm_yday=293, tm_isdst=0), 'title': 'Latent Spaces Beyond Synthesis: From GANs to Diffusion Models', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=%28%22review%20diffusion%20models%20GANs%20VAEs%22%20OR%20%28abs%3A%22diffusion%20models%22%20AND%20%28GANs%20OR%20VAEs%29%29%29%20AND%20%28abs%3Aarchitectures%20OR%20abs%3AFID%20OR%20abs%3AIS%20OR%20abs%3A%22perceptual%20quality%22%20OR%20abs%3Adiversity%20OR%20abs%3A%22computational%20cost%22%29&max_results=5&sortBy=submittedDate&sortOrder=descending', 'value': 'Latent Spaces Beyond Synthesis: From GANs to Diffusion Models'}, 'summary': 'This paper examines the evolving nature of internal representations in\\ngenerative visual models, focusing on the conceptual and technical shift from\\nGANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi\\'s\\naccount of synthesis as the amalgamation of distributed representations, we\\npropose a distinction between \"synthesis in a strict sense\", where a compact\\nlatent space wholly determines the generative process, and \"synthesis in a\\nbroad sense,\" which characterizes models whose representational labor is\\ndistributed across layers. Through close readings of model architectures and a\\ntargeted experimental setup that intervenes in layerwise representations, we\\nshow how diffusion models fragment the burden of representation and thereby\\nchallenge assumptions of unified internal space. By situating these findings\\nwithin media theoretical frameworks and critically engaging with metaphors such\\nas the latent space and the Platonic Representation Hypothesis, we argue for a\\nreorientation of how generative AI is understood: not as a direct synthesis of\\ncontent, but as an emergent configuration of specialized processes.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=%28%22review%20diffusion%20models%20GANs%20VAEs%22%20OR%20%28abs%3A%22diffusion%20models%22%20AND%20%28GANs%20OR%20VAEs%29%29%29%20AND%20%28abs%3Aarchitectures%20OR%20abs%3AFID%20OR%20abs%3AIS%20OR%20abs%3A%22perceptual%20quality%22%20OR%20abs%3Adiversity%20OR%20abs%3A%22computational%20cost%22%29&max_results=5&sortBy=submittedDate&sortOrder=descending', 'value': 'This paper examines the evolving nature of internal representations in\\ngenerative visual models, focusing on the conceptual and technical shift from\\nGANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi\\'s\\naccount of synthesis as the amalgamation of distributed representations, we\\npropose a distinction between \"synthesis in a strict sense\", where a compact\\nlatent space wholly determines the generative process, and \"synthesis in a\\nbroad sense,\" which characterizes models whose representational labor is\\ndistributed across layers. Through close readings of model architectures and a\\ntargeted experimental setup that intervenes in layerwise representations, we\\nshow how diffusion models fragment the burden of representation and thereby\\nchallenge assumptions of unified internal space. By situating these findings\\nwithin media theoretical frameworks and critically engaging with metaphors such\\nas the latent space and the Platonic Representation Hypothesis, we argue for a\\nreorientation of how generative AI is understood: not as a direct synthesis of\\ncontent, but as an emergent configuration of specialized processes.'}, 'authors': [{'name': 'Ludovica Schaerf'}], 'author_detail': {'name': 'Ludovica Schaerf'}, 'author': 'Ludovica Schaerf', 'arxiv_comment': \"Presented and published at Ethics and Aesthetics of Artificial\\n  Intelligence Conference (EA-AI'25)\", 'links': [{'href': 'http://arxiv.org/abs/2510.17383v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2510.17383v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CY', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}], []]\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "from urllib.parse import quote\n",
    "\n",
    "base_url = 'http://export.arxiv.org/api/query?'\n",
    "results = []\n",
    "max_results = 5\n",
    "\n",
    "for query in queries_json_dict:\n",
    "    search_query = quote(query[\"search_query\"])  # URL-encode\n",
    "    url = base_url + f\"search_query={search_query}&max_results={max_results}&sortBy=submittedDate&sortOrder=descending\"\n",
    "    print(url)\n",
    "    feed = feedparser.parse(url)\n",
    "    results.append(feed.entries)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da333649",
   "metadata": {},
   "source": [
    "Let's explore the results of the arxiv search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0916fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: BadGraph: A Backdoor Attack Against Latent Diffusion Model for\n",
      "  Text-Guided Graph Generation\n",
      "Authors: ['Liang Ye', 'Shengqin Chen', 'Jiazhu Dai']\n",
      "Published: 2025-10-23T17:54:17Z\n",
      "Summary: The rapid progress of graph generation has raised new security concerns,\n",
      "particularly regarding backdoor vulnerabilities. While prior work has explored\n",
      "backdoor attacks in image diffusion and unconditional graph generation,\n",
      "conditional, especially text-guided graph generation remains largely\n",
      "unexamined. This paper proposes BadGraph, a backdoor attack method targeting\n",
      "latent diffusion models for text-guided graph generation. BadGraph leverages\n",
      "textual triggers to poison training data, covertly implanting backdoors that\n",
      "induce attacker-specified subgraphs during inference when triggers appear,\n",
      "while preserving normal performance on clean inputs. Extensive experiments on\n",
      "four benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the\n",
      "effectiveness and stealth of the attack: less than 10% poisoning rate can\n",
      "achieves 50% attack success rate, while 24% suffices for over 80% success rate,\n",
      "with negligible performance degradation on benign samples. Ablation studies\n",
      "further reveal that the backdoor is implanted during VAE and diffusion training\n",
      "rather than pretraining. These findings reveal the security vulnerabilities in\n",
      "latent diffusion models of text-guided graph generation, highlight the serious\n",
      "risks in models' applications such as drug discovery and underscore the need\n",
      "for robust defenses against the backdoor attack in such diffusion models.\n",
      "Link: http://arxiv.org/abs/2510.20792v1\n",
      "\n",
      "\n",
      "Title: Learning and Simulating Building Evacuation Patterns for Enhanced Safety\n",
      "  Design Using Generative Models\n",
      "Authors: ['Jin Han', 'Zhe Zheng', 'Yi Gu', 'Jia-Rui Lin', 'Xin-Zheng Lu']\n",
      "Published: 2025-10-22T14:20:00Z\n",
      "Summary: Evacuation simulation is essential for building safety design, ensuring\n",
      "properly planned evacuation routes. However, traditional evacuation simulation\n",
      "relies heavily on refined modeling with extensive parameters, making it\n",
      "challenging to adopt such methods in a rapid iteration process in early design\n",
      "stages. Thus, this study proposes DiffEvac, a novel method to learn building\n",
      "evacuation patterns based on Generative Models (GMs), for efficient evacuation\n",
      "simulation and enhanced safety design. Initially, a dataset of 399 diverse\n",
      "functional layouts and corresponding evacuation heatmaps of buildings was\n",
      "established. Then, a decoupled feature representation is proposed to embed\n",
      "physical features like layouts and occupant density for GMs. Finally, a\n",
      "diffusion model based on image prompts is proposed to learn evacuation patterns\n",
      "from simulated evacuation heatmaps. Compared to existing research using\n",
      "Conditional GANs with RGB representation, DiffEvac achieves up to a 37.6%\n",
      "improvement in SSIM, 142% in PSNR, and delivers results 16 times faster,\n",
      "thereby cutting simulation time to 2 minutes. Case studies further demonstrate\n",
      "that the proposed method not only significantly enhances the rapid design\n",
      "iteration and adjustment process with efficient evacuation simulation but also\n",
      "offers new insights and technical pathways for future safety optimization in\n",
      "intelligent building design. The research implication is that the approach\n",
      "lowers the modeling burden, enables large-scale what-if exploration, and\n",
      "facilitates coupling with multi-objective design tools.\n",
      "Link: http://arxiv.org/abs/2510.19623v1\n",
      "\n",
      "\n",
      "Title: A Frequentist Statistical Introduction to Variational Inference,\n",
      "  Autoencoders, and Diffusion Models\n",
      "Authors: ['Yen-Chi Chen']\n",
      "Published: 2025-10-21T16:25:19Z\n",
      "Summary: While Variational Inference (VI) is central to modern generative models like\n",
      "Variational Autoencoders (VAEs) and Denoising Diffusion Models (DDMs), its\n",
      "pedagogical treatment is split across disciplines. In statistics, VI is\n",
      "typically framed as a Bayesian method for posterior approximation. In machine\n",
      "learning, however, VAEs and DDMs are developed from a Frequentist viewpoint,\n",
      "where VI is used to approximate a maximum likelihood estimator. This creates a\n",
      "barrier for statisticians, as the principles behind VAEs and DDMs are hard to\n",
      "contextualize without a corresponding Frequentist introduction to VI. This\n",
      "paper provides that introduction: we explain the theory for VI, VAEs, and DDMs\n",
      "from a purely Frequentist perspective, starting with the classical\n",
      "Expectation-Maximization (EM) algorithm. We show how VI arises as a scalable\n",
      "solution for intractable E-steps and how VAEs and DDMs are natural,\n",
      "deep-learning-based extensions of this framework, thereby bridging the gap\n",
      "between classical statistical inference and modern generative AI.\n",
      "Link: http://arxiv.org/abs/2510.18777v1\n",
      "\n",
      "\n",
      "Title: Vision Foundation Models Can Be Good Tokenizers for Latent Diffusion\n",
      "  Models\n",
      "Authors: ['Tianci Bi', 'Xiaoyi Zhang', 'Yan Lu', 'Nanning Zheng']\n",
      "Published: 2025-10-21T09:30:45Z\n",
      "Summary: The performance of Latent Diffusion Models (LDMs) is critically dependent on\n",
      "the quality of their visual tokenizer. While recent works have explored\n",
      "incorporating Vision Foundation Models (VFMs) via distillation, we identify a\n",
      "fundamental flaw in this approach: it inevitably weakens the robustness of\n",
      "alignment with the original VFM, causing the aligned latents to deviate\n",
      "semantically under distribution shifts. In this paper, we bypass distillation\n",
      "by proposing a more direct approach: Vision Foundation Model Variational\n",
      "Autoencoder (VFM-VAE). To resolve the inherent tension between the VFM's\n",
      "semantic focus and the need for pixel-level fidelity, we redesign the VFM-VAE\n",
      "decoder with Multi-Scale Latent Fusion and Progressive Resolution\n",
      "Reconstruction blocks, enabling high-quality reconstruction from spatially\n",
      "coarse VFM features. Furthermore, we provide a comprehensive analysis of\n",
      "representation dynamics during diffusion training, introducing the proposed\n",
      "SE-CKNNA metric as a more precise tool for this diagnosis. This analysis allows\n",
      "us to develop a joint tokenizer-diffusion alignment strategy that dramatically\n",
      "accelerates convergence. Our innovations in tokenizer design and training\n",
      "strategy lead to superior performance and efficiency: our system reaches a gFID\n",
      "(w/o CFG) of 2.20 in merely 80 epochs (a 10x speedup over prior tokenizers).\n",
      "With continued training to 640 epochs, it further attains a gFID (w/o CFG) of\n",
      "1.62, establishing direct VFM integration as a superior paradigm for LDMs.\n",
      "Link: http://arxiv.org/abs/2510.18457v1\n",
      "\n",
      "\n",
      "Title: Latent Spaces Beyond Synthesis: From GANs to Diffusion Models\n",
      "Authors: ['Ludovica Schaerf']\n",
      "Published: 2025-10-20T10:20:42Z\n",
      "Summary: This paper examines the evolving nature of internal representations in\n",
      "generative visual models, focusing on the conceptual and technical shift from\n",
      "GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's\n",
      "account of synthesis as the amalgamation of distributed representations, we\n",
      "propose a distinction between \"synthesis in a strict sense\", where a compact\n",
      "latent space wholly determines the generative process, and \"synthesis in a\n",
      "broad sense,\" which characterizes models whose representational labor is\n",
      "distributed across layers. Through close readings of model architectures and a\n",
      "targeted experimental setup that intervenes in layerwise representations, we\n",
      "show how diffusion models fragment the burden of representation and thereby\n",
      "challenge assumptions of unified internal space. By situating these findings\n",
      "within media theoretical frameworks and critically engaging with metaphors such\n",
      "as the latent space and the Platonic Representation Hypothesis, we argue for a\n",
      "reorientation of how generative AI is understood: not as a direct synthesis of\n",
      "content, but as an emergent configuration of specialized processes.\n",
      "Link: http://arxiv.org/abs/2510.17383v1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(len(results))\n",
    "# print(results[0])\n",
    "# print(results[1])\n",
    "# print(results[2])\n",
    "\n",
    "for i in range(len(results)):\n",
    "    for result in results[i]:\n",
    "        # print(result)\n",
    "        print(\"Title:\", result.title)\n",
    "        print(\"Authors:\", [author['name'] for author in result['authors']])\n",
    "        print(\"Published:\", result['published'])\n",
    "        print(\"Summary:\", result['summary'])\n",
    "        print(\"Link:\", result['link'])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbcfecd",
   "metadata": {},
   "source": [
    "Perfect! Now we have a list of papers with their summaries and pdf link. We can store the title+summary+pdf_link in a vector db and use it to retrieve top-k relevant documents for the reflection step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0420d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'http://arxiv.org/abs/1601.06914v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/1601.06914v1', 'updated': '2016-01-26T07:50:51Z', 'updated_parsed': time.struct_time(tm_year=2016, tm_mon=1, tm_mday=26, tm_hour=7, tm_min=50, tm_sec=51, tm_wday=1, tm_yday=26, tm_isdst=0), 'published': '2016-01-26T07:50:51Z', 'published_parsed': time.struct_time(tm_year=2016, tm_mon=1, tm_mday=26, tm_hour=7, tm_min=50, tm_sec=51, tm_wday=1, tm_yday=26, tm_isdst=0), 'title': 'LLM Magnons', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'LLM Magnons'}, 'summary': 'We consider excitations of LLM geometries described by coloring the LLM plane\\nwith concentric black rings. Certain closed string excitations are localized at\\nthe edges of these rings. The string theory predictions for the energies of\\nmagnon excitations of these strings depends on the radii of the edges of the\\nrings. In this article we construct the operators dual to these closed string\\nexcitations and show how to reproduce the string theory predictions for magnon\\nenergies by computing one loop anomalous dimensions. These operators are linear\\ncombinations of restricted Schur polynomials. The distinction between what is\\nthe background and what is the excitation is accomplished in the choice of the\\nsubgroup and the representations used to construct the operator.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'We consider excitations of LLM geometries described by coloring the LLM plane\\nwith concentric black rings. Certain closed string excitations are localized at\\nthe edges of these rings. The string theory predictions for the energies of\\nmagnon excitations of these strings depends on the radii of the edges of the\\nrings. In this article we construct the operators dual to these closed string\\nexcitations and show how to reproduce the string theory predictions for magnon\\nenergies by computing one loop anomalous dimensions. These operators are linear\\ncombinations of restricted Schur polynomials. The distinction between what is\\nthe background and what is the excitation is accomplished in the choice of the\\nsubgroup and the representations used to construct the operator.'}, 'authors': [{'name': 'Robert de Mello Koch'}, {'name': 'Christopher Mathwin'}, {'name': 'Hendrik J. R. van Zyl'}], 'author_detail': {'name': 'Hendrik J. R. van Zyl'}, 'author': 'Hendrik J. R. van Zyl', 'arxiv_doi': '10.1007/JHEP03(2016)110', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.1007/JHEP03(2016)110', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/1601.06914v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/1601.06914v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': '42 pages, 4 figures', 'arxiv_primary_category': {'term': 'hep-th', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'hep-th', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}, {'id': 'http://arxiv.org/abs/2510.19422v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2510.19422v1', 'updated': '2025-10-22T09:44:36Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=10, tm_mday=22, tm_hour=9, tm_min=44, tm_sec=36, tm_wday=2, tm_yday=295, tm_isdst=0), 'published': '2025-10-22T09:44:36Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=10, tm_mday=22, tm_hour=9, tm_min=44, tm_sec=36, tm_wday=2, tm_yday=295, tm_isdst=0), 'title': 'LLM Unlearning with LLM Beliefs', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'LLM Unlearning with LLM Beliefs'}, 'summary': \"Large language models trained on vast corpora inherently risk memorizing\\nsensitive or harmful content, which may later resurface in their outputs.\\nPrevailing unlearning methods generally rely on gradient ascent and its\\nvariants to lower the probability of specific target responses. However, we\\nfind that this strategy induces a critical side effect: probability mass is\\nredistributed into high-likelihood regions, often corresponding to semantically\\nrelated rephrasings of the targets. We refer to this as the squeezing effect,\\nwhich explains why many methods yield merely spurious unlearning, a problem\\nfurther obscured by automated metrics (e.g., ROUGE, truth ratio) that misreport\\nactual success. To address this, we propose a bootstrapping (BS) framework that\\nexplicitly links the squeezing effect with the model's own high-confidence\\ngenerations, namely its model beliefs. Since model beliefs inherently capture\\nthe very high-likelihood regions where probability mass is squeezed,\\nincorporating them into the unlearning objective directly counters the\\nsqueezing effect. By jointly suppressing both target responses and model\\nbeliefs, BS-T (token) attenuates high-probability tokens, whereas BS-S\\n(sequence) removes entire high-confidence generations, together achieving more\\nthorough forgetting while preserving utility. Extensive experiments across\\ndiverse benchmarks with various model families confirm the effectiveness of our\\napproach.\", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': \"Large language models trained on vast corpora inherently risk memorizing\\nsensitive or harmful content, which may later resurface in their outputs.\\nPrevailing unlearning methods generally rely on gradient ascent and its\\nvariants to lower the probability of specific target responses. However, we\\nfind that this strategy induces a critical side effect: probability mass is\\nredistributed into high-likelihood regions, often corresponding to semantically\\nrelated rephrasings of the targets. We refer to this as the squeezing effect,\\nwhich explains why many methods yield merely spurious unlearning, a problem\\nfurther obscured by automated metrics (e.g., ROUGE, truth ratio) that misreport\\nactual success. To address this, we propose a bootstrapping (BS) framework that\\nexplicitly links the squeezing effect with the model's own high-confidence\\ngenerations, namely its model beliefs. Since model beliefs inherently capture\\nthe very high-likelihood regions where probability mass is squeezed,\\nincorporating them into the unlearning objective directly counters the\\nsqueezing effect. By jointly suppressing both target responses and model\\nbeliefs, BS-T (token) attenuates high-probability tokens, whereas BS-S\\n(sequence) removes entire high-confidence generations, together achieving more\\nthorough forgetting while preserving utility. Extensive experiments across\\ndiverse benchmarks with various model families confirm the effectiveness of our\\napproach.\"}, 'authors': [{'name': 'Kemou Li'}, {'name': 'Qizhou Wang'}, {'name': 'Yue Wang'}, {'name': 'Fengpeng Li'}, {'name': 'Jun Liu'}, {'name': 'Bo Han'}, {'name': 'Jiantao Zhou'}], 'author_detail': {'name': 'Jiantao Zhou'}, 'author': 'Jiantao Zhou', 'links': [{'href': 'http://arxiv.org/abs/2510.19422v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2510.19422v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}, {'id': 'http://arxiv.org/abs/2312.10321v4', 'guidislink': True, 'link': 'http://arxiv.org/abs/2312.10321v4', 'updated': '2025-03-12T03:16:27Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=3, tm_mday=12, tm_hour=3, tm_min=16, tm_sec=27, tm_wday=2, tm_yday=71, tm_isdst=0), 'published': '2023-12-16T05:01:23Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=12, tm_mday=16, tm_hour=5, tm_min=1, tm_sec=23, tm_wday=5, tm_yday=350, tm_isdst=0), 'title': 'LLM-SQL-Solver: Can LLMs Determine SQL Equivalence?', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'LLM-SQL-Solver: Can LLMs Determine SQL Equivalence?'}, 'summary': 'Judging the equivalence between two SQL queries is a fundamental problem with\\nmany practical applications in data management and SQL generation (i.e.,\\nevaluating the quality of generated SQL queries in text-to-SQL task). While the\\nresearch community has reasoned about SQL equivalence for decades, it poses\\nconsiderable difficulties and no complete solutions exist. Recently, Large\\nLanguage Models (LLMs) have shown strong reasoning capability in conversation,\\nquestion answering and solving mathematics challenges. In this paper, we study\\nif LLMs can be used to determine the equivalence between SQL queries under two\\nnotions of SQL equivalence (semantic equivalence and relaxed equivalence). To\\nassist LLMs in generating high quality responses, we present two prompting\\ntechniques: Miniature & Mull and Explain & Compare. The former technique is\\nused to evaluate the semantic equivalence in which it asks LLMs to execute a\\nquery on a simple database instance and then explore if a counterexample exists\\nby modifying the database. The latter technique is used to evaluate the relaxed\\nequivalence in which it asks LLMs to explain the queries and then compare if\\nthey contain significant logical differences. Our experiments demonstrate using\\nour techniques, LLMs is a promising tool to help data engineers in writing\\nsemantically equivalent SQL queries, however challenges still persist, and is a\\nbetter metric for evaluating SQL generation than the popular execution\\naccuracy.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'Judging the equivalence between two SQL queries is a fundamental problem with\\nmany practical applications in data management and SQL generation (i.e.,\\nevaluating the quality of generated SQL queries in text-to-SQL task). While the\\nresearch community has reasoned about SQL equivalence for decades, it poses\\nconsiderable difficulties and no complete solutions exist. Recently, Large\\nLanguage Models (LLMs) have shown strong reasoning capability in conversation,\\nquestion answering and solving mathematics challenges. In this paper, we study\\nif LLMs can be used to determine the equivalence between SQL queries under two\\nnotions of SQL equivalence (semantic equivalence and relaxed equivalence). To\\nassist LLMs in generating high quality responses, we present two prompting\\ntechniques: Miniature & Mull and Explain & Compare. The former technique is\\nused to evaluate the semantic equivalence in which it asks LLMs to execute a\\nquery on a simple database instance and then explore if a counterexample exists\\nby modifying the database. The latter technique is used to evaluate the relaxed\\nequivalence in which it asks LLMs to explain the queries and then compare if\\nthey contain significant logical differences. Our experiments demonstrate using\\nour techniques, LLMs is a promising tool to help data engineers in writing\\nsemantically equivalent SQL queries, however challenges still persist, and is a\\nbetter metric for evaluating SQL generation than the popular execution\\naccuracy.'}, 'authors': [{'name': 'Fuheng Zhao'}, {'name': 'Jiayue Chen'}, {'name': 'Lawrence Lim'}, {'name': 'Ishtiyaque Ahmad'}, {'name': 'Divyakant Agrawal'}, {'name': 'Amr El Abbadi'}], 'author_detail': {'name': 'Amr El Abbadi'}, 'author': 'Amr El Abbadi', 'links': [{'href': 'http://arxiv.org/abs/2312.10321v4', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2312.10321v4', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.DB', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.DB', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}, {'id': 'http://arxiv.org/abs/2401.02412v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2401.02412v1', 'updated': '2024-01-04T18:53:01Z', 'updated_parsed': time.struct_time(tm_year=2024, tm_mon=1, tm_mday=4, tm_hour=18, tm_min=53, tm_sec=1, tm_wday=3, tm_yday=4, tm_isdst=0), 'published': '2024-01-04T18:53:01Z', 'published_parsed': time.struct_time(tm_year=2024, tm_mon=1, tm_mday=4, tm_hour=18, tm_min=53, tm_sec=1, tm_wday=3, tm_yday=4, tm_isdst=0), 'title': 'LLM Augmented LLMs: Expanding Capabilities through Composition', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'LLM Augmented LLMs: Expanding Capabilities through Composition'}, 'summary': \"Foundational models with billions of parameters which have been trained on\\nlarge corpora of data have demonstrated non-trivial skills in a variety of\\ndomains. However, due to their monolithic structure, it is challenging and\\nexpensive to augment them or impart new skills. On the other hand, due to their\\nadaptation abilities, several new instances of these models are being trained\\ntowards new domains and tasks. In this work, we study the problem of efficient\\nand practical composition of existing foundation models with more specific\\nmodels to enable newer capabilities. To this end, we propose CALM --\\nComposition to Augment Language Models -- which introduces cross-attention\\nbetween models to compose their representations and enable new capabilities.\\nSalient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'\\nexisting LLMs along with a few additional parameters and data, (ii) Existing\\nmodel weights are kept intact, and hence preserves existing capabilities, and\\n(iii) Applies to diverse domains and settings. We illustrate that augmenting\\nPaLM2-S with a smaller model trained on low-resource languages results in an\\nabsolute improvement of up to 13\\\\% on tasks like translation into English and\\narithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is\\naugmented with a code-specific model, we see a relative improvement of 40\\\\%\\nover the base model for code generation and explanation tasks -- on-par with\\nfully fine-tuned counterparts.\", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': \"Foundational models with billions of parameters which have been trained on\\nlarge corpora of data have demonstrated non-trivial skills in a variety of\\ndomains. However, due to their monolithic structure, it is challenging and\\nexpensive to augment them or impart new skills. On the other hand, due to their\\nadaptation abilities, several new instances of these models are being trained\\ntowards new domains and tasks. In this work, we study the problem of efficient\\nand practical composition of existing foundation models with more specific\\nmodels to enable newer capabilities. To this end, we propose CALM --\\nComposition to Augment Language Models -- which introduces cross-attention\\nbetween models to compose their representations and enable new capabilities.\\nSalient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'\\nexisting LLMs along with a few additional parameters and data, (ii) Existing\\nmodel weights are kept intact, and hence preserves existing capabilities, and\\n(iii) Applies to diverse domains and settings. We illustrate that augmenting\\nPaLM2-S with a smaller model trained on low-resource languages results in an\\nabsolute improvement of up to 13\\\\% on tasks like translation into English and\\narithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is\\naugmented with a code-specific model, we see a relative improvement of 40\\\\%\\nover the base model for code generation and explanation tasks -- on-par with\\nfully fine-tuned counterparts.\"}, 'authors': [{'name': 'Rachit Bansal'}, {'name': 'Bidisha Samanta'}, {'name': 'Siddharth Dalmia'}, {'name': 'Nitish Gupta'}, {'name': 'Shikhar Vashishth'}, {'name': 'Sriram Ganapathy'}, {'name': 'Abhishek Bapna'}, {'name': 'Prateek Jain'}, {'name': 'Partha Talukdar'}], 'author_detail': {'name': 'Partha Talukdar'}, 'author': 'Partha Talukdar', 'arxiv_comment': '17 pages, 2 figures, 8 tables', 'links': [{'href': 'http://arxiv.org/abs/2401.02412v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2401.02412v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}, {'id': 'http://arxiv.org/abs/2407.00215v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2407.00215v1', 'updated': '2024-06-28T19:53:17Z', 'updated_parsed': time.struct_time(tm_year=2024, tm_mon=6, tm_mday=28, tm_hour=19, tm_min=53, tm_sec=17, tm_wday=4, tm_yday=180, tm_isdst=0), 'published': '2024-06-28T19:53:17Z', 'published_parsed': time.struct_time(tm_year=2024, tm_mon=6, tm_mday=28, tm_hour=19, tm_min=53, tm_sec=17, tm_wday=4, tm_yday=180, tm_isdst=0), 'title': 'LLM Critics Help Catch LLM Bugs', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'LLM Critics Help Catch LLM Bugs'}, 'summary': 'Reinforcement learning from human feedback (RLHF) is fundamentally limited by\\nthe capacity of humans to correctly evaluate model output. To improve human\\nevaluation ability and overcome that limitation this work trains \"critic\"\\nmodels that help humans to more accurately evaluate model-written code. These\\ncritics are themselves LLMs trained with RLHF to write natural language\\nfeedback highlighting problems in code from real-world assistant tasks. On code\\ncontaining naturally occurring LLM errors model-written critiques are preferred\\nover human critiques in 63% of cases, and human evaluation finds that models\\ncatch more bugs than human contractors paid for code review. We further confirm\\nthat our fine-tuned LLM critics can successfully identify hundreds of errors in\\nChatGPT training data rated as \"flawless\", even though the majority of those\\ntasks are non-code tasks and thus out-of-distribution for the critic model.\\nCritics can have limitations of their own, including hallucinated bugs that\\ncould mislead humans into making mistakes they might have otherwise avoided,\\nbut human-machine teams of critics and contractors catch similar numbers of\\nbugs to LLM critics while hallucinating less than LLMs alone.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'Reinforcement learning from human feedback (RLHF) is fundamentally limited by\\nthe capacity of humans to correctly evaluate model output. To improve human\\nevaluation ability and overcome that limitation this work trains \"critic\"\\nmodels that help humans to more accurately evaluate model-written code. These\\ncritics are themselves LLMs trained with RLHF to write natural language\\nfeedback highlighting problems in code from real-world assistant tasks. On code\\ncontaining naturally occurring LLM errors model-written critiques are preferred\\nover human critiques in 63% of cases, and human evaluation finds that models\\ncatch more bugs than human contractors paid for code review. We further confirm\\nthat our fine-tuned LLM critics can successfully identify hundreds of errors in\\nChatGPT training data rated as \"flawless\", even though the majority of those\\ntasks are non-code tasks and thus out-of-distribution for the critic model.\\nCritics can have limitations of their own, including hallucinated bugs that\\ncould mislead humans into making mistakes they might have otherwise avoided,\\nbut human-machine teams of critics and contractors catch similar numbers of\\nbugs to LLM critics while hallucinating less than LLMs alone.'}, 'authors': [{'name': 'Nat McAleese'}, {'name': 'Rai Michael Pokorny'}, {'name': 'Juan Felipe Ceron Uribe'}, {'name': 'Evgenia Nitishinskaya'}, {'name': 'Maja Trebacz'}, {'name': 'Jan Leike'}], 'author_detail': {'name': 'Jan Leike'}, 'author': 'Jan Leike', 'links': [{'href': 'http://arxiv.org/abs/2407.00215v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2407.00215v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.SE', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.SE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}, {'id': 'http://arxiv.org/abs/2409.11901v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2409.11901v1', 'updated': '2024-09-18T11:54:45Z', 'updated_parsed': time.struct_time(tm_year=2024, tm_mon=9, tm_mday=18, tm_hour=11, tm_min=54, tm_sec=45, tm_wday=2, tm_yday=262, tm_isdst=0), 'published': '2024-09-18T11:54:45Z', 'published_parsed': time.struct_time(tm_year=2024, tm_mon=9, tm_mday=18, tm_hour=11, tm_min=54, tm_sec=45, tm_wday=2, tm_yday=262, tm_isdst=0), 'title': 'LLMs + Persona-Plug = Personalized LLMs', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'LLMs + Persona-Plug = Personalized LLMs'}, 'summary': \"Personalization plays a critical role in numerous language tasks and\\napplications, since users with the same requirements may prefer diverse outputs\\nbased on their individual interests. This has led to the development of various\\npersonalized approaches aimed at adapting large language models (LLMs) to\\ngenerate customized outputs aligned with user preferences. Some of them involve\\nfine-tuning a unique personalized LLM for each user, which is too expensive for\\nwidespread application. Alternative approaches introduce personalization\\ninformation in a plug-and-play manner by retrieving the user's relevant\\nhistorical texts as demonstrations. However, this retrieval-based strategy may\\nbreak the continuity of the user history and fail to capture the user's overall\\nstyles and patterns, hence leading to sub-optimal performance. To address these\\nchallenges, we propose a novel personalized LLM model, \\\\ours{}. It constructs a\\nuser-specific embedding for each individual by modeling all her historical\\ncontexts through a lightweight plug-in user embedder module. By attaching this\\nembedding to the task input, LLMs can better understand and capture user habits\\nand preferences, thereby producing more personalized outputs without tuning\\ntheir own parameters. Extensive experiments on various tasks in the language\\nmodel personalization (LaMP) benchmark demonstrate that the proposed model\\nsignificantly outperforms existing personalized LLM approaches.\", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': \"Personalization plays a critical role in numerous language tasks and\\napplications, since users with the same requirements may prefer diverse outputs\\nbased on their individual interests. This has led to the development of various\\npersonalized approaches aimed at adapting large language models (LLMs) to\\ngenerate customized outputs aligned with user preferences. Some of them involve\\nfine-tuning a unique personalized LLM for each user, which is too expensive for\\nwidespread application. Alternative approaches introduce personalization\\ninformation in a plug-and-play manner by retrieving the user's relevant\\nhistorical texts as demonstrations. However, this retrieval-based strategy may\\nbreak the continuity of the user history and fail to capture the user's overall\\nstyles and patterns, hence leading to sub-optimal performance. To address these\\nchallenges, we propose a novel personalized LLM model, \\\\ours{}. It constructs a\\nuser-specific embedding for each individual by modeling all her historical\\ncontexts through a lightweight plug-in user embedder module. By attaching this\\nembedding to the task input, LLMs can better understand and capture user habits\\nand preferences, thereby producing more personalized outputs without tuning\\ntheir own parameters. Extensive experiments on various tasks in the language\\nmodel personalization (LaMP) benchmark demonstrate that the proposed model\\nsignificantly outperforms existing personalized LLM approaches.\"}, 'authors': [{'name': 'Jiongnan Liu'}, {'name': 'Yutao Zhu'}, {'name': 'Shuting Wang'}, {'name': 'Xiaochi Wei'}, {'name': 'Erxue Min'}, {'name': 'Yu Lu'}, {'name': 'Shuaiqiang Wang'}, {'name': 'Dawei Yin'}, {'name': 'Zhicheng Dou'}], 'author_detail': {'name': 'Zhicheng Dou'}, 'author': 'Zhicheng Dou', 'links': [{'href': 'http://arxiv.org/abs/2409.11901v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2409.11901v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}, {'id': 'http://arxiv.org/abs/2509.18661v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2509.18661v1', 'updated': '2025-09-23T05:28:43Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=9, tm_mday=23, tm_hour=5, tm_min=28, tm_sec=43, tm_wday=1, tm_yday=266, tm_isdst=0), 'published': '2025-09-23T05:28:43Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=9, tm_mday=23, tm_hour=5, tm_min=28, tm_sec=43, tm_wday=1, tm_yday=266, tm_isdst=0), 'title': 'Agentic AutoSurvey: Let LLMs Survey LLMs', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'Agentic AutoSurvey: Let LLMs Survey LLMs'}, 'summary': \"The exponential growth of scientific literature poses unprecedented\\nchallenges for researchers attempting to synthesize knowledge across rapidly\\nevolving fields. We present \\\\textbf{Agentic AutoSurvey}, a multi-agent\\nframework for automated survey generation that addresses fundamental\\nlimitations in existing approaches. Our system employs four specialized agents\\n(Paper Search Specialist, Topic Mining \\\\& Clustering, Academic Survey Writer,\\nand Quality Evaluator) working in concert to generate comprehensive literature\\nsurveys with superior synthesis quality. Through experiments on six\\nrepresentative LLM research topics from COLM 2024 categories, we demonstrate\\nthat our multi-agent approach achieves significant improvements over existing\\nbaselines, scoring 8.18/10 compared to AutoSurvey's 4.77/10. The multi-agent\\narchitecture processes 75--443 papers per topic (847 total across six topics)\\nwhile targeting high citation coverage (often $\\\\geq$80\\\\% on 75--100-paper sets;\\nlower on very large sets such as RLHF) through specialized agent orchestration.\\nOur 12-dimension evaluation captures organization, synthesis integration, and\\ncritical analysis beyond basic metrics. These findings demonstrate that\\nmulti-agent architectures represent a meaningful advancement for automated\\nliterature survey generation in rapidly evolving scientific domains.\", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': \"The exponential growth of scientific literature poses unprecedented\\nchallenges for researchers attempting to synthesize knowledge across rapidly\\nevolving fields. We present \\\\textbf{Agentic AutoSurvey}, a multi-agent\\nframework for automated survey generation that addresses fundamental\\nlimitations in existing approaches. Our system employs four specialized agents\\n(Paper Search Specialist, Topic Mining \\\\& Clustering, Academic Survey Writer,\\nand Quality Evaluator) working in concert to generate comprehensive literature\\nsurveys with superior synthesis quality. Through experiments on six\\nrepresentative LLM research topics from COLM 2024 categories, we demonstrate\\nthat our multi-agent approach achieves significant improvements over existing\\nbaselines, scoring 8.18/10 compared to AutoSurvey's 4.77/10. The multi-agent\\narchitecture processes 75--443 papers per topic (847 total across six topics)\\nwhile targeting high citation coverage (often $\\\\geq$80\\\\% on 75--100-paper sets;\\nlower on very large sets such as RLHF) through specialized agent orchestration.\\nOur 12-dimension evaluation captures organization, synthesis integration, and\\ncritical analysis beyond basic metrics. These findings demonstrate that\\nmulti-agent architectures represent a meaningful advancement for automated\\nliterature survey generation in rapidly evolving scientific domains.\"}, 'authors': [{'name': 'Yixin Liu'}, {'name': 'Yonghui Wu'}, {'name': 'Denghui Zhang'}, {'name': 'Lichao Sun'}], 'author_detail': {'name': 'Lichao Sun'}, 'author': 'Lichao Sun', 'arxiv_comment': '29 pages, 7 figures', 'links': [{'href': 'http://arxiv.org/abs/2509.18661v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2509.18661v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.IR', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.IR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.HC', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}, {'id': 'http://arxiv.org/abs/2510.03930v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2510.03930v1', 'updated': '2025-10-04T20:21:39Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=10, tm_mday=4, tm_hour=20, tm_min=21, tm_sec=39, tm_wday=5, tm_yday=277, tm_isdst=0), 'published': '2025-10-04T20:21:39Z', 'published_parsed': time.struct_time(tm_year=2025, tm_mon=10, tm_mday=4, tm_hour=20, tm_min=21, tm_sec=39, tm_wday=5, tm_yday=277, tm_isdst=0), 'title': 'LLM Chemistry Estimation for Multi-LLM Recommendation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'LLM Chemistry Estimation for Multi-LLM Recommendation'}, 'summary': 'Multi-LLM collaboration promises accurate, robust, and context-aware\\nsolutions, yet existing approaches rely on implicit selection and output\\nassessment without analyzing whether collaborating models truly complement or\\nconflict. We introduce LLM Chemistry -- a framework that measures when LLM\\ncombinations exhibit synergistic or antagonistic behaviors that shape\\ncollective performance beyond individual capabilities. We formalize the notion\\nof chemistry among LLMs, propose algorithms that quantify it by analyzing\\ninteraction dependencies, and recommend optimal model ensembles accordingly.\\nOur theoretical analysis shows that chemistry among collaborating LLMs is most\\nevident under heterogeneous model profiles, with its outcome impact shaped by\\ntask type, group size, and complexity. Evaluation on classification,\\nsummarization, and program repair tasks provides initial evidence for these\\ntask-dependent effects, thereby reinforcing our theoretical results. This\\nestablishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and\\na foundation for ensemble recommendation.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'Multi-LLM collaboration promises accurate, robust, and context-aware\\nsolutions, yet existing approaches rely on implicit selection and output\\nassessment without analyzing whether collaborating models truly complement or\\nconflict. We introduce LLM Chemistry -- a framework that measures when LLM\\ncombinations exhibit synergistic or antagonistic behaviors that shape\\ncollective performance beyond individual capabilities. We formalize the notion\\nof chemistry among LLMs, propose algorithms that quantify it by analyzing\\ninteraction dependencies, and recommend optimal model ensembles accordingly.\\nOur theoretical analysis shows that chemistry among collaborating LLMs is most\\nevident under heterogeneous model profiles, with its outcome impact shaped by\\ntask type, group size, and complexity. Evaluation on classification,\\nsummarization, and program repair tasks provides initial evidence for these\\ntask-dependent effects, thereby reinforcing our theoretical results. This\\nestablishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and\\na foundation for ensemble recommendation.'}, 'authors': [{'name': 'Huascar Sanchez'}, {'name': 'Briland Hitaj'}], 'author_detail': {'name': 'Briland Hitaj'}, 'author': 'Briland Hitaj', 'arxiv_comment': '20 pages, 5 figures, 5 tables', 'links': [{'href': 'http://arxiv.org/abs/2510.03930v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2510.03930v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}, {'id': 'http://arxiv.org/abs/1208.5979v3', 'guidislink': True, 'link': 'http://arxiv.org/abs/1208.5979v3', 'updated': '2013-02-08T14:03:07Z', 'updated_parsed': time.struct_time(tm_year=2013, tm_mon=2, tm_mday=8, tm_hour=14, tm_min=3, tm_sec=7, tm_wday=4, tm_yday=39, tm_isdst=0), 'published': '2012-08-29T18:20:53Z', 'published_parsed': time.struct_time(tm_year=2012, tm_mon=8, tm_mday=29, tm_hour=18, tm_min=20, tm_sec=53, tm_wday=2, tm_yday=242, tm_isdst=0), 'title': 'Beyond LLM in M-theory', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'Beyond LLM in M-theory'}, 'summary': 'The Lin, Lunin, Maldacena (LLM) ansatz in D = 11 supports two independent\\nKilling directions when a general Killing spinor ansatz is considered. Here we\\nshow that these directions always commute, identify when the Killing spinors\\nare charged, and show that both their inner product and resulting geometry are\\ngoverned by two fundamental constants. In particular, setting one constant to\\nzero leads to AdS7 x S4, setting the other to zero gives AdS4 x S7, while flat\\nspacetime is recovered when both these constants are zero. Furthermore, when\\nthe constants are equal, the spacetime is either LLM, or it corresponds to the\\nKowalski-Glikman solution where the constants are simply the mass parameter.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'The Lin, Lunin, Maldacena (LLM) ansatz in D = 11 supports two independent\\nKilling directions when a general Killing spinor ansatz is considered. Here we\\nshow that these directions always commute, identify when the Killing spinors\\nare charged, and show that both their inner product and resulting geometry are\\ngoverned by two fundamental constants. In particular, setting one constant to\\nzero leads to AdS7 x S4, setting the other to zero gives AdS4 x S7, while flat\\nspacetime is recovered when both these constants are zero. Furthermore, when\\nthe constants are equal, the spacetime is either LLM, or it corresponds to the\\nKowalski-Glikman solution where the constants are simply the mass parameter.'}, 'authors': [{'name': 'Eoin  Colgin'}], 'author_detail': {'name': 'Eoin  Colgin'}, 'author': 'Eoin  Colgin', 'arxiv_doi': '10.1007/JHEP12(2012)023', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.1007/JHEP12(2012)023', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/1208.5979v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/1208.5979v3', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': '1+30 pages, footnote added', 'arxiv_journal_ref': 'JHEP 1212 (2012) 023', 'arxiv_primary_category': {'term': 'hep-th', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'hep-th', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}, {'id': 'http://arxiv.org/abs/2305.19321v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.19321v2', 'updated': '2024-04-18T08:27:16Z', 'updated_parsed': time.struct_time(tm_year=2024, tm_mon=4, tm_mday=18, tm_hour=8, tm_min=27, tm_sec=16, tm_wday=3, tm_yday=109, tm_isdst=0), 'published': '2023-05-30T18:00:04Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=30, tm_hour=18, tm_min=0, tm_sec=4, tm_wday=1, tm_yday=150, tm_isdst=0), 'title': 'Chaotic LLM billiards', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'Chaotic LLM billiards'}, 'summary': 'We study null geodesics of the ten-dimensional LLM geometries. In particular,\\nwe show that there are a subset of these null geodesics that are confined to\\nthe LLM plane. The effective dynamics of these in-plane geodesics is that of a\\nHamiltonian system with two degrees of freedom (a phase space of dimension 4).\\nWe show that these are chaotic. In the two-coloring of the LLM plane, if they\\nstart in the empty region, they cannot penetrate the filled region and\\nviceversa. The dynamical problem is therefore very similar to that of a\\nbilliards problem with fixed obstacles. We study to what extent LLM geometries\\nwith many droplets may be treated as an incipient black hole and draw analogies\\nwith the fuzzball proposal.\\n  We argue that for in-plane null geodesics deep in the interior of a region\\nwith a lot of droplets, in order to exit towards the $AdS$ boundary they will\\nneed to undergo a process that resembles diffusion. This mechanism can account\\nfor signals getting lost in the putative black hole for a very long time.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'We study null geodesics of the ten-dimensional LLM geometries. In particular,\\nwe show that there are a subset of these null geodesics that are confined to\\nthe LLM plane. The effective dynamics of these in-plane geodesics is that of a\\nHamiltonian system with two degrees of freedom (a phase space of dimension 4).\\nWe show that these are chaotic. In the two-coloring of the LLM plane, if they\\nstart in the empty region, they cannot penetrate the filled region and\\nviceversa. The dynamical problem is therefore very similar to that of a\\nbilliards problem with fixed obstacles. We study to what extent LLM geometries\\nwith many droplets may be treated as an incipient black hole and draw analogies\\nwith the fuzzball proposal.\\n  We argue that for in-plane null geodesics deep in the interior of a region\\nwith a lot of droplets, in order to exit towards the $AdS$ boundary they will\\nneed to undergo a process that resembles diffusion. This mechanism can account\\nfor signals getting lost in the putative black hole for a very long time.'}, 'authors': [{'name': 'David Berenstein'}, {'name': 'Elliot Maderazo'}, {'name': 'Robinson Mancilla'}, {'name': 'Anayeli Ramirez'}], 'author_detail': {'name': 'Anayeli Ramirez'}, 'author': 'Anayeli Ramirez', 'arxiv_comment': '18 pages, 8 figures, uses JHEP. v2: Typos corrected, references added', 'links': [{'href': 'http://arxiv.org/abs/2305.19321v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.19321v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'hep-th', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'hep-th', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'gr-qc', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}, {'id': 'http://arxiv.org/abs/1806.06586v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/1806.06586v1', 'updated': '2018-06-18T10:30:20Z', 'updated_parsed': time.struct_time(tm_year=2018, tm_mon=6, tm_mday=18, tm_hour=10, tm_min=30, tm_sec=20, tm_wday=0, tm_yday=169, tm_isdst=0), 'published': '2018-06-18T10:30:20Z', 'published_parsed': time.struct_time(tm_year=2018, tm_mon=6, tm_mday=18, tm_hour=10, tm_min=30, tm_sec=20, tm_wday=0, tm_yday=169, tm_isdst=0), 'title': 'Exciting LLM Geometries', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'Exciting LLM Geometries'}, 'summary': 'We study excitations of LLM geometries. These geometries arise from the\\nbackreaction of a condensate of giant gravitons. Excitations of the condensed\\nbranes are open strings, which give rise to an emergent Yang-Mills theory at\\nlow energy. We study the dynamics of the planar limit of these emergent gauge\\ntheories, accumulating evidence that they are planar ${\\\\cal N}=4$ super\\nYang-Mills. There are three observations supporting this conclusion: (i) we\\nargue for an isomorphism between the planar Hilbert space of the original\\n${\\\\cal N}=4$ super Yang-Mills and the planar Hilbert space of the emergent\\ngauge theory, (ii) we argue that the OPE coefficients of the planar limit of\\nthe emergent gauge theory vanish and (iii) we argue that the planar spectrum of\\nanomalous dimensions of the emergent gauge theory is that of planar ${\\\\cal\\nN}=4$ super Yang-Mills. Despite the fact that the planar limit of the emergent\\ngauge theory is planar ${\\\\cal N}=4$ super Yang-Mills, we explain why the\\nemergent gauge theory is not ${\\\\cal N}=4$ super Yang-Mills theory.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'We study excitations of LLM geometries. These geometries arise from the\\nbackreaction of a condensate of giant gravitons. Excitations of the condensed\\nbranes are open strings, which give rise to an emergent Yang-Mills theory at\\nlow energy. We study the dynamics of the planar limit of these emergent gauge\\ntheories, accumulating evidence that they are planar ${\\\\cal N}=4$ super\\nYang-Mills. There are three observations supporting this conclusion: (i) we\\nargue for an isomorphism between the planar Hilbert space of the original\\n${\\\\cal N}=4$ super Yang-Mills and the planar Hilbert space of the emergent\\ngauge theory, (ii) we argue that the OPE coefficients of the planar limit of\\nthe emergent gauge theory vanish and (iii) we argue that the planar spectrum of\\nanomalous dimensions of the emergent gauge theory is that of planar ${\\\\cal\\nN}=4$ super Yang-Mills. Despite the fact that the planar limit of the emergent\\ngauge theory is planar ${\\\\cal N}=4$ super Yang-Mills, we explain why the\\nemergent gauge theory is not ${\\\\cal N}=4$ super Yang-Mills theory.'}, 'authors': [{'name': 'Robert de Mello Koch'}, {'name': 'Jia-Hui Huang'}, {'name': 'Laila Tribelhorn'}], 'author_detail': {'name': 'Laila Tribelhorn'}, 'author': 'Laila Tribelhorn', 'arxiv_doi': '10.1007/JHEP07(2018)146', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.1007/JHEP07(2018)146', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/1806.06586v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/1806.06586v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': '30 pages plus Appendices', 'arxiv_primary_category': {'term': 'hep-th', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'hep-th', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}, {'id': 'http://arxiv.org/abs/2308.05481v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2308.05481v2', 'updated': '2023-08-11T07:55:19Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=8, tm_mday=11, tm_hour=7, tm_min=55, tm_sec=19, tm_wday=4, tm_yday=223, tm_isdst=0), 'published': '2023-08-10T10:12:43Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=8, tm_mday=10, tm_hour=10, tm_min=12, tm_sec=43, tm_wday=3, tm_yday=222, tm_isdst=0), 'title': 'LLM As DBA', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'LLM As DBA'}, 'summary': 'Database administrators (DBAs) play a crucial role in managing, maintaining\\nand optimizing a database system to ensure data availability, performance, and\\nreliability. However, it is hard and tedious for DBAs to manage a large number\\nof database instances (e.g., millions of instances on the cloud databases).\\nRecently large language models (LLMs) have shown great potential to understand\\nvaluable documents and accordingly generate reasonable answers. Thus, we\\npropose D-Bot, a LLM-based database administrator that can continuously acquire\\ndatabase maintenance experience from textual sources, and provide reasonable,\\nwell-founded, in-time diagnosis and optimization advice for target databases.\\nThis paper presents a revolutionary LLM-centric framework for database\\nmaintenance, including (i) database maintenance knowledge detection from\\ndocuments and tools, (ii) tree of thought reasoning for root cause analysis,\\nand (iii) collaborative diagnosis among multiple LLMs. Our preliminary\\nexperimental results that D-Bot can efficiently and effectively diagnose the\\nroot causes and our code is available at\\ngithub.com/TsinghuaDatabaseGroup/DB-GPT.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'Database administrators (DBAs) play a crucial role in managing, maintaining\\nand optimizing a database system to ensure data availability, performance, and\\nreliability. However, it is hard and tedious for DBAs to manage a large number\\nof database instances (e.g., millions of instances on the cloud databases).\\nRecently large language models (LLMs) have shown great potential to understand\\nvaluable documents and accordingly generate reasonable answers. Thus, we\\npropose D-Bot, a LLM-based database administrator that can continuously acquire\\ndatabase maintenance experience from textual sources, and provide reasonable,\\nwell-founded, in-time diagnosis and optimization advice for target databases.\\nThis paper presents a revolutionary LLM-centric framework for database\\nmaintenance, including (i) database maintenance knowledge detection from\\ndocuments and tools, (ii) tree of thought reasoning for root cause analysis,\\nand (iii) collaborative diagnosis among multiple LLMs. Our preliminary\\nexperimental results that D-Bot can efficiently and effectively diagnose the\\nroot causes and our code is available at\\ngithub.com/TsinghuaDatabaseGroup/DB-GPT.'}, 'authors': [{'name': 'Xuanhe Zhou'}, {'name': 'Guoliang Li'}, {'name': 'Zhiyuan Liu'}], 'author_detail': {'name': 'Zhiyuan Liu'}, 'author': 'Zhiyuan Liu', 'links': [{'href': 'http://arxiv.org/abs/2308.05481v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2308.05481v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.DB', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.DB', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}, {'id': 'http://arxiv.org/abs/2309.13308v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2309.13308v1', 'updated': '2023-09-23T08:46:11Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=9, tm_mday=23, tm_hour=8, tm_min=46, tm_sec=11, tm_wday=5, tm_yday=266, tm_isdst=0), 'published': '2023-09-23T08:46:11Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=9, tm_mday=23, tm_hour=8, tm_min=46, tm_sec=11, tm_wday=5, tm_yday=266, tm_isdst=0), 'title': 'Calibrating LLM-Based Evaluator', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'Calibrating LLM-Based Evaluator'}, 'summary': 'Recent advancements in large language models (LLMs) on language modeling and\\nemergent capabilities make them a promising reference-free evaluator of natural\\nlanguage generation quality, and a competent alternative to human evaluation.\\nHowever, hindered by the closed-source or high computational demand to host and\\ntune, there is a lack of practice to further calibrate an off-the-shelf\\nLLM-based evaluator towards better human alignment. In this work, we propose\\nAutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate\\nand align an LLM-based evaluator toward human preference. Instead of explicitly\\nmodeling human preferences, we first implicitly encompass them within a set of\\nhuman labels. Then, an initial set of scoring criteria is drafted by the\\nlanguage model itself, leveraging in-context learning on different few-shot\\nexamples. To further calibrate this set of criteria, we select the best\\nperformers and re-draft them with self-refinement. Our experiments on multiple\\ntext quality evaluation datasets illustrate a significant improvement in\\ncorrelation with expert evaluation through calibration. Our comprehensive\\nqualitative analysis conveys insightful intuitions and observations on the\\nessence of effective scoring criteria.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'Recent advancements in large language models (LLMs) on language modeling and\\nemergent capabilities make them a promising reference-free evaluator of natural\\nlanguage generation quality, and a competent alternative to human evaluation.\\nHowever, hindered by the closed-source or high computational demand to host and\\ntune, there is a lack of practice to further calibrate an off-the-shelf\\nLLM-based evaluator towards better human alignment. In this work, we propose\\nAutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate\\nand align an LLM-based evaluator toward human preference. Instead of explicitly\\nmodeling human preferences, we first implicitly encompass them within a set of\\nhuman labels. Then, an initial set of scoring criteria is drafted by the\\nlanguage model itself, leveraging in-context learning on different few-shot\\nexamples. To further calibrate this set of criteria, we select the best\\nperformers and re-draft them with self-refinement. Our experiments on multiple\\ntext quality evaluation datasets illustrate a significant improvement in\\ncorrelation with expert evaluation through calibration. Our comprehensive\\nqualitative analysis conveys insightful intuitions and observations on the\\nessence of effective scoring criteria.'}, 'authors': [{'name': 'Yuxuan Liu'}, {'name': 'Tianchi Yang'}, {'name': 'Shaohan Huang'}, {'name': 'Zihan Zhang'}, {'name': 'Haizhen Huang'}, {'name': 'Furu Wei'}, {'name': 'Weiwei Deng'}, {'name': 'Feng Sun'}, {'name': 'Qi Zhang'}], 'author_detail': {'name': 'Qi Zhang'}, 'author': 'Qi Zhang', 'arxiv_comment': '22 pages,11 figures', 'links': [{'href': 'http://arxiv.org/abs/2309.13308v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2309.13308v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}, {'id': 'http://arxiv.org/abs/2310.19462v3', 'guidislink': True, 'link': 'http://arxiv.org/abs/2310.19462v3', 'updated': '2025-09-26T10:39:10Z', 'updated_parsed': time.struct_time(tm_year=2025, tm_mon=9, tm_mday=26, tm_hour=10, tm_min=39, tm_sec=10, tm_wday=4, tm_yday=269, tm_isdst=0), 'published': '2023-10-30T11:39:11Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=10, tm_mday=30, tm_hour=11, tm_min=39, tm_sec=11, tm_wday=0, tm_yday=303, tm_isdst=0), 'title': 'Constituency Parsing using LLMs', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'Constituency Parsing using LLMs'}, 'summary': 'Constituency parsing is a fundamental yet unsolved challenge in natural\\nlanguage processing. In this paper, we examine the potential of recent large\\nlanguage models (LLMs) to address this challenge. We reformat constituency\\nparsing as a sequence-to-sequence generation problem and evaluate the\\nperformance of a diverse range of LLMs under zero-shot, few-shot, and\\nsupervised fine-tuning learning paradigms. We observe that while LLMs achieve\\nacceptable improvements, they still encounter substantial limitations, due to\\nthe absence of mechanisms to guarantee the validity and faithfulness of the\\ngenerated constituent trees. Motivated by this observation, we propose two\\nstrategies to guide LLMs to generate more accurate constituent trees by\\nlearning from erroneous samples and refining outputs in a multi-agent\\ncollaboration way, respectively. The experimental results demonstrate that our\\nmethods effectively reduce the occurrence of invalid and unfaithful trees,\\nthereby enhancing overall parsing performance and achieving promising results\\nacross different learning paradigms.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'Constituency parsing is a fundamental yet unsolved challenge in natural\\nlanguage processing. In this paper, we examine the potential of recent large\\nlanguage models (LLMs) to address this challenge. We reformat constituency\\nparsing as a sequence-to-sequence generation problem and evaluate the\\nperformance of a diverse range of LLMs under zero-shot, few-shot, and\\nsupervised fine-tuning learning paradigms. We observe that while LLMs achieve\\nacceptable improvements, they still encounter substantial limitations, due to\\nthe absence of mechanisms to guarantee the validity and faithfulness of the\\ngenerated constituent trees. Motivated by this observation, we propose two\\nstrategies to guide LLMs to generate more accurate constituent trees by\\nlearning from erroneous samples and refining outputs in a multi-agent\\ncollaboration way, respectively. The experimental results demonstrate that our\\nmethods effectively reduce the occurrence of invalid and unfaithful trees,\\nthereby enhancing overall parsing performance and achieving promising results\\nacross different learning paradigms.'}, 'authors': [{'name': 'Xuefeng Bai'}, {'name': 'Jialong Wu'}, {'name': 'Yulong Chen'}, {'name': 'Zhongqing Wang'}, {'name': 'Kehai Chen'}, {'name': 'Min Zhang'}, {'name': 'Yue Zhang'}], 'author_detail': {'name': 'Yue Zhang'}, 'author': 'Yue Zhang', 'arxiv_doi': '10.1109/taslpro.2025.3600867', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.1109/taslpro.2025.3600867', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2310.19462v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2310.19462v3', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': 'Accepted at IEEE Transactions on Audio, Speech, and Language\\n  Processing (TASLP). See https://ieeexplore.ieee.org/document/11130901/ for\\n  the official version', 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}, {'id': 'http://arxiv.org/abs/2311.02268v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2311.02268v1', 'updated': '2023-11-03T23:12:57Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=11, tm_mday=3, tm_hour=23, tm_min=12, tm_sec=57, tm_wday=4, tm_yday=307, tm_isdst=0), 'published': '2023-11-03T23:12:57Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=11, tm_mday=3, tm_hour=23, tm_min=12, tm_sec=57, tm_wday=4, tm_yday=307, tm_isdst=0), 'title': 'LLMs-augmented Contextual Bandit', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'LLMs-augmented Contextual Bandit'}, 'summary': 'Contextual bandits have emerged as a cornerstone in reinforcement learning,\\nenabling systems to make decisions with partial feedback. However, as contexts\\ngrow in complexity, traditional bandit algorithms can face challenges in\\nadequately capturing and utilizing such contexts. In this paper, we propose a\\nnovel integration of large language models (LLMs) with the contextual bandit\\nframework. By leveraging LLMs as an encoder, we enrich the representation of\\nthe context, providing the bandit with a denser and more informative view.\\nPreliminary results on synthetic datasets demonstrate the potential of this\\napproach, showing notable improvements in cumulative rewards and reductions in\\nregret compared to traditional bandit algorithms. This integration not only\\nshowcases the capabilities of LLMs in reinforcement learning but also opens the\\ndoor to a new era of contextually-aware decision systems.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': 'https://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15', 'value': 'Contextual bandits have emerged as a cornerstone in reinforcement learning,\\nenabling systems to make decisions with partial feedback. However, as contexts\\ngrow in complexity, traditional bandit algorithms can face challenges in\\nadequately capturing and utilizing such contexts. In this paper, we propose a\\nnovel integration of large language models (LLMs) with the contextual bandit\\nframework. By leveraging LLMs as an encoder, we enrich the representation of\\nthe context, providing the bandit with a denser and more informative view.\\nPreliminary results on synthetic datasets demonstrate the potential of this\\napproach, showing notable improvements in cumulative rewards and reductions in\\nregret compared to traditional bandit algorithms. This integration not only\\nshowcases the capabilities of LLMs in reinforcement learning but also opens the\\ndoor to a new era of contextually-aware decision systems.'}, 'authors': [{'name': 'Ali Baheri'}, {'name': 'Cecilia O. Alm'}], 'author_detail': {'name': 'Cecilia O. Alm'}, 'author': 'Cecilia O. Alm', 'arxiv_comment': 'Accepted by the Foundation Models for Decision Making workshop at\\n  NeurIPS 2023', 'links': [{'href': 'http://arxiv.org/abs/2311.02268v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.02268v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}]\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "\n",
    "url = 'http://export.arxiv.org/api/query?search_query=ti:LLM&max_results=15'\n",
    "feed = feedparser.parse(url)\n",
    "\n",
    "print(feed.entries)\n",
    "print(len(feed.entries))\n",
    "# print(feed.entries[0].keys())\n",
    "\n",
    "# for entry in feed.entries:\n",
    "#     print(\"Title:\", entry.title)\n",
    "#     print(\"Authors:\", [author.name for author in entry.authors])\n",
    "#     print(\"Published:\", entry.published)\n",
    "#     print(\"Summary:\", entry.summary)\n",
    "#     print(\"Link:\", entry.link)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
